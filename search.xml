<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Centos7安装Mongodb数据库]]></title>
    <url>%2Fmongodb%2F</url>
    <content type="text"><![CDATA[原文链接：NoSQL——MongoDB mongodb安装按照官方网站 https://docs.mongodb.com/manual/tutorial/install-mongodb-on-red-hat/， 编辑 repo 文件。 123456789[root@ying01 ~]# cd /etc/yum.repos.d/[root@ying01 yum.repos.d]# vim mongodb-org-4.0.repo[mongodb-org-4.0]name = MongoDB Repositorybaseurl = https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/4.0/x86_64/gpgcheck = 1enabled = 1gpgkey = https://www.mongodb.org/static/pgp/server-4.0.asc 查看 yum list 当中新生成的 mongodb-org，并用 yum 安装所有的 mongodb-org。 12345678[root@ying01 yum.repos.d]# yum list|grep mongodb-orgmongodb-org.x86_64 4.0.1-1.el7 @mongodb-org-4.0mongodb-org-mongos.x86_64 4.0.1-1.el7 @mongodb-org-4.0mongodb-org-server.x86_64 4.0.1-1.el7 @mongodb-org-4.0mongodb-org-shell.x86_64 4.0.1-1.el7 @mongodb-org-4.0mongodb-org-tools.x86_64 4.0.1-1.el7 @mongodb-org-4.0[root@ying01 yum.repos.d]# yum install -y mongodb-org 连接mongodb连接配置在 mongod.conf 文件中，添加本机 ip，用逗号隔开。 123[root@ying01 ~]# vim /etc/mongod.confbindIp: 127.0.0.1,192.168.112.136 //增加IP 启动，并查看进程和端口。 123456789[root@ying01 ~]# systemctl start mongod[root@ying01 ~]# ps aux |grep mongodmongod 8169 0.8 3.2 1074460 60764 ? Sl 09:51 0:01 /usr/bin/mongod -f /etc/mongod.confroot 8197 0.0 0.0 112720 984 pts/0 S+ 09:54 0:00 grep --color=auto mongod[root@ying01 ~]# netstat -lntp |grep mongodtcp 0 0 192.168.112.136:27017 0.0.0.0:* LISTEN 8169/mongod tcp 0 0 127.0.0.1:27017 0.0.0.0:* LISTEN 8169/mongod [root@ying01 ~]# 连接数据库直接连接1[root@ying01 ~]# mongo 指定端口（配置文件没有明确指定）1[root@ying01 ~]# mongo --port 27017 远程连接，需要IP 和端口1[root@ying01 ~]# mongo --host 192.168.112.136 --port 27017 mongodb 用户管理进入数据库，创建用户123456789101112131415161718[root@ying01 ~]# mongo&gt; use admin //切换到admin库switched to db admin&gt; db.createUser( &#123; user: "admin", customData: &#123;description: "superuser"&#125;, pwd: "www123", roles: [ &#123; role: "root", db: "admin" &#125; ] &#125; ) //创建admin用户Successfully added user: &#123; "user" : "admin", "customData" : &#123; "description" : "superuser" &#125;, "roles" : [ &#123; "role" : "root", "db" : "admin" &#125; ]&#125; 语句释义： 1234567db.createUser( &#123; user: "admin", customData: &#123;description: "superuser"&#125;, pwd: "admin122", roles: [ &#123; role: "root", db: "admin" &#125; ] &#125; )db.createUser 创建用户的命令user: "admin" 定义用户名customData: &#123;description: "superuser"&#125;pwd: "admin122" 定义用户密码roles: [ &#123; role: "root", db: "admin" &#125; ] 规则：角色 root,数据库admin 列出所有用户，需要切换到admin库12&gt; db.system.users.find()&#123; "_id" : "admin.admin", "user" : "admin", "db" : "admin", "credentials" : &#123; "SCRAM-SHA-1" : &#123; "iterationCount" : 10000, "salt" : "NRoDD1kSxLktW8vyDg4mpw==", "storedKey" : "yX+2kSbCl1bPpsh+0ZeE7QlcW6A=", "serverKey" : "XM9NgrMNOwXAvuWusY6iVhpyuFw=" &#125;, "SCRAM-SHA-256" : &#123; "iterationCount" : 15000, "salt" : "MOokBWPCOobBeNwHnhm/2QagzAT8h2yIuCzROg==", "storedKey" : "tAqs7zMF8InT0FU09lCgq2ZVB9wRgeIyoa1UONgRDM0=", "serverKey" : "lN2TYZX5Snik4gMthUNZE7jw71Nkxo13LAChh9K8ZiI=" &#125; &#125;, "customData" : &#123; "description" : "superuser" &#125;, "roles" : [ &#123; "role" : "root", "db" : "admin" &#125; ] &#125; 查看当前库下所有的用户12345678910111213141516171819&gt; show users&#123; "_id" : "admin.admin", "user" : "admin", "db" : "admin", "customData" : &#123; "description" : "superuser" &#125;, "roles" : [ &#123; "role" : "root", "db" : "admin" &#125; ], "mechanisms" : [ "SCRAM-SHA-1", "SCRAM-SHA-256" ]&#125; 创建新用户1234567891011121314151617181920212223242526272829303132333435363738394041424344&gt; db.createUser(&#123;user:"ying",pwd:"www123",roles:[&#123;role:"read",db:"testdb"&#125;]&#125;)Successfully added user: &#123; "user" : "ying", "roles" : [ &#123; "role" : "read", "db" : "testdb" &#125; ]&#125;&gt; show users &#123; "_id" : "admin.admin", "user" : "admin", "db" : "admin", "customData" : &#123; "description" : "superuser" &#125;, "roles" : [ &#123; "role" : "root", "db" : "admin" &#125; ], "mechanisms" : [ "SCRAM-SHA-1", "SCRAM-SHA-256" ]&#125;&#123; "_id" : "admin.ying", "user" : "ying", "db" : "admin", "roles" : [ &#123; "role" : "read", "db" : "testdb" &#125; ], "mechanisms" : [ "SCRAM-SHA-1", "SCRAM-SHA-256" ]&#125; 删除用户123456789101112131415161718192021&gt; db.dropUser('ying')true&gt; show users&#123; "_id" : "admin.admin", "user" : "admin", "db" : "admin", "customData" : &#123; "description" : "superuser" &#125;, "roles" : [ &#123; "role" : "root", "db" : "admin" &#125; ], "mechanisms" : [ "SCRAM-SHA-1", "SCRAM-SHA-256" ]&#125; 切换到 testdb 库，若此库不存在，会自动创建123456&gt; use testdbswitched to db testdb&gt; show users //当前库，无用户&gt; db.system.user.find()&gt; ^Cbye 要使用用户生效，需要编辑启动服务脚本，添加 –auth，以后登录需要身份验证1234[root@ying01 ~]# vim /usr/lib/systemd/system/mongod.serviceEnvironment="OPTIONS=-f /etc/mongod.conf" 改为 Environment="OPTIONS=--auth -f /etc/mongod.conf" 重启 mongod 服务 1234567[root@ying01 ~]# systemctl restart mongodWarning: mongod.service changed on disk. Run 'systemctl daemon-reload' to reload units.[root@ying01 ~]# systemctl daemon-reload[root@ying01 ~]# systemctl restart mongod[root@ying01 ~]# ps aux |grep mongodmongod 8611 12.6 2.8 1068324 52744 ? Sl 11:42 0:01 /usr/bin/mongod --auth -f /etc/mongod.confroot 8642 0.0 0.0 112720 980 pts/0 S+ 11:42 0:00 grep --color=auto mongod 现在直接访问，不加密码，不会查看数据库，因为需要认证 12345678[root@ying01 ~]# mongo --host 192.168.112.136 --port 27017MongoDB shell version v4.0.1connecting to: mongodb://192.168.112.136:27017/MongoDB server version: 4.0.1&gt; use adminswitched to db admin&gt; show users2018-08-27T11:43:36.654+0800 E QUERY [js] Error: command usersInfo requires authentication : //需要身份验证 使用密码，身份认证，登录 1234567891011121314151617181920212223[root@ying01 ~]# mongo --host 192.168.112.136 --port 27017 -u admin -p 'admin122' --authenticationDatabase "admin"&gt; use adminswitched to db admin&gt; show users&#123; "_id" : "admin.admin", "user" : "admin", "db" : "admin", "customData" : &#123; "description" : "superuser" &#125;, "roles" : [ &#123; "role" : "root", "db" : "admin" &#125; ], "mechanisms" : [ "SCRAM-SHA-1", "SCRAM-SHA-256" ]&#125; 切换到db1下，创建新用户12345678910111213141516171819202122232425262728293031323334353637383940414243&gt; use db1switched to db db1&gt; show users&gt; db.createUser( &#123; user: "test1", pwd: "www123", roles: [ &#123; role: "readWrite", db: "db1" &#125;, &#123;role: "read", db: "db2" &#125; ] &#125; )Successfully added user: &#123; "user" : "test1", "roles" : [ &#123; "role" : "readWrite", "db" : "db1" &#125;, &#123; "role" : "read", "db" : "db2" &#125; ]&#125;&gt; show users //查看在db1库下，创建的用户&#123; "_id" : "db1.test1", "user" : "test1", "db" : "db1", "roles" : [ &#123; "role" : "readWrite", "db" : "db1" &#125;, &#123; "role" : "read", "db" : "db2" &#125; ], "mechanisms" : [ "SCRAM-SHA-1", "SCRAM-SHA-256" ]&#125;&gt; use db1switched to db db1&gt; db.auth('test1','www123') //验证用户test1，返回值为1，则验证成功 1 MongoDB用户角色 12345678910Read：允许用户读取指定数据库readWrite：允许用户读写指定数据库dbAdmin：允许用户在指定数据库中执行管理函数，如索引创建、删除，查看统计或访问system.profileuserAdmin：允许用户向system.users集合写入，可以找指定数据库里创建、删除和管理用户clusterAdmin：只在admin数据库中可用，赋予用户所有分片和复制集相关函数的管理权限。readAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的读权限readWriteAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的读写权限userAdminAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的userAdmin权限dbAdminAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的dbAdmin权限。root：只在admin数据库中可用。超级账号，超级权限 mongodb创建集合及数据管理在db1下，创建 mycol的集合 123456789101112131415161718192021222324252627282930313233[root@ying01 ~]# mongo --host 192.168.112.136 --port 27017 -u admin -p 'www123' --authenticationDatabase "admin"&gt; use db1switched to db db1&gt; show users&#123; "_id" : "db1.test1", "user" : "test1", "db" : "db1", "roles" : [ &#123; "role" : "readWrite", "db" : "db1" &#125;, &#123; "role" : "read", "db" : "db2" &#125; ], "mechanisms" : [ "SCRAM-SHA-1", "SCRAM-SHA-256" ]&#125;&gt; db.createCollection("mycol", &#123; capped : true, autoIndexID : true, size : 6142800, max : 10000 &#125; )&#123; "ok" : 0, "errmsg" : "too many users are authenticated", "code" : 13, "codeName" : "Unauthorized"&#125; 语法：db.createCollection(name,options) 123456name就是集合的名字options可选，用来配置集合的参数，参数如下: capped true/false （可选）如果为true，则启用封顶集合。封顶集合是固定大小的集合，当它达到其最大大小，会自动覆盖最早的条目。如果指定true，则也需要指定尺寸参数。 autoindexID true/false （可选）如果为true，自动创建索引_id字段的默认值是false。 size （可选）指定最大大小字节封顶集合。如果封顶如果是 true，那么你还需要指定这个字段。单位B max （可选）指定封顶集合允许在文件的最大数量 查看集合：show collections 或者使用show tables12&gt; show collectionsmycol 在集合Account中，直接插入数据。如果该集合不存在，则mongodb会自动创建集合12345678910&gt; db.Account.insert(&#123;AccountID:1,UserName:"123",password:"123456"&#125;)WriteResult(&#123; "nInserted" : 1 &#125;)&gt; show tables //多了一个Accountmycol&gt; db.Account.insert(&#123;AccountID:2,UserName:"ying",password:"abcdef"&#125;) //再插入一条信息WriteResult(&#123; "nInserted" : 1 &#125;)&gt; show tablesAccountmycol 在集合中更新信息数据12&gt; db.Account.update(&#123;AccountID:1&#125;,&#123;"$set":&#123;"Age":20&#125;&#125;) //在集合Account中，第一条中，增加一项信息WriteResult(&#123; "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 &#125;) 查看所有文档：db.Account.find()，此时在第一条 有更新的信息123&gt; db.Account.find()&#123; "_id" : ObjectId("5b83bf9209eb45c97dce1c4c"), "AccountID" : 1, "UserName" : "123", "password" : "123456", "Age" : 20 &#125;&#123; "_id" : ObjectId("5b83bfe509eb45c97dce1c4d"), "AccountID" : 2, "UserName" : "ying", "password" : "abcdef" &#125; 查看指定的文档, db.Account.find({AccountID:2})12&gt; db.Account.find(&#123;AccountID:2&#125;) //查看数据库集合Account中，ID为2的文档&#123; "_id" : ObjectId("5b83bfe509eb45c97dce1c4d"), "AccountID" : 2, "UserName" : "ying", "password" : "abcdef" &#125; 移除指定的文档：db.Account.remove({AccountID:1})1234&gt; db.Account.remove(&#123;AccountID:1&#125;) //移除id为1d 的文档WriteResult(&#123; "nRemoved" : 1 &#125;)&gt; db.Account.find() //查看所有文档，此时已经没有ID为1的文档&#123; "_id" : ObjectId("5b83bfe509eb45c97dce1c4d"), "AccountID" : 2, "UserName" : "ying", "password" : "abcdef" &#125; 删除集合：db.Account.drop()12345678&gt; db.Account.drop() //删除Account集合true&gt; show tablesmycol&gt; db.mycol.drop() //删除mycol集合true&gt; show tables 重新创建集合col212&gt; db.col2.insert(&#123;AccountID:1,UserName:"123",password:"123456"&#125;)WriteResult(&#123; "nInserted" : 1 &#125;) 查看所有集合的状态：db.printCollectionStats()123456789101112131415&gt; db.printCollectionStats()col2&#123; "ns" : "db1.col2", "size" : 80, "count" : 1, "avgObjSize" : 80, "storageSize" : 16384, "capped" : false, "wiredTiger" : &#123; "metadata" : &#123; "formatVersion" : 1 &#125;,......]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫遇到的问题及解决方法]]></title>
    <url>%2Fsolved%2F</url>
    <content type="text"><![CDATA[Python3 unicode转中文今天使用 python3.7 版本写爬虫的时候输出信息应该是中文，但是实际是显示为中文的 unicode 的形式，后去查资料后，有方法说使用 decode 方法，但是我使用 decode 方法却提示我 AttributeError: &#39;str&#39; object has no attribute &#39;decode&#39; 错误，继续查找资料发现 Python3 以上取消了 decode 方法，所以直接对字符串进行 decode 操作就会报错。正确操作方式如下： 1str.encode('utf-8').decode('unicode-escape') Scrapy 提示 DEBUG: Filtered offsite request to ‘xxx.com’出现此问题需要修改 spider.py 文件中 allowed_domains 项：将allowed_domains = [‘www.zhenai.com&#39;]更改为allowed_domains = [‘zhenai.com’] 即更换为对应的一级域名。]]></content>
  </entry>
  <entry>
    <title><![CDATA[破解有道翻译js加密参数]]></title>
    <url>%2Fyoudao%2F</url>
    <content type="text"><![CDATA[分析页面首先进行一次正常的翻译操作，F12 打开开发者工具，点击 Network 标签，点击翻译按钮，可以看到下面第一条请求，点击进去查看。 这条请求的方式是 POST，其 Form Data 参数如下所示，其中可能加密的参数有 salt, sign, ts, bv 这四个参数。 1234567891011121314i: 你好from: AUTOto: AUTOsmartresult: dictclient: fanyideskwebsalt: 15483221921975sign: 3d70a5b668a2edc0c8651848e99ec40dts: 1548322192197bv: 7f2901ed530536104d65f4d3f630826adoctype: jsonversion: 2.1keyfrom: fanyi.webaction: FY_BY_REALTIMEtypoResult: false 分析 js 文件通过查看网站源文件，发现其加密参数被放在了 fanyi.min.js 文件中，由于 js 代码被压缩过，因此需要先把代码进行格式化，点击左下角的 {} 就可以把代码格式化。 格式化后如下图所示，看着舒服多了。 按照关键词 salt 和 sign 搜索它们出现在代码中的位置，可以看到下图。 可以看到，参数中的 ts, bv, salt 和 sign 参数都在这里了，这里先打一个断点看下结果。 其实在这里就已经大致能知道各个参数的加密过程了，加密过程在上图中有解释，不再过多介绍。唯一不明显的就是这个 appVersion ，其实它也是个固定值，是跟着浏览器的 User-Agent 变动的，鼠标放置在 navigator 上面，出现了 navigator 的属性框，其中第三行就是 appVersion， bv 值其实就是将 appVersion 加密后的值。 模拟加密过程salt 参数1234567def get_salt(): import time,random salt = int(time.time()*1000)+random.randint(0,10) return salt sign 参数12345678910111213141516171819def get_md5(v): import hashlib md5 = hashlib.md5() md5.update(v.encode('utf-8')) value = md5.hexdigest() return valuedef get_sign(key,salt): sign = 'fanyideskweb' + key + str(salt) + 'p09@Bn&#123;h02_BIEe]$P^nG' sign = get_md5(sign) return sign ts 参数1234567def getts(): import time ts = int(time.time() * 1000) return ts bv 参数1234567891011def getbv(self): appVersion = '5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36' md5 = hashlib.md5() md5.update(appVersion.encode('utf-8')) bv = md5.hexdigest() return bv 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124import hashlibimport randomimport timeimport requestsfrom urllib.parse import urlencodeclass YoudaoFanyi(object): def __init__(self, key): self.url = 'http://fanyi.youdao.com/translate_o?smartresult=dict&amp;smartresult=true' self.key = key def getSalt(self): """ 获取 salt 参数 :return: salt 参数 """ salt = int(time.time() * 1000) + random.randint(0, 10) return salt def getMD5(self, v): """ 加密值 :param v: 将加密的值 :return: 加密后的值 """ md5 = hashlib.md5() md5.update(v.encode('utf-8')) sign = md5.hexdigest() return sign def getSign(self, salt): """ 获取 sign 参数 :param salt: salt :return: sign 参数 """ sign = "fanyideskweb" + self.key + str(salt) + 'p09@Bn&#123;h02_BIEe]$P^nG' sign = self.getMD5(sign) return sign def getbv(self): """ 获取 bv 参数 :return: bv 参数 """ appVersion = '5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36' md5 = hashlib.md5() md5.update(appVersion.encode('utf-8')) bv = md5.hexdigest() return bv def getts(self): """ 获取 ts 参数 :return: ts 参数 """ ts = int(time.time() * 1000) return ts def main(self): salt = self.getSalt() data = &#123; 'i': self.key, 'from': 'AUTO', 'to': 'AUTO', 'smartresult': 'dict', 'client': 'fanyideskweb', 'salt': str(salt), 'sign': self.getSign(salt), 'ts': self.getts(), 'bv': self.getbv(), 'doctype': 'json', 'version': '2.1', 'keyfrom': 'fanyi.web', 'action': 'FY_BY_REALTIME', 'typoResult': 'false' &#125; data = urlencode(data).encode() headers = &#123; "Accept": "application/json, text/javascript, */*; q=0.01", "Accept-Language": "zh-CN,zh;q=0.9", "Connection": "keep-alive", "Content-Length": str(len(data)), "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8", "Cookie": "ANTICSRF=cleared; P_INFO=xiezhengwei2008; NTES_OSESS=cleared; S_OINFO=; OUTFOX_SEARCH_USER_ID=1515915441@10.168.8.61; JSESSIONID=aaaAAT3WmCOQ6-0Zne_Hw; OUTFOX_SEARCH_USER_ID_NCOO=1910496961.1878116; ___rl__test__cookies=1548303281445", "Host": "fanyi.youdao.com", "Origin": "http://fanyi.youdao.com", "Referer": "http://fanyi.youdao.com/", "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36", &#125; response = requests.post(self.url, data=data, headers=headers) if response.status_code == 200: result = response.json().get('translateResult')[0][0].get('tgt') print('Translation result: %s' % result)if __name__ == '__main__': while True: word = input("Please enter the word you want to translate:") if word == 'quit': break youdao = YoudaoFanyi(word) youdao.main() 运行效果]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>加密</tag>
        <tag>js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy对接Selenium京东爬虫]]></title>
    <url>%2Fscrapy-selenium%2F</url>
    <content type="text"><![CDATA[准备工作闲话不多说，准备工作都已经做好了。确保电脑上 chromedriver 和 selenium 框架都已安装好。新建一个名为 jd_selenium 的项目，然后新建一个爬虫，修改 settings.py 中的 ROBOTSTXT_OBEY = False，最后在项目根目录下新建启动文件 run.py。 定义 Item123456789import scrapyclass JdSeleniumItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() price = scrapy.Field() title = scrapy.Field() comments = scrapy.Field() 初步实现 Spider 的 start_requests() 方法，如下所示：1234567891011121314import scrapyfrom urllib.parse import quoteclass SpiderSpider(scrapy.Spider): name = 'spider' allowed_domains = ['jd.com'] base_url = 'https://search.jd.com/Search?keyword=' def start_requests(self): for keyword in self.settings.get('KEYWORDS'): for page in range(1, 100, 2): url = self.base_url.format(keyword, page) yield scrapy.Request(url, callback=self.parse, dont_filter=True) 首先定义了一个 base_url，即商品列表的 URL，其后拼接一个搜索的关键词就是该关键词在京东的搜索结果商品列表页面。 关键词用 KEYWORDS 标识，定义为一个列表。定义在 settings.py 里面，如下所示：1KEYWORDS = ['iPhone XS'] 在 start_requests() 方法里，首先遍历了关键词，再遍历了页码，构造并生成了 Request。 对接 Selenium接下来需要成功处理这些请求的抓取。这里选择对接 Selenium 进行抓取，采用 Downloader Middleware 来实现。这里浏览器使用的是 Chrome 的 headless 模式。在 Middleware 里面的 process_request() 方法里对每个抓取请求进行处理，启动浏览器并进行页面渲染，再将渲染后的结果构造一个 HtmlResponse 对象返回。代码实现如下：12345678910111213141516171819202122232425262728293031323334353637383940414243import timefrom scrapy import signalsfrom selenium import webdriverfrom selenium.common.exceptions import TimeoutExceptionfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.webdriver.chrome.options import Optionsfrom logging import getLoggerfrom scrapy.http import HtmlResponseclass SeleniumMiddleware(): # 加入了 chrome headless 模式 chrome_options = Options() chrome_options.add_argument('--headless') chrome_options.add_argument('--disable-gpu') def __init__(self): self.logger = getLogger(__name__) self.timeout = 10 self.browser = webdriver.Chrome(chrome_options=self.chrome_options) self.browser.set_page_load_timeout(self.timeout) self.wait = WebDriverWait(self.browser, self.timeout) def __del__(self): self.browser.close() def process_request(self, request, spider): """ 用 Chrome 抓取页面 :param request: Request 对象 :param spider: Spider 对象 :return: HtmlResponse """ self.logger.debug('Chrome is Starting') page = request.meta.get('page', 1) try: self.browser.get(request.url) except TimeoutException as e: print('请求超时') time.sleep(2) return HtmlResponse(url=request.url, body=self.browser.page_source, encoding='utf-8', request=request) 这样就定义好了一个 Selenium 中间件，然后在 settings.py 中设置调用 SeleniumMiddleware，如下所示：123DOWNLOADER_MIDDLEWARES = &#123; 'jd_selenium.middlewares.SeleniumMiddleware': 543,&#125; 解析页面Response 对象回传给 Spider 中的回调函数进行解析。因此这步来实现其回调函数，对网页来进行解析，代码如下所示。1234567891011def parse(self, response): selector = response.xpath("//*[@id='J_goodsList']/ul//li[@class='gl-item']") print('---------------------------------------------------') print(len(selector)) print('---------------------------------------------------') for item in selector: title = ''.join(item.xpath(".//div[4]/a/em//text()").getall()) price = ''.join(item.xpath(".//div[3]/strong//text()").getall()) comments = ''.join(item.xpath(".//div[5]/strong//text()").getall()) item = JdSeleniumItem(title=title, price=price, comments=comments) yield item 在上面代码中，对 selector 的长度进行了输出，因为看到的商品数量明显少于直接打开浏览器的商品结果，后来发现在京东输入关键词点击搜索之后，页面的返回是分成两个步骤，它首先会直接返回一个静态的页面，页面的商品信息是30个，之后，当我们鼠标向下滑动时，后台会通过 ajax 技术加载另外的30个商品，因此直接打开浏览器看到的商品列表其实是分两次加载出来的，而且只是在鼠标下滑到一定位置的时候才会加载那另外的30个商品。 执行 js 脚本因此，为了实现鼠标下滑的现象，需要 Selenium 执行一段 js 代码，将网页下拉到一定位置。 修改后的 SeleniumMiddleware 代码如下：1234567891011121314151617def process_request(self, request, spider): """ 用 Chrome headless 抓取页面 :param request: Request 对象 :param spider: Spider 对象 :return: HtmlResponse """ self.logger.debug('Chrome is Starting') page = request.meta.get('page', 1) try: self.browser.get(request.url) self.browser.execute_script('window.scrollTo(0, document.body.scrollHeight)') except TimeoutException as e: print('请求超时') self.browser.execute_script('window.stop()') time.sleep(2) return HtmlResponse(url=request.url, body=self.browser.page_source, encoding='utf-8', request=request) 代码修改后运行程序，这次选择器的长度变成了59，正常应该是60，可能是哪里出问题了。-.- 存入数据库实现一个 Item Pipeline，将结果保存到 MongoDB，如下所示。1234567891011121314151617181920import pymongofrom .settings import mongo_uri, mongo_dbclass JdSeleniumPipeline(object): def __init__(self): self.mongo_uri = mongo_uri self.mongo_db = mongo_db def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_uri) self.db = self.client[self.mongo_db] self.collection = self.db['collection'] def process_item(self, item, spider): self.collection.insert(dict(item)) return item def close_spider(self, spider): self.client.close() 编写完成需要在 settings.py 中开启调用。如下所示：123ITEM_PIPELINES = &#123; 'jd_selenium.pipelines.JdSeleniumPipeline': 300,&#125; 其中 mongo_uri 和 mongo_db 分别定义:12mongo_uri = 'localhost'mongi_db = 'jd_scrapy' 如图所示，已经将数据存入了数据库。 运行效果]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy</tag>
        <tag>Selenium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pyppeteer]]></title>
    <url>%2Fpyppeteer%2F</url>
    <content type="text"><![CDATA[Pyppeteer简介PuppeteerPuppeteer 是 Chrome 团队开发的一个 node 库，可以通过 api 来控制浏览器的行为，比如点击，跳转，刷新，在控制台执行js脚本等等。使用这个神器作为爬虫访问页面收集数据十分方便。 PyppeteerPyppeteer 是 Puppeteer 的 python 版本。中文资料可以说非常少，只能看官方文档了。不过还是可以看看 Puppeteer 的教程，功能原理相同，只是因为语言差异实现方式不同。 Pyppeteer文档: 点我 Pyppeteer安装以下内容来自 pyppeteer 文档。 Pyppeteer 需要 python3.6+，安装方式如下。 使用 pip 方式安装：1python3 -m pip install pyppeteer 使用来自 github 的最新版本：1python3 -m pip install -U git+https://github.com/miyakogi/pyppeteer.git@dev Pyppeteer使用注意： 当你首次运行 pyppeteer 时，它会下载 Chromium 浏览器的最近版本。如果你不想它这么做的话，请在使用 pyppeteer 的脚本之前运行 pyppeteer-install 命令。 示例 1 打开网页并截图。1234567891011import asynciofrom pyppeteer import launchasync def main(): browser = await launch() page = await browser.newPage() await page.goto('http://example.com') await page.screenshot(&#123;'path': 'example.png'&#125;) await browser.close()asyncio.get_event_loop().run_until_complete(main()) 示例 2 执行 js 脚本12345678910111213141516171819202122import asynciofrom pyppeteer import launchasync def main(): browser = await launch() page = await browser.newPage() await page.goto('http://example.com') await page.screenshot(&#123;'path': 'example.png'&#125;) dimensions = await page.evaluate('''() =&gt; &#123; return &#123; width: document.documentElement.clientWidth, height: document.documentElement.clientHeight, deviceScaleFactor: window.devicePixelRatio, &#125; &#125;''') print(dimensions) # &gt;&gt;&gt; &#123;'width': 800, 'height': 600, 'deviceScaleFactor': 1&#125; await browser.close()asyncio.get_event_loop().run_until_complete(main())]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>自动化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用with结构打开n个文件]]></title>
    <url>%2Fpython-with%2F</url>
    <content type="text"><![CDATA[对一个文件读写有时候在爬虫中对最后数据的处理可能是将其存入某个文件内，那么此时就可以使用 with 语句对某个文件进行操作。 写入一个文件12with open(filename1, 'w') as fp: fp.write(item) 读取一个文件12with open(filename1, 'r') as fp: data = fp.read() 对多个文件进行读写将 A 文件内容写入 B 文件123with open(A,'r') as fp1 , open(B,'a') as fp2: for line in fp1: fp2.write(line) 同时读取 A B C 三个文件同时打开三个文件，文件行数一样，要求实现每个文件依次读取一行，然后输出，我们先来看比较容易想到的写法： 1234567with open(A, 'rb') as fp1: with open(B, 'rb') as fp2: with open(C, 'rb') as fp3: for i in fp1: j = fp2.readline() k = fp3.readline() print(i, j, k) 注意这里只能对单个文件进行for循环读取，不能写成：12for i, j, k in fp1, fp2, fp3: print(i, j, k) 但可使用强大的zip操作：12for i, j, k in zip(fp1, fp2, fp3): print(i, j, k) 这样层层的嵌套未免啰嗦，with结构支持一种更简洁的写法：12345with open(A, 'rb') as fp1, open(B, 'rb') as fp2, open(C, 'rb') as fp3: for i in fp1: j = fp2.readline() k = fp3.readline() print(i, j, k) 或者使用更为优雅的写法，此时需要 contextlib 语法糖：12345from contextlib improt ExitStackwith ExitStack() as stack: files = [stack.enter_context(open(fname)) for fname in [A, B, C]] for i, j, k in zip(files[0], files[1], files[2]): print(i, j, k)]]></content>
  </entry>
  <entry>
    <title><![CDATA[mysql]]></title>
    <url>%2Fmysql%2F</url>
    <content type="text"><![CDATA[教程节选自 w3school 的 sql教程 SQL 语法数据库表一个数据库通常包含一个或多个表。每个表由一个名字标识（例如“客户”或者“订单”）。表包含带有数据的记录（行）。 下面的例子是一个名为 “Persons” 的表： Id LastName FirstName Address City 1 Adams John Oxford Street London 2 Bush George Fifth Avenue New York 3 Carter Thomas Changan Street Beijing 上面的表包含三条记录（每一条对应一个人）和五个列（Id、姓、名、地址和城市）。 SQL 语句您需要在数据库上执行的大部分工作都由 SQL 语句完成。 下面的语句从表中选取 LastName 列的数据： 1SELECT LastName FROM Persons 结果集类似这样： LastName Adams Bush Carter SQL DML 和 DDL可以把 SQL 分为两个部分：数据操作语言 (DML) 和 数据定义语言 (DDL)。SQL (结构化查询语言)是用于执行查询的语法。但是 SQL 语言也包含用于更新、插入和删除记录的语法。 查询和更新指令构成了 SQL 的 DML 部分： SELECT - 从数据库表中获取数据 UPDATE - 更新数据库表中的数据 DELETE - 从数据库表中删除数据 INSERT INTO - 向数据库表中插入数据 SQL 的数据定义语言 (DDL) 部分使我们有能力创建或删除表格。我们也可以定义索引（键），规定表之间的链接，以及施加表间的约束。 SQL 中最重要的 DDL 语句: CREATE DATABASE - 创建新数据库 ALTER DATABASE - 修改数据库 CREATE TABLE - 创建新表 ALTER TABLE - 变更（改变）数据库表 DROP TABLE - 删除表 CREATE INDEX - 创建索引（搜索键） DROP INDEX - 删除索引 SQL SELECT 语句SELECT 语句用于从表中选取数据。 结果被存储在一个结果表中（称为结果集）。 SQL SELECT 语法1SELECT 列名称 FROM 表名称 以及：1SELECT * FROM 表名称 注释：SQL 语句对大小写不敏感。SELECT 等效于 select。 SQL SELECT 实例如需获取名为 “LastName” 和 “FirstName” 的列的内容（从名为 “Persons” 的数据库表），请使用类似这样的 SELECT 语句：1SELECT LastName,FirstName FROM Persons “Persons” 表 Id LastName FirstName Address City 1 Adams John Oxford Street London 2 Bush George Fifth Avenue New York 3 Carter Thomas Changan Street Beijing 结果： LastName FirstName Adams John Bush George Carter Thomas SQL SELECT * 实例现在我们希望从 “Persons” 表中选取所有的列。 请使用符号 * 取代列的名称，就像这样：1SELECT * FROM Persons 提示：星号（ * ）是选取所有列的快捷方式。 结果： Id LastName FirstName Address City 1 Adams John Oxford Street London 2 Bush George Fifth Avenue New York 3 Carter Thomas Changan Street Beijing SQL SELECT DISTINCT 语句在表中，可能会包含重复值。这并不成问题，不过，有时您也许希望仅仅列出不同（distinct）的值。 关键词 DISTINCT 用于返回唯一不同的值。 语法：1SELECT DISTINCT 列名称 FROM 表名称 使用 DISTINCT 关键词如果要从 “Company” 列中选取所有的值，我们需要使用 SELECT 语句：1SELECT Company FROM Orders “Orders”表 Company OrderNumber IBM 3532 W3School 2356 Apple 4698 W3School 6953 结果： Company IBM W3School Apple W3School 请注意，在结果集中，W3School 被列出了两次。 如需从 Company” 列中仅选取唯一不同的值，我们需要使用 SELECT DISTINCT 语句：1SELECT DISTINCT Company FROM Orders 结果： Company IBM W3School Apple 现在，在结果集中，”W3School” 仅被列出了一次。 SQL WHERE 子句如需有条件地从表中选取数据，可将 WHERE 子句添加到 SELECT 语句。 语法：1SELECT 列名称 FROM 表名称 WHERE 列 运算符 值 下面的运算符可在 WHERE 子句中使用： 操作符 描述 = 等于 &lt;&gt; 不等于 &gt; 大于 &lt; 小于 &gt;= 大于等于 &lt;= 小于等于 BETWWEEN 在某个范围内 LIKE 搜索某种模式 注释：在某些版本的 SQL 中，操作符 &lt;&gt; 可以写为 !=。 使用 WHERE 子句如果只希望选取居住在城市 “Beijing” 中的人，我们需要向 SELECT 语句添加 WHERE 子句：1SELECT * FROM Persons WHERE City='Beijing' “Persons” 表 LastName FirstName Address City Year Adams John Oxford Street London 1970 Bush George Fifth Avenue New York 1975 Carter Thomas Changan Street Beijing 1980 Gates Bill Xuanwumen 10 Beijing 1985 结果： LastName FirstName Address City Year Carter Thomas Changan Street Beijing 1980 Gates Bill Xuanwumen 10 Beijing 1985 引号的使用请注意，我们在例子中的条件值周围使用的是单引号。 SQL 使用单引号来环绕文本值（大部分数据库系统也接受双引号）。如果是数值，请不要使用引号。 文本值：12345这是正确的：SELECT * FROM Persons WHERE FirstName='Bush'这是错误的：SELECT * FROM Persons WHERE FirstName=Bush 数值:12345这是正确的：SELECT * FROM Persons WHERE Year&gt;1965这是错误的：SELECT * FROM Persons WHERE Year&gt;'1965' SQL AND &amp; OR 运算符AND 和 OR 运算符用于基于一个以上的条件对记录进行过滤。 AND 和 OR 可在 WHERE 子语句中把两个或多个条件结合起来。 如果第一个条件和第二个条件都成立，则 AND 运算符显示一条记录。 如果第一个条件和第二个条件中只要有一个成立，则 OR 运算符显示一条记录。 原始的表 (用在例子中的)： LastName FirstName Address City Adams John Oxford Street London Bush George Fifth Avenue New York Carter Thomas Changan Street Beijing Carter William Xuanwumen 10 Beijing AND 运算符实例使用 AND 来显示所有姓为 “Carter” 并且名为 “Thomas” 的人：1SELECT * FROM Persons WHERE FirstName='Thomas' AND LastName='Carter' 结果： LastName FirstName Address City Carter Thomas Changan Street Beijing OR 运算符实例使用 OR 来显示所有姓为 “Carter” 或者名为 “Thomas” 的人：1SELECT * FROM Persons WHERE firstname='Thomas' OR lastname='Carter' 结果： LastName FirstName Address City Carter Thomas Changan Street Beijing Carter William Xuanwumen 10 Beijing 结合 AND 和 OR 运算符我们也可以把 AND 和 OR 结合起来（使用圆括号来组成复杂的表达式）:12SELECT * FROM Persons WHERE (FirstName='Thomas' OR FirstName='William')AND LastName='Carter' 结果： LastName FirstName Address City Carter Thomas Changan Street Beijing Carter William Xuanwumen 10 Beijing SQL ORDER BY 子句ORDER BY 语句用于对结果集进行排序。 ORDER BY 语句用于根据指定的列对结果集进行排序。 ORDER BY 语句默认按照升序对记录进行排序。 如果您希望按照降序对记录进行排序，可以使用 DESC 关键字。 原始的表 (用在例子中的)： Orders 表: Company OrderNumber IBM 3532 W3School 2356 Apple 4698 W3School 6953 实例 1以字母顺序显示公司名称：1SELECT Company, OrderNumber FROM Orders ORDER BY Company 结果： Company OrderNumber Apple 4698 IBM 3532 W3School 6953 W3School 2356 实例 2以字母顺序显示公司名称（Company），并以数字顺序显示顺序号（OrderNumber）：1SELECT Company, OrderNumber FROM Orders ORDER BY Company, OrderNumber 结果： Company OrderNumber Apple 4698 IBM 3532 W3School 2356 W3School 6953 实例 3以逆字母顺序显示公司名称：1SELECT Company, OrderNumber FROM Orders ORDER BY Company DESC 结果： Company OrderNumber W3School 6953 W3School 2356 IBM 3532 Apple 4698 实例 4以逆字母顺序显示公司名称，并以数字顺序显示顺序号：1SELECT Company, OrderNumber FROM Orders ORDER BY Company DESC, OrderNumber ASC 结果： Company OrderNumber W3School 2356 W3School 6953 IBM 3532 Apple 4698 注意： 在以上的结果中有两个相等的公司名称 (W3School)。只有这一次，在第一列中有相同的值时，第二列是以升序排列的。如果第一列中有些值为 nulls 时，情况也是这样的。 SQL INSERT INTO 语句INSERT INTO 语句用于向表格中插入新的行。 语法1INSERT INTO 表名称 VALUES (值1, 值2,....) 我们也可以指定所要插入数据的列：1INSERT INTO 表名称 (列1, 列2,...) VALUES (值1, 值2,....) 插入新的行“Persons” 表 LastName FirstName Address City Carter Thomas Changan Street Beijing SQL 语句1INSERT INTO Persons VALUES ('Gates', 'Bill', 'Xuanwumen 10', 'Beijing') 结果 LastName FirstName Address City Carter Thomas Changan Street Beijing Gates Bill Xuanwumen 10 Beijing 在指定的列中插入数据“Persons” 表 LastName FirstName Address City Carter Thomas Changan Street Beijing Gates Bill Xuanwumen 10 Beijing SQL 语句1INSERT INTO Persons (LastName, Address) VALUES ('Wilson', 'Champs-Elysees') 结果 LastName FirstName Address City Carter Thomas Changan Street Beijing Gates Bill Xuanwumen 10 Beijing Wilson Champs-Elysees ——- SQL UPDATE 语句Update 语句用于修改表中的数据。 语法1UPDATE 表名称 SET 列名称 = 新值 WHERE 列名称 = 某值 “Persons” 表 LastName FirstName Address City Carter Thomas Changan Street Beijing Wilson Champs-Elysees ——- 更新某一行中的一个列我们为 lastname 是 “Wilson” 的人添加 firstname：1UPDATE Person SET FirstName = 'Fred' WHERE LastName = 'Wilson' 结果 LastName FirstName Address City Carter Thomas Changan Street Beijing Wilson Fred Champs-Elysees ——- 更新某一行中的若干列我们会修改地址（address），并添加城市名称（city）：12UPDATE Person SET Address = 'Zhongshan 23', City = 'Nanjing'WHERE LastName = 'Wilson' 结果 LastName FirstName Address City Carter Thomas Changan Street Beijing Wilson Fred Zhongshan 23 Nanjing SQL DELETE 语句DELETE 语句用于删除表中的行。 语法1DELETE FROM 表名称 WHERE 列名称 = 值 “Persons” 表 LastName FirstName Address City Carter Thomas Changan Street Beijing Wilson Fred Zhongshan 23 Nanjing 删除某行“Fred Wilson” 会被删除：1DELETE FROM Person WHERE LastName = 'Wilson' 结果 LastName FirstName Address City Carter Thomas Changan Street Beijing 删除所有行可以在不删除表的情况下删除所有的行。这意味着表的结构、属性和索引都是完整的： 1DELETE FROM table_name 或者：1DELETE * FROM table_name]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[域名ssl证书部署到服务器]]></title>
    <url>%2Fssl%2F</url>
    <content type="text"><![CDATA[申请 SSL 证书首先进入腾讯云 SSL域名管理，点击按钮申请证书，选择第一个免费证书，我这里已经申请过了，简单演示一下，输入绑定域名，邮箱然后选填的都可以不填，直接下一步，如果域名已经绑定了服务器就选择自动，否则选择手动，最后一个不用管。 提交之后耐心等待，审核通过腾讯云官方会发送邮件提醒。 下载 SSL 证书申请成功后，点击右边的下载按钮下载证书文件。 下载解压后，文件中有几个子文件夹，分别是 Apache、IIS、Nginx 服务器的证书文件。证书安装指引 Nginx 证书部署Nginx 文件夹内获得 SSL 证书文件 1_www.domain.com_bundle.crt 和私钥文件2_www.domain.com.key 。 1_www.domain.com_bundle.crt 文件包括两段证书代码 “—–BEGIN CERTIFICATE—–” 和 “—–END CERTIFICATE—–”，2_www.domain.com.key 文件包括一段私钥代码 “—–BEGIN RSA PRIVATE KEY—–” 和 “—–END RSA PRIVATE KEY—–”。 将这两个文件上传到服务器中，将域名 www.domain.com 的证书文件 1_www.domain.com_bundle.crt 、私钥文件 2_www.domain.com.key 保存到同一个目录，例如 /usr/local/nginx 目录下。 修改 Nginx 根目录下 conf/nginx.conf 文件，内容如下：123456789101112131415server &#123; listen 443; server_name www.domain.com; #填写绑定证书的域名 ssl on; ssl_certificate 1_www.domain.com_bundle.crt; ssl_certificate_key 2_www.domain.com.key; ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; #按照这个协议配置 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE;#按照这个套件配置 ssl_prefer_server_ciphers on; location / &#123; root html; #站点目录 index index.html index.htm; &#125; &#125; 配置完成后，请先执行命令 nginx –t 测试 Nginx 配置是否有误。若无报错，重启 Nginx 之后，即可使用 https://www.domain.com 来访问。 如果这里你的 https 域名已经可以正常访问了，那就不用往下看了。 问题我重启了 Nginx 服务器后，重新使用 https 访问我的网址，可以正常访问，但是直接输入我的网址域名，不加 http 或者 https，网页提示“400 Bad Request: The plain HTTP request was sent to HTTPS port”，如图。 引自 https://blog.csdn.net/system1024/article/details/52636147 解决方法是修改服务器的 /usr/local/nginx/nginx.conf 文件如下。12345678910111213server &#123; listen 80 default_server; listen 443 ssl; # 443 后面加上 ssl #ssl on; # 删除此行 server_name tangx1.com; root /usr/share/nginx/html; ssl_certificate 1_tangx1.com_bundle.crt; ssl_certificate_key 2_tangx1.com.key; ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE; ssl_prefer_server_ciphers on; &#125;]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>服务器</tag>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中的网络编程]]></title>
    <url>%2Fsocket%2F</url>
    <content type="text"><![CDATA[socket() 模块函数要创建套接字，必须使用 socket.socket() 函数，它一般的语法如下。1socket(socket_family, socket_type, protocol=0) 其中， socket_family 是 AF_UNIX 或 AF_INET， socket_type 是 SOCK_STREAM 或 SOCK_DGRAM 。protocol 通常省略，默认为 0 。 所以，为了创建 TCP/IP 套接字，可以用下面的方式调用 socket.socket()。1tcpSock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) 同样，为了创建 UDP/IP 套接字，需要执行以下语句。1udpSock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) 因为有很多 socket 模块属性，所以此时可以使用下面的语句来导入 socket 模块。1from socket import * 然后可以创建套接字对象，再之后就可以使用套接字对象的方法进行进一步的交互。1tcpSock = socket(AF_INET, SOCK_STREAM) UDP 客户端与服务器的创建UDP 客户端的创建创建一个套接字对象，代码如下12from socket import *udpSock = socket(AF_INET, SOCK_DGRAM) 然后使用 sendto() 方法可以发送信息，代码如下123data = input("请输入你想要发送的信息：")# sendto(data, (ip, port))udpSock.sendto(data.encode('gb2312'), ('172.16.217.129', 8080)) 这里使用 mac 向 windows 发送一段信息，由于 windows 软件以 gb2312 解码，因此在 Python 代码中需要把要发送的信息编码成 gb2312，如图。 UDP 服务器的创建同上先创建一个套接字对象，和创建客户端不同，服务器需要绑定到某一个地址上，只有这样客户端才知道如何给服务器发送信息。1234567from socket import *udpSock = socket(AF_INET, SOCK_DGRAM)# 绑定到 7788 端口udpSock.bind(("", 7788))udpSock.recvfrom(1024)content, ip = udpdataprint(ip, ":", content.decode("gb2312")) 这次使用 windows 向 mac 发送信息，如图。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy模拟登录豆瓣网进阶篇]]></title>
    <url>%2Fscrapy-login-captcha%2F</url>
    <content type="text"><![CDATA[内容和上一篇大致差不多，重点说一下接入第三方验证码识别平台的问题。 验证码识别平台我使用的是 超级鹰 ，其是全球领先的智能图片分类及识别商家 ，具有安全、准确、高效、稳定、开放的特点，并且拥有强大的技术及校验团队。 超级鹰的开发文档: API地址 为了方便，这里将 Python 版本的代码贴在下面，以后还用的上。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#!/usr/bin/env python# coding:utf-8import requestsfrom hashlib import md5class Chaojiying_Client(object): def __init__(self, username, password, soft_id): self.username = username password = password.encode('utf8') self.password = md5(password).hexdigest() self.soft_id = soft_id self.base_params = &#123; 'user': self.username, 'pass2': self.password, 'softid': self.soft_id, &#125; self.headers = &#123; 'Connection': 'Keep-Alive', 'User-Agent': 'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)', &#125; def PostPic(self, im, codetype): """ im: 图片字节 codetype: 题目类型 参考 http://www.chaojiying.com/price.html """ params = &#123; 'codetype': codetype, &#125; params.update(self.base_params) files = &#123;'userfile': ('ccc.jpg', im)&#125; r = requests.post('http://upload.chaojiying.net/Upload/Processing.php', data=params, files=files, headers=self.headers) return r.json() def ReportError(self, im_id): """ im_id:报错题目的图片ID """ params = &#123; 'id': im_id, &#125; params.update(self.base_params) r = requests.post('http://upload.chaojiying.net/Upload/ReportError.php', data=params, headers=self.headers) return r.json()if __name__ == '__main__': chaojiying = Chaojiying_Client('超级鹰用户名', '超级鹰用户名的密码', '96001') #用户中心&gt;&gt;软件ID 生成一个替换 96001 im = open('a.jpg', 'rb').read() #本地图片文件路径 来替换 a.jpg 有时WIN系统须要// print chaojiying.PostPic(im, 1902) #1902 验证码类型 官方网站&gt;&gt;价格体系 3.4+版 print 后要加() 只要把开发文档中对应的 soft_id 和 codetype 填上去，将验证码图片保存到本地，就可以使用超级鹰识别验证码了。话不多说，代码如下。 完整代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980class LoginSpider(scrapy.Spider): name = 'login' allowed_domains = ['douban.com'] start_urls = ['https://www.douban.com/people/xxxxxx/'] edit_signature_url = 'https://www.douban.com/j/people/xxxxxx/edit_signature' headers = &#123; 'Connection': 'Keep-Alive', 'User-Agent': 'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)', &#125; def parse(self ,response): if response.url == 'https://www.douban.com/people/xxxxxx/': print('-----已经进入个人详情页-----') print('-----正在修改个人签名-----') ck = response.xpath("//*[@id='edit_signature']/form/div/input/@value").get() data = &#123; 'ck': ck, 'signature': '我可以自动识别验证码啦~~' &#125; yield FormRequest(self.edit_signature_url, formdata=data) # 模拟登录 login_url = 'https://accounts.douban.com/login' def start_requests(self): yield Request(self.login_url, callback=self.login) def login(self, response): print('-----登录程序-----') captcha_id = response.xpath(".//input[@name='captcha-id']/@value").get() captcha_url = response.xpath("//*[@id='captcha_image']/@src").get() if captcha_url is None: print('-----登录时无验证码-----') data = &#123; 'form_email': 'xxxxxx@qq.com', 'form_password': 'xxxxxx' &#125; else: print('-----登录时有验证码-----') print('-----即将下载验证码-----') request.urlretrieve(captcha_url, 'captcha.png',) captcha_solution = self.recognize_captcha('captcha.png') data = &#123; 'form_email': 'xxxxxx@qq.com', 'form_password': 'xxxxxx', 'captcha-solution': captcha_solution, 'captcha-id': captcha_id, 'login': '登录' &#125; print('-----登录中-----') yield FormRequest.from_response(response, formdata=data, callback=self.parse_after_login) def parse_after_login(self, response): if "xxxxxx的帐号" in response.text: yield from super().start_requests() def recognize_captcha(self, im): print('-----正在进行验证码识别-----') username = 'xxxxxx' password = 'xxxxxx'.encode('utf-8') password = md5(password).hexdigest() soft_id = '898320' codetype = '1007' base_params = &#123; 'user': username, 'pass2': password, 'softid': soft_id, 'codetype': codetype &#125; im = open(im, 'rb').read() files = &#123;'userfile': ('ccc.jpg', im)&#125; r = requests.post('http://upload.chaojiying.net/Upload/Processing.php', data=params, files=files, headers=self.headers) captcha = r.json()['pic_str'] print('-----验证码识别完毕-----') print('验证码为：%s' % captcha) return captcha]]></content>
      <tags>
        <tag>爬虫</tag>
        <tag>Python</tag>
        <tag>Scrapy</tag>
        <tag>验证码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy模拟登录豆瓣网初级篇]]></title>
    <url>%2Fscrapy_login%2F</url>
    <content type="text"><![CDATA[在进行模拟登录之前，应该先对网站登录的原理有所了解，首先在 Chrome 浏览器中进行一次实际的登录操作，再来观察浏览器和网站服务器是如何交互的。 在这里我使用 豆瓣网 作为此次模拟登录的示例。 首先打开 F12 开发者模式，在登录表单中输入用户名和密码(这里我的验证码是因为我尝试次数过多出现)，点击登录按钮，观察控制台中 Network 下第一条请求，其为一条 post 请求，且参数在图中有所展示，那么如果需要模拟登录，就需要对这些参数进行构造。 登录的核心其实就是向服务器发送含有登录表单数据的 HTTP 请求(通常是 POST)，在 Scrapy 中提供了一个 FormRequest 类(Request的子类)，专门用于构造含有登录表单的请求，FormRequest 的构造器方法有一个 formdata 参数，接收字典形式的表单数据。 在本篇文章中，我先模拟登录到网站后，跳转至个人中心，然后修改我的个人签名。 模拟登录要构造 post 请求的参数，来看上图参数中 source, redir 和 login 都是固定值，form_email, form_password 分别为用户名和密码，captcha-solution 是图片验证码的字符，captcha-id 就先去网页源代码中寻找，如下图。 在这里我将 start_urls 设置为我的个人详情页，模拟登录这里需要重写 start_requests 方法，因为如果不去重写这个方法，那么 scrapy 就会对我的个人详情页直接进行请求。 FormRequest 的 from_response 方法需传入一个 Response 对象作为第一个参数，该方法会解析 Response 对象所包含的 form 元素，帮助用户创建 FormRequest 对象，并将隐藏 input 中的信息自动填入表单数据。使用这种方法，只需通过 formdata 参数填写账号和密码即可。这里使用 PIL 的 Image 方法将图片展示出来，人工识别并输入到程序中，程序继续进行登录。 123456789101112131415161718192021222324252627282930313233343536# 模拟登录login_url = 'https://accounts.douban.com/login'def start_requests(self): yield Request(self.login_url, callback=self.login)def login(self, response): print('-----登录程序-----') captcha_id = response.xpath(".//input[@name='captcha-id']/@value").get() captcha_url = response.xpath("//*[@id='captcha_image']/@src").get() if captcha_url is None: print('-----登录时无验证码-----') data = &#123; 'form_email': 'xxxxxx@qq.com', 'form_password': 'xxxxxx' &#125; else: print('-----登录时有验证码-----') print('-----即将下载验证码-----') # 使用urllib 的 urlretrieve 直接下载验证码图片到本地 request.urlretrieve(captcha_url, 'captcha.png',) try: image = Image('captcha.png') image.show() except Exception as e: pass captcha_solution = input("请输入图片中的验证码") data = &#123; 'form_email': 'xxxxxx@qq.com', 'form_password': 'xxxxxx', 'captcha-solution': captcha_solution, 'captcha-id': captcha_id, 'login': '登录' &#125; print('-----登录中-----') yield FormRequest.from_response(response, formdata=data, callback=self.parse_after_login) 在 login 函数中，最后的 FormRequest 的回调函数是 parse_after_login 函数，代码如下。 123def parse_after_login(self, response): if "xxxxxxx的帐号" in response.text: print("-----登录成功-----") 修改签名在登录成功之后，需要先跳转到我的个人详情页面，再进行修改签名操作。 先手动修改签名一次，观察浏览器的请求过程，如图所示。点击修改后，浏览器中这条 POST 请求的 formdata 只有两个参数，一个 signature 就是我们正在修改的签名。 另一个是 ck 参数，ck 参数在网页源代码中同样可以找到，如下图。 有了修改签名的两个参数，我们就可以构造修改签名的 FormRequest 了。 1234567891011121314def parse_after_login(self, response): if "xxxxxxx的帐号" in response.text: yield from super().start_requests()def parse(self, response): if response.url == 'https://www.douban.com/people/xxxxxxx/': print('-----已经进入个人详情页-----') print('-----正在修改个人签名-----') ck = response.xpath("//*[@id='edit_signature']/form/div/input/@value").get() data = &#123; 'ck': ck, 'signature': '我是 scrapy 修改的~~' &#125; yield FormRequest(self.edit_signature_url, formdata=data) 完整代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# -*- coding: utf-8 -*-import scrapyfrom scrapy.http import Request, FormRequestimport requestsfrom urllib import requestfrom PIL import Imageclass LoginSpider(scrapy.Spider): name = 'login' allowed_domains = ['douban.com'] start_urls = ['https://www.douban.com/people/xxxxxx/'] edit_signature_url = 'https://www.douban.com/j/people/xxxxxx/edit_signature' headers = &#123; 'Connection': 'Keep-Alive', 'User-Agent': 'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)', &#125; def parse(self ,response): if response.url == 'https://www.douban.com/people/xxxxxx/': print('-----已经进入个人详情页-----') print('-----正在修改个人签名-----') ck = response.xpath("//*[@id='edit_signature']/form/div/input/@value").get() data = &#123; 'ck': ck, 'signature': '我是 scrapy 修改的~~' &#125; yield FormRequest(self.edit_signature_url, formdata=data, callback=self.success) def success(self, response): print('-----个人签名修改成功-----') # 模拟登录 login_url = 'https://accounts.douban.com/login' def start_requests(self): yield Request(self.login_url, callback=self.login) def login(self, response): print('-----登录程序-----') captcha_id = response.xpath(".//input[@name='captcha-id']/@value").get() captcha_url = response.xpath("//*[@id='captcha_image']/@src").get() if captcha_url is None: print('-----登录时无验证码-----') data = &#123; 'form_email': 'xxxxxx@qq.com', 'form_password': 'xxxxxx' &#125; else: print('-----登录时有验证码-----') print('-----即将下载验证码-----') request.urlretrieve(captcha_url, 'captcha.png',) image = Image.open('captcha.png') image.show() captcha_solution = input("请输入验证码:") # captcha_solution = self.recognize_captcha('captcha.png') data = &#123; 'form_email': 'xxxxxx@qq.com', 'form_password': 'xxxxxx', 'captcha-solution': captcha_solution, 'captcha-id': captcha_id, 'login': '登录' &#125; print('-----登录中-----') yield FormRequest.from_response(response, formdata=data, callback=self.parse_after_login) def parse_after_login(self, response): if "xxxxxx的帐号" in response.text: print('-----登录成功-----') yield from super().start_requests()]]></content>
      <tags>
        <tag>爬虫</tag>
        <tag>Python</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[七牛云数据迁移至网易nos]]></title>
    <url>%2Fqiniu%2F</url>
    <content type="text"><![CDATA[使用 qshell 将 bucket 文件迁移到新 bucket 中安装 qshell 工具。下载地址：qshell 官方文档将下载下来的压缩文件解压到任意目录，将其中的名字为 darwin 的文件重命名为 qshell 并放入 mac 的 /usr/local/bin 目录，最后在命令行输入 qshell 。 密钥设置这里需要使用七牛账号中个人密钥管理下的 AccessKey 和 SecretKey 。 找到这两个值后在命令行中输入如下命令。(其中 ak 和 sk 分别对应 AccessKey 和 SecretKey ， name 为账户名称)1$ qshell account ak sk name 如果没有报错的话，输入以下命令来显示账号信息。1$ qshell account 迁移文件使用如下命令进行文件迁移并生成包含文件名的文本文件。 12$ qshell listbucket &#123; bucket_name &#125; | awk -F"\t" '&#123;print $1&#125;' &gt; files.txt$ qshell batchcopy &#123; bucket_name &#125; &#123; new_bucket_name &#125; -i files.txt 批量下载文件到本地上面只是将旧桶内的文件转移到新的桶内，原本无法预览、下载的文件现在都可以正常操作了，可是里面有很多图片总不能从网页上一张一张下载下来然后再上传到其他的储存空间里吧，因此这里就需要思考如何将图片批量下载到本地磁盘。七牛云的 qshell 工具有 qdownload 方法能批量下载文件，但是尝试多次均以失败告终，后看到 qshell 的 get 方法只能每次操作一个文件，如果让电脑代替人工去重复操作这一 get 方法，就可以把每一张图片都下载下来。 python 脚本代码如下：1234567import osimport linecachefor num in range(1, total_num): result = linecache.getline('./files.txt', num).strip() print(result) os.system('/usr/local/bin/qshell get test ' + result) 上传至网易 nos网易nos 的注册与创建桶的过程不再过多介绍，需要注意的是在创建储存桶完毕之后要进行两个关键的配置。 存储桶（bucket）访问权限 防盗链设置 1、访问权限 访问权限应设置为公有读。如图中解释： 2、防盗链设置 为了保护自己的免费额度，防止图片被他人盗用，因此需要开启防盗链。]]></content>
      <categories>
        <category>日常</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[PHP框架--Laravel]]></title>
    <url>%2Flaravel_admin%2F</url>
    <content type="text"><![CDATA[Laravel-Admin 是一个帮我们快速建立后台管理的工具。它提供了页面组件和表单元素等功能，而且还有很多附加功能，同时也支持我们去自定义一些插件，非常方便。 由于之前没有接触过 PHP这门语言, 所以一开始接手项目时先看了看框架的代码，果然看不懂。回头看了看 PHP 的基础语法，再去简单做了一下 Laravel 的 demo ，开始慢慢理解了某些语法。 下面是我遇到的几个小问题，记录下来以作参考。 switch状态值因为 switch 组件默认存入数据库的状态是[ 开-&gt;1, 关-&gt;0 ]，但是由于 0 会影响数据判断的正确性和安全性，因此需要把 0 和 1 的状态改为 1 和 2，由于需要操作的按钮在表格页，也就是 Grid 页，而 Grid 页的操作都是由 Form 页传递过去的，因此只要找到 Form 中对应的 switch 操作数据库的地方就可以修改入库状态值了。经过一层一层的溯源，先找到 Form.php 文件，查看可使用的操作，发现了 SwitchField 关键词，对其进行查找，找到了 SwitchField.php 文件，其中有一段代码是用来写开关入库的状态值的，对此进行修改后通过测试。 1234protected $states = [ 'on' =&gt; ['value' =&gt; 1, 'text' =&gt; 'ON', 'color' =&gt; 'primary'], 'off' =&gt; ['value' =&gt; 2, 'text' =&gt; 'OFF', 'color' =&gt; 'default'], ]; switch控制多个页面有几个相同的表结构，被显示在不同的页面，页面结构完全相同，因此为了节省操作量，需要用一个页面的 swtich 开关去控制多个页面的状态。Google 后发现了解决方案。 12345678910public static function boot()&#123; parent::boot(); static::saving(function ($model) &#123; // 从$model取出数据并进行处理 &#125;);&#125;]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python算法--快速排序]]></title>
    <url>%2Fquick_sort%2F</url>
    <content type="text"><![CDATA[快速排序（Quicksort），又称划分交换排序（partition-exchange sort），通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。 步骤为： 1、从数列中挑出一个元素，称为”基准”（pivot） 2、重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作 3、递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序 递归的最底部情形，是数列的大小是零或一，也就是永远都已经被排序好了。虽然一直递归下去，但是这个算法总会结束，因为在每次的迭代（iteration）中，它至少会把一个元素摆到它最后的位置去。 过程分析 快速排序演示 代码123456789101112131415161718192021222324252627def quick_sort(alist, start, end): '''快速排序''' if start &gt;= end: return mid = alist[start] low = start high = end while low &lt; high: while low &lt; high and alist[high] &gt;= mid: high -= 1 alist[low] = alist[high] while low &lt; high and alist[low] &lt; mid: low += 1 alist[high] = alist[low] alist[low] = mid quick_sort(alist, start, low-1) quick_sort(alist, low+1, end) return alistif __name__ == '__main__': alist = [26, 54, 15, 57, 6] print(alist) print(quick_sort(alist, 0, len(alist)-1)) 时间复杂度 最优时间复杂度：O(nlogn) 最坏时间复杂度：O(n2) 稳定性：不稳定]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python算法--插入排序]]></title>
    <url>%2Finsert_sort%2F</url>
    <content type="text"><![CDATA[插入排序（英语：Insertion Sort）是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。插入排序在实现上，在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。 过程分析 代码12345678910111213def insert_sort(alist): n = len(alist) for i in range(1, n): for j in range(i, 0, -1): if alist[j] &lt; alist[j-1]: alist[j], alist[j-1] = alist[j-1], alist[j] return alistif __name__ == "__main__": alist = [26, 54, 15, 57, 6] print(alist) print(insert_sort(alist)) 时间复杂度 最优时间复杂度：O(n) （升序排列，序列已经处于升序状态） 最坏时间复杂度：O(n2) 稳定性：稳定 插入排序演示]]></content>
  </entry>
  <entry>
    <title><![CDATA[Python算法--选择排序]]></title>
    <url>%2Fselect_sort%2F</url>
    <content type="text"><![CDATA[选择排序（ Selection sort ）是一种简单直观的排序算法。它的工作原理如下。首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 选择排序的主要优点与数据移动有关。如果某个元素位于正确的最终位置上，则它不会被移动。选择排序每次交换一对元素，它们当中至少有一个将被移到其最终位置上，因此对n个元素的表进行排序总共进行至多n-1次交换。在所有的完全依靠交换去移动元素的排序方法中，选择排序属于非常好的一种。 过程分析 选择排序演示 代码123456789101112131415def select_sort(alist): n = len(alist) for i in range(n-1): min_index = i for j in range(i+1, n): if alist[j] &lt; alist[min_index]: min_index = j alist[i], alist[min_index] = alist[min_index], alist[i] return alistif __name__ == '__main__': alist = [26, 54, 15, 57, 6] print(alist) print(selection_sort(alist)) 时间复杂度 最优时间复杂度：O(n2) 最坏时间复杂度：O(n2) 稳定性：不稳定]]></content>
      <categories>
        <category>算法</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[冒泡排序]]></title>
    <url>%2Fbubble_sort%2F</url>
    <content type="text"><![CDATA[冒泡排序（Bubble Sort）是一种简单的排序算法。它重复地遍历要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。遍历数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 冒泡排序算法的运作如下： 比较相邻的元素。如果第一个比第二个大（升序），就交换他们两个。 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。 过程分析交换过程图示(第一次)： 那么我们需要进行 n-1 次冒泡过程，每次对应的比较次数如下图所示： 冒泡排序演示 代码12345678910111213def bubble_sort(alist): n = len(alist) for j in range(n-1): for i in range(n-1-j): if alist[i] &gt; alist[i+1]: alist[i], alist[i+1] = alist[i+1], alist[i] return alistif __name__ == '__main__': alist = [26, 54, 15, 57, 6] print(alist) print(bubble_sort(alist)) 时间复杂度 最优时间复杂度：O(n) （表示遍历一次发现没有任何可以交换的元素，排序结束。） 最坏时间复杂度：O(n2) 稳定性：稳定]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫--Scrapy爬取简书全站文章]]></title>
    <url>%2Fjianshu%2F</url>
    <content type="text"><![CDATA[最近学习了 scrapy ，之前刚开始爬虫的时候有接触过这个框架，当时看了下工作原理有点难懂，现在慢慢地接触爬虫多了，回过头来开始了解爬虫框架，现在再来看它的工作流程就明白了很多。 本篇文章使用 scrapy 来爬取简书全站文章。 scrapy 工程创建与配置步骤(个人习惯)： 创建 scrapy 工程，创建启动文件 start.py ，修改 settings.py 配置文件 进入 spider.py 文件开始写爬虫规则 item.py 中设置存储模板 写 pipeline 存入数据库 创建 scrapy 工程windows 系统在 scrapy 工程文件根目录：打开命令行工具，输入命令创建工程。1$ scrapy startproject [工程名字] cd 到工程文件夹下，创建爬虫文件，默认使用 basic 模板，同样在命令行中。输入1$ scrapy genspider [爬虫名字] [爬虫网址] 这样就完成了一个 scrapy 工程的创建。但是这里爬取简书全站我使用的是 crawlspider 爬虫，其有可编写的爬虫规则，使用起来比较方便。 为了方便启动工程，我都会在创建好 scrapy 后再来创建一个启动文件 start.py 。 修改 settings.py 文件，将其中的遵守 robots.txt 协议关闭，开启 headers 其它配置等需要的时候再去更改。 进入 spider.py 写爬虫规则这次爬取的是简书全站的文章，因此要找所有文章的链接规则，每篇文章阅读到最底部，简书会推荐给我们一些其它文章，几乎每篇文章下面都会有推荐，因此我们从这里入手，查看了源代码，发现了它们的链接形式都大致相同，https://www.jianshu.com/p/7a4879ef6f8d ，https://www.jianshu.com/p/cde1742518c8 ，如图。 可以看到，都是 p 后面接上一大串数字字母的混合字符串，因此可以写出它的规则，使用正则表达式，如下。123rules = ( Rule(LinkExtractor(allow=r'.+/p/[a-z0-9].+'), callback='parse_detail', follow=True), ) rules 是一个元组，其中，Rule 写的是爬虫的规则；callback 指的是回调函数，也就是当获取到了前面取到的 url 之后，程序该去调用哪一个函数的操作，而这里就是去调用 parse_detail 这个函数； follow 表示跟进，如果其 ==True 表示要继续跟进，也就是我们进入一片文章之后，要继续跟进下一篇文章。 在进入一篇文章之后，我们要获取到它的标题，发布者，发布时间，还有内容这四个部分。这里使用 xpath 方法来获取。 12345def parse_detail(self, response): title = response.xpath("/html/body/div[1]/div[1]/div[1]/h1/text()").get() pub_name = response.xpath("/html/body/div[1]/div[1]/div[1]/div[1]/div/span/a/text()").get() release_time = response.xpath("/html/body/div[1]/div[1]/div[1]/div[1]/div/div/span[1]/text()").get() content = response.xpath("/html/body/div[1]/div[1]/div[1]/div[2]/div").get() items.py 中设置存储模板在上面已经决定了要采集者四个信息，那么在 item.py 中设置好这四项。12345class JianshuItem(scrapy.Item): title = scrapy.Field() pub_name = scrapy.Field() release_time = scrapy.Field() content = scrapy.Field() 最后在 parse_detail 尾部加入以下代码，再将 item 返回去。12item = JianshuItem(title=title, pub_name=pub_name, release_time=release_time, content=content)yield item pipelines.py到这里就可以运行一下程序了，看一下是否能正常输出我们采集的信息。 可以看到这里可以正常爬取数据，接下来需要将其存入数据库。存入数据库需要在 pipelines.py 中编写相应的代码。1234567891011121314151617class JianshuMongoDBPipeline(object): def __init__(self): self.DB_URI = 'localhost' self.DB_PORT = 27017 self.client = pymongo.MongoClient(self.DB_URI, self.DB_PORT) self.db = self.client['jianshu'] self.collection = self.db['jianshu_spider'] def process_item(self, item, spdier): try: if self.collection.insert(dict(item)): print('保存至MongoDB成功') else: print('保存至MongoDB失败！') except Exception as error: print(error) return item 在 pipelines.py 中写好 MongoDB 部分后，在 settings.py 中将对应的 pipelines 打开。 然后重新运行 start.py ，启动爬虫。 启动爬虫后，一直没有遇到反爬措施，运行了大概30分钟， ROBO 3T 管理工具得到的数据有3300条。感觉速度还是慢，有待优化(突然发现在 settings.py 中设置了1s延时….关掉之后快多了。。 )]]></content>
      <tags>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy]]></title>
    <url>%2Fscrapy%2F</url>
    <content type="text"><![CDATA[Scrapy 是一个为了爬取网站数据，提取结构性数据而编写的应用框架。可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。 其最初是为了网络抓取所设计的，也可以应用在获取 API 所返回的数据或者通用的网络爬虫。 Scrapy 是使用 Python 语言(基于 Twisted 框架)编写的开源网络爬虫框架。其简单易用、灵活易扩展、开发社区活跃，并且是跨平台的(支持Linux, MacOS, Windows)。 安装方式：pip 安装1$ pip install scrapy 为了确认 Scrapy 被成功安装，可尝试在 python 命令行中将 scrapy 导入，如未报错，则安装成功。 12&gt;&gt;&gt; import scrapy&gt;&gt;&gt; scrapy.version_info scrapy 框架结构 ENGINE 引擎，框架的核心，用于协调其它组件间工作 SCHEDULER 调度器，负责对 SPIDERS 提交的请求进行调度 DOWNLOADER 下载器，负责下载页面 SPIDERS 爬虫，负责提取页面中的数据，并产生对新页面的下载请求 MIDDLEWARES 中间件，负责对 Request 对象和 Response 对象进行处理 ITEM PIPELINE 数据管道，负责对爬取到的数据进行处理 scrapy 工作流程1、引擎 ENGINE 从调度器中取出一个链接 URL 用于接下来的抓取 2、引擎 ENGINE 把 URL 封装成一个请求 Request 传给下载器 Downloader 3、下载器 Downloader 把资源下载下来，并封装成 Response 4、爬虫解析 Response 5、若解析出实体 Item，交给数据管道 Item Pipeline 做进一步处理 6、若解析出链接 URL，就把 URL 交给调度器等待抓取 scrapy 简单爬虫示例专供爬虫初学者训练爬虫技术的网站 http://books.toscrape.com ，从这里开始。 创建项目首先，来创建一个 scrapy 项目，在新建好的 scrapy_demo 文件夹下打开命令行工具，输入如下命令,在这里我创建好了名为 demo 的工程文件夹。1$ scrapy startproject projectName 接下来再切换到项目文件夹根目录下，使用命令创建爬虫文件。可以看到提示已经使用 basic 模板创建了一个名为 books 的爬虫文件。 12$ cd projectName$ scrapy genspider spiderName[爬虫名称] url[爬虫网址] 在此之后，使用 Pycharm 将整个 scrapy 项目导入。 scrapy.cfg 项目的配置信息，主要为Scrapy命令行工具提供一个基础的配置信息。（真正爬虫相关的配置信息在settings.py文件中） items.py 设置数据存储模板，用于结构化数据，如：Django的Model pipelines.py 数据处理行为，如：一般结构化的数据持久化 settings.py 配置文件，如：递归的层数、并发数，延迟下载等 spiders 爬虫目录，如：创建文件，编写爬虫规则 设置数据存储模板要爬取的是各个图书的标题，价格信息。需要在 items.py 中设置数据存储模板。 12345678import scrapyclass DemoItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() title = scrapy.Field() price = scrapy.Field() 编写爬虫爬虫规则在 books.py 中编写。1234567891011121314151617import scrapyfrom demo.items import DemoItemclass TestSpider(scrapy.Spider): name = 'books' allowed_domains = ['toscrape.com'] # 爬虫起始页 start_urls = ['http://books.toscrape.com/catalogue/page-1.html'] def parse(self, response): items = response.xpath("//article[@class='product_pod']") for item in items: title = item.xpath("./h3/a/text()").get() price = item.xpath("./div[2]/p/text()").get() item = DemoItem(title=title, price=price) yield item 在运行爬虫之前，先在 settings.py 中设置好相关项，把 ROBOTSTXT_OBEY 置为 False ，并且打开浏览器模拟。 一切都设置好后，就可以运行爬虫了。为了方便运行，我习惯在项目根目录下新建一个 start.py 用来启动项目，其中代码如下。 123from scrapy import cmdlinecmdline.execute('scrapy crawl books'.split()) 直接运行 start.py，得到如下结果。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394952018-08-20 17:30:36 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: demo)2018-08-20 17:30:36 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 17.5.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o 27 Mar 2018), cryptography 2.2.2, Platform Windows-7-6.1.7601-SP12018-08-20 17:30:36 [scrapy.crawler] INFO: Overridden settings: &#123;'BOT_NAME': 'demo', 'NEWSPIDER_MODULE': 'demo.spiders', 'SPIDER_MODULES': ['demo.spiders']&#125;2018-08-20 17:30:36 [scrapy.middleware] INFO: Enabled extensions:['scrapy.extensions.corestats.CoreStats', 'scrapy.extensions.telnet.TelnetConsole', 'scrapy.extensions.logstats.LogStats']2018-08-20 17:30:37 [scrapy.middleware] INFO: Enabled downloader middlewares:['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware', 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware', 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware', 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware', 'scrapy.downloadermiddlewares.retry.RetryMiddleware', 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware', 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware', 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware', 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware', 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware', 'scrapy.downloadermiddlewares.stats.DownloaderStats']2018-08-20 17:30:37 [scrapy.middleware] INFO: Enabled spider middlewares:['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware', 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware', 'scrapy.spidermiddlewares.referer.RefererMiddleware', 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware', 'scrapy.spidermiddlewares.depth.DepthMiddleware']2018-08-20 17:30:37 [scrapy.middleware] INFO: Enabled item pipelines:[]2018-08-20 17:30:37 [scrapy.core.engine] INFO: Spider opened2018-08-20 17:30:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)2018-08-20 17:30:37 [py.warnings] WARNING: C:\ProgramData\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py:59: URLWarning: allowed_domains accepts only domains, not URLs. Ignoring URL entry http://toscrape.com/ in allowed_domains. warnings.warn("allowed_domains accepts only domains, not URLs. Ignoring URL entry %s in allowed_domains." % domain, URLWarning)2018-08-20 17:30:37 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:60232018-08-20 17:30:37 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://books.toscrape.com/catalogue/page-1.html&gt; (referer: None)2018-08-20 17:30:37 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/catalogue/page-1.html&gt;&#123;'price': '£51.77', 'title': 'A Light in the ...'&#125;2018-08-20 17:30:37 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/catalogue/page-1.html&gt;&#123;'price': '£53.74', 'title': 'Tipping the Velvet'&#125;2018-08-20 17:30:37 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/catalogue/page-1.html&gt;&#123;'price': '£50.10', 'title': 'Soumission'&#125;2018-08-20 17:30:37 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/catalogue/page-1.html&gt;&#123;'price': '£47.82', 'title': 'Sharp Objects'&#125;2018-08-20 17:30:37 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/catalogue/page-1.html&gt;&#123;'price': '£54.23', 'title': 'Sapiens: A Brief History ...'&#125;2018-08-20 17:30:37 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/catalogue/page-1.html&gt;&#123;'price': '£22.65', 'title': 'The Requiem Red'&#125;2018-08-20 17:30:37 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/catalogue/page-1.html&gt;&#123;'price': '£33.34', 'title': 'The Dirty Little Secrets ...'&#125;2018-08-20 17:30:37 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/catalogue/page-1.html&gt;&#123;'price': '£17.93', 'title': 'The Coming Woman: A ...'&#125;2018-08-20 17:30:37 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/catalogue/page-1.html&gt;&#123;'price': '£22.60', 'title': 'The Boys in the ...'&#125;2018-08-20 17:30:37 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/catalogue/page-1.html&gt;&#123;'price': '£52.15', 'title': 'The Black Maria'&#125;2018-08-20 17:30:37 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/catalogue/page-1.html&gt;&#123;'price': '£13.99', 'title': 'Starving Hearts (Triangular Trade ...'&#125;2018-08-20 17:30:37 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/catalogue/page-1.html&gt;&#123;'price': '£20.66', 'title': "Shakespeare's Sonnets"&#125;2018-08-20 17:30:37 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/catalogue/page-1.html&gt;&#123;'price': '£17.46', 'title': 'Set Me Free'&#125;2018-08-20 17:30:37 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/catalogue/page-1.html&gt;&#123;'price': '£52.29', 'title': "Scott Pilgrim's Precious Little ..."&#125;2018-08-20 17:30:37 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/catalogue/page-1.html&gt;&#123;'price': '£35.02', 'title': 'Rip it Up and ...'&#125;2018-08-20 17:30:37 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/catalogue/page-1.html&gt;&#123;'price': '£57.25', 'title': 'Our Band Could Be ...'&#125;2018-08-20 17:30:37 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/catalogue/page-1.html&gt;&#123;'price': '£23.88', 'title': 'Olio'&#125;2018-08-20 17:30:37 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/catalogue/page-1.html&gt;&#123;'price': '£37.59', 'title': 'Mesaerion: The Best Science ...'&#125;2018-08-20 17:30:37 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/catalogue/page-1.html&gt;&#123;'price': '£51.33', 'title': 'Libertarianism for Beginners'&#125;2018-08-20 17:30:37 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://books.toscrape.com/catalogue/page-1.html&gt;&#123;'price': '£45.17', 'title': "It's Only the Himalayas"&#125;2018-08-20 17:30:37 [scrapy.core.engine] INFO: Closing spider (finished)2018-08-20 17:30:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:&#123;'downloader/request_bytes': 315, 'downloader/request_count': 1, 'downloader/request_method_count/GET': 1, 'downloader/response_bytes': 5889, 'downloader/response_count': 1, 'downloader/response_status_count/200': 1, 'finish_reason': 'finished', 'finish_time': datetime.datetime(2018, 8, 20, 9, 30, 37, 857663), 'item_scraped_count': 20, 'log_count/DEBUG': 22, 'log_count/INFO': 7, 'log_count/WARNING': 1, 'response_received_count': 1, 'scheduler/dequeued': 1, 'scheduler/dequeued/memory': 1, 'scheduler/enqueued': 1, 'scheduler/enqueued/memory': 1, 'start_time': datetime.datetime(2018, 8, 20, 9, 30, 37, 96620)&#125;2018-08-20 17:30:37 [scrapy.core.engine] INFO: Spider closed (finished) 可以看到，scrapy 在经过初始化之后开始爬虫，并且输出了所需的价格和标题信息。 保存数据暂不做介绍。]]></content>
      <tags>
        <tag>爬虫</tag>
        <tag>Python</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github模拟登陆]]></title>
    <url>%2Fgithub%2F</url>
    <content type="text"><![CDATA[github 的登录页面没有验证码，而且比较简单，所以先拿 github 练练手。 按照惯例，先规划一下步骤： 1.进入登录页，F12 查看请求，查看必要参数。 2.构造登录函数。 3.爬虫成功登陆后采集信息。 进入登录页真实登录登录页网址: https://github.com/login 页面如下： 现在先输入正确的账号密码进行登录。 打开 F12 开发工具，打开 Network 标签，先输入正确的账号密码，点击 Sign in，然后观察请求。可以看到，第一条请求是 https://github.com/session , 请求方式是 post 方式，而且 Form Data 里面存在账号密码信息，ok，就是这个请求，如下图。 能看到请求方式为 post 然后看下 Form Data ，其中有这么几个参数，commit，utf8，authenticity_token，login，password，经过观察，commit，utf8，login，password 这几个都是固定值，而 authenticity_token 每次登录都会发生改变，先找到这个参数是怎么出来的，才方便构造登录函数，经查找后发现在登录页，也就是 https://github.com/login 这个页面，再查看源代码可以查找到这个 token 值，如下图所示。 构造登录函数123456789101112131415161718192021222324252627282930313233343536import requestsfrom lxml import etreeclass github_login(object): def __init__(self): self.headers = &#123; 'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Mobile Safari/537.36', &#125; self.login_url = 'https://github.com/login' self.post_url = 'https://github.com/session' self.logined_url = 'https://github.com/settings/profile' # 为了保持会话，需要用到 requests 的 session 服务 self.session = requests.Session() # 获取token值 def get_token(self): html = self.session.get(self.login_url, headers=self.headers) response = etree.HTML(html.text) token = response.xpath("//input[@name='authenticity_token']/@value")[0] return token # 上面已经获取到了token参数，那么开始写登录函数 # 把username和password作为变量，后期传入 def login(self, token, email, password): data = &#123; 'commit': 'Sign in', 'utf8': '✓', 'authenticity_token': token, 'login': email, 'password': password &#125; response = self.session.post(self.post_url, data=data, headers=self.headers) if response.status_code == 200: print('登录成功，正在跳转到个人信息...') 采集信息上面得到 token 值，也成功登录了 github 。现在我们就可以开始采集信息了。在这里我选择进入个人详情页来获取用户名和地址信息。 12345def parse(self, html): response = etree.HTML(html) name = response.xpath("//dl[@class='form-group'][1]/dd[1]/input/@value")[0] location = response.xpath("//dl[@class='form-group'][6]/dd[1]/input/@value")[0] print('姓名: %s, 地址: %s' % (name, location)) 获取到用户名和地址信息。 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import requestsfrom lxml import etreeclass github_login(object): def __init__(self): self.headers = &#123; 'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Mobile Safari/537.36', &#125; self.login_url = 'https://github.com/login' self.post_url = 'https://github.com/session' self.logined_url = 'https://github.com/settings/profile' self.session = requests.Session() def get_token(self): html = self.session.get(self.login_url, headers=self.headers) response = etree.HTML(html.text) token = response.xpath("//input[@name='authenticity_token']/@value")[0] return token def parse(self, html): response = etree.HTML(html) name = response.xpath("//dl[@class='form-group'][1]/dd[1]/input/@value")[0] location = response.xpath("//dl[@class='form-group'][6]/dd[1]/input/@value")[0] print('姓名: %s, 地址: %s' % (name, location)) def login(self, token, email, password): data = &#123; 'commit': 'Sign in', 'utf8': '✓', 'authenticity_token': token, 'login': email, 'password': password &#125; response = self.session.post(self.post_url, data=data, headers=self.headers) if response.status_code == 200: print('登录成功，正在跳转到个人信息...') response = self.session.get(self.logined_url, headers=self.headers) self.parse(response.text)if __name__ == '__main__': login = github_login() token = login.get_token() login.login(token, 'username', 'password')]]></content>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫--美团美食信息]]></title>
    <url>%2Fmeituanspider%2F</url>
    <content type="text"><![CDATA[美团网也是动态加载的网页，但是我看了下，其美食信息也在源代码中有，因此可以按照Python爬虫–淘宝(2)其中的方式去爬美食信息。 步骤照搬： 1.获取网页源代码。 2.使用网页解析工具进行解析，构造翻页网址，提取所需信息。 3.整理并保存信息。 获取网页源代码123456789101112import requestsimport reheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36'&#125;# 以爬取北京美团美食列表为例url = 'http://bj.meituan.com/meishi/'# 请求网页response = requests.get(url, headers=headers)print(response.text) 提取信息在上一步操作得到源代码之后，仔细观察，可以看到带有商家信息的部分为一段 json 数据，这里稍有不同的是，我们可以直接在源代码中使用正则表达式匹配出含有商家信息的 json 信息。 匹配 json 数据段12345import json# findall获取得到的是一个列表items = re.findall(r'"poiLists":(.`?),"comHeader"', response.text, re.S)# 而列表中只有一个元素item = json.loads(items[0]) 提取信息123456789if item.get('poiInfos'): for info in item.get('poiInfos'): id = info.get('poiId') title = info.get('title') addr = info.get('address') avgScore = '平均分' + str(info.get('avgScore')) allCommentNum = str(info.get('allCommentNum')) + '条评论' avgPrice = str(info.get('avgPrice')) + '元/人' print(id, title, addr, avgPrice, avgScore, allCommentNum) 构造翻页网址通过翻页找到页码规律，每个网页最后面 pn 后面的数字就是页码。123# 爬取 1-9 页美食信息for page in range(1, 10): url = 'http://bj.meituan.com/meishi/pn&#123;&#125;'.format(page) 保存数据保存到MongoDB数据库。 代码12345678910111213141516171819202122import requestsimport reimport jsonheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36'&#125;for page in range(1, 10): url = 'http://bj.meituan.com/meishi/pn&#123;&#125;'.format(page) response = requests.get(url, headers=headers) items = re.findall(r'"poiLists":(.`?),"comHeader"', response.text, re.S) item = json.loads(items[0]) if item.get('poiInfos'): for info in item.get('poiInfos'): id = info.get('poiId') title = info.get('title') addr = info.get('address') avgScore = '平均分' + str(info.get('avgScore')) allCommentNum = str(info.get('allCommentNum')) + '条评论' avgPrice = str(info.get('avgPrice')) + '元/人' print(id, title, addr, avgPrice, avgScore, allCommentNum)]]></content>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫之淘宝(二)]]></title>
    <url>%2Ftaobaospider2%2F</url>
    <content type="text"><![CDATA[上一篇介绍了如何用 selenium 自动化工具去帮助爬取淘宝商品的各项数据，但是，我发现，淘宝的网页源代码中就包含有宝贝信息，信息被放在了 script 标签中，既然信息被包含在源代码中，那就证明可以通过正则或者其它的网页解析方式可以获取到需要的信息，因此这篇就来写一写对于淘宝商品来说更为简单的小爬虫。 这次的流程就比较简单了，先做好准备工作： 1.获取网页源代码。 2.使用网页解析工具进行解析，构造翻页网址，提取所需信息。 3.整理并保存信息。 获取网页源代码123456789101112import requestsimport reheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36'&#125;# 搜索关键词为python的商品列表页url = 'https://s.taobao.com/search?q=python'# 请求网页response = requests.get(url, headers=headers)print(response.text) 通过 requests 的 get 方法请求网页之后，得到网页源代码。 提取信息123456title = re.findall(r'"raw_title":"(.`?)"', response.text, re.I)price = re.findall(r'"view_price":"(.`?)"', response.text, re.I)location = re.findall(r'"item_loc":"(.`?)"', response.text, re.I)n = len(title)for i in range(n): print(title[i], price[i], location[i]) 在这里只对商品的标题，价格和发货地进行了正则匹配，因为它们都是列表，要想让宝贝信息都一一对应的显示出来，就要进行遍历，最后把宝贝信息都进行输出。 整理信息获取到了宝贝信息之后我们就可以把它们存入文件或者数据库了。这里我们先把它们存入文件。 123456file = open('taobao.txt', 'a', encoding='utf-8')n = len(title)for i in range(n): info = str(page ` 44 + i + 1) + '标题：'+ title[i] + '\n' + '价格：' + price[i] + '\n' + '发货地：' + location[i] + '\n' file.write(info)file.close() 完整代码1234567891011121314151617181920212223import requestsimport reheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36'&#125;payload = &#123;'q':'python', 's':'1', 'ie':'utf8'&#125;file = open('taobao.txt', 'a', encoding='utf-8')for page in range(10): payload['s'] = 44 ` page + 1 url = 'https://s.taobao.com/search?q=python' response = requests.get(url, params=payload, headers=headers) title = re.findall(r'"raw_title":"(.`?)"', response.text, re.I) price = re.findall(r'"view_price":"(.`?)"', response.text, re.I) location = re.findall(r'"item_loc":"(.`?)"', response.text, re.I) n = len(title) for i in range(n): info = str(page ` 44 + i + 1) + '标题：'+ title[i] + '\n' + '价格：' + price[i] + '\n' + '发货地：' + location[i] + '\n' file.write(info)file.close()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫之淘宝(一)]]></title>
    <url>%2Ftaobaospider1%2F</url>
    <content type="text"><![CDATA[淘宝网也是动态加载的网页，虽然其页面数据也是通过 Ajax 获取的，但是若想像前面一样去分析 Ajax ，在淘宝这里是很复杂的，因为其参数会包含加密密钥，自己构造 Ajax 参数过于复杂。所以并不建议使用和爬取今日头条一样的方法来爬取淘宝。 先来分析下淘宝的接口，来观察 Ajax 复杂程度。 打开淘宝，再打开开发者工具，搜索关键词 python ，截获 Ajax 请求，这里看到只有一条请求，并且其内容为 json 形式的商品信息。 再来查看其构造参数。可以看到其中的 ksTS 和 rn 参数不能直接找到其规律，如果想找，只会消耗大量时间。但是如果使用 selenium 来模拟浏览器操作的话，那就不需要再关注这些参数了。因此接下来的部分使用 selenium 来爬取淘宝宝贝信息。 selenium 是一个用于 Web 应用程序测试的工具。Selenium 直接运行在浏览器中，就好像真实用户在操作一样，因此在爬虫中主要用来解决 js 渲染问题。其支持很多浏览器，常见的 Chrome， Firefox， ie，safari 和无头浏览器 phantomjs 。 这次就使用 selenium 这个工具来操作对淘宝商品的爬虫。 selenium 的使用方法步骤：首先导入所需要的包，然后声明浏览器对象，再去请求网页，最后进行查找节点等等操作。 导入所需要的包：1from selenium import webdriver 声明浏览器对象(使用 Chrome 浏览器)：1browser = webdriver.Chrome() 访问网页12url = 'https://www.taobao.com'browser.get(url) 下面来写整个项目，按照惯例，先列出步骤。 1.使用 selenium 访问淘宝商品列表网页，获取网页源代码。 2.使用网页解析工具采集所需数据。 3.导出并整理数据。 使用 selenium 获取网页源代码1234567from selenium import webdriverbrowser = webdriver.Chrome()url = 'https://s.taobao.com/search?q=python'browser.get(url)# 使用page_source获取网页源代码html = browser.page_source 获取所需数据在商品列表页中，要爬取的信息有商品图片、商品价格、商品成交量、商品名称、店铺名称和位置这六项信息。 1234567891011121314from pyquery import PyQuery as pqdoc = pq(html)items = doc('#mainsrp-itemlist .items .item').items()for item in items: product = &#123; 'image': item.find('.pic .img').attr('data-src').replace('//', 'http://'), 'price': item.find('.price').text().replace('\n', ''), 'deal': item.find('.deal-cnt').text(), 'title': item.find('.title').text(), 'shop': item.find('.shop').text(), 'location': item.find('.location').text() &#125; print(product) 获取到以下数据。 翻页在 index_page 中首先访问了 url ，然后判断当前页码，如果大于1，就进行跳页操作，否则等待页面加载完成。 等待加载时，使用了 WebDriverWait 对象，它可以指定等待条件，这里指定为10s，如果在这个时间内成功匹配了等待条件，就立即返回结果并向下执行，否则就抛出超时异常。 比如在这里要等待商品信息加载出来，就制定了 presence_of_element_located 这个条件，然后传入 .m-itemlist .items .item 选择器，而这个选择器对应的页面内容就是每个商品的新消息块，如果加载成功了，就会执行后续的 get_products() 方法。 关于翻页操作，这里先获取了页码的输入框，赋值为 input ，然后再获取确定按钮，赋值为 submit 。获取到两个元素后，先调用 clear() 方法将页码输入框进行清空，再调用 send_keys() 方法将页码填充进去。 那么如何确定浏览器有没有跳转到对应的页码呢，可以看到，成功跳转到某一页时，当前页码会在网页底部高亮显示，因此可以拿到高亮显示的 css 选择器与当前传入的页码做对比，如果一致，则跳转成功。继续等待商品加载完成…12345678910111213141516171819202122232425262728293031323334from urllib.parse import quotefrom selenium.common.exceptions import TimeoutExceptionfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.webdriver.support.wait import WebDriverWaitwait = WebDriverWait(browser, 10)KEYWORD = 'python'def index_page(page): try: url = 'https://s.taobao.com/search?q=' + quote(KEYWORD) browser.get(url) if page &gt; 1: # 页码输入框 input = wait.until( EC.presence_of_element_located((By.CSS_SELECTOR, '#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; input')) ) # 页码确定按钮 submit = wait.until( EC.presence_of_element_located((By.CSS_SELECTOR,'#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; span.btn.J_Submit')) ) input.clear() input.send_keys(page) submit.click() wait.until( EC.text_to_be_present_in_element((By.CSS_SELECTOR, '#mainsrp-pager li.item.active &gt; span'), str(page)) ) wait.until( EC.presence_of_element_located((By.CSS_SELECTOR, '.m-itemlist .items .item')) ) get_products() except TimeoutException: index_page(page) 保存数据到 MongoDB12345678client = pymongo.MongoClient('localhost', 27017)db = client['taobao']collection = db['taobao1_spider']def save_to_mongo(data): if collection.insert(data): print('保存到MongoDB成功') 采集到的数据如下。 完整代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071from selenium import webdriverfrom urllib.parse import quotefrom selenium.common.exceptions import TimeoutExceptionfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.webdriver.support.wait import WebDriverWaitfrom pyquery import PyQuery as pqimport pymongobrowser = webdriver.Chrome()wait = WebDriverWait(browser, 10)KEYWORD = 'python'client = pymongo.MongoClient('localhost', 27017)db = client['taobao']collection = db['taobao1_spider']def index_page(page): try: url = 'https://s.taobao.com/search?q=' + quote(KEYWORD) browser.get(url) if page &gt; 1: input = wait.until( EC.presence_of_element_located((By.CSS_SELECTOR, '#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; input')) ) submit = wait.until( EC.presence_of_element_located((By.CSS_SELECTOR,'#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; span.btn.J_Submit')) ) input.clear() input.send_keys(page) submit.click() wait.until( EC.text_to_be_present_in_element((By.CSS_SELECTOR, '#mainsrp-pager li.item.active &gt; span'), str(page)) ) wait.until( EC.presence_of_element_located((By.CSS_SELECTOR, '.m-itemlist .items .item')) ) get_products() except TimeoutException: index_page(page)def get_products(): html = browser.page_source doc = pq(html) items = doc('#mainsrp-itemlist .items .item').items() for item in items: product = &#123; 'image': item.find('.pic .img').attr('data-src'), 'price': item.find('.price').text(), 'deal': item.find('.deal-cnt').text(), 'title': item.find('.title').text(), 'shop': item.find('.shop').text(), 'location': item.find('.location').text() &#125; print(product) save_to_mongo(product)def save_to_mongo(data): if collection.insert(data): print('保存到MongoDB成功')def main(): for page in range(1, 101): index_page(page) browser.close()if __name__ == "__main__": main()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Python</tag>
        <tag>selenium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫--京东商品评论]]></title>
    <url>%2Fjd_comments%2F</url>
    <content type="text"><![CDATA[京东网站是我经常去购物的网站，现在来爬京东商品的评论。有时候网速慢，打开评论的时候还要等一会，一直在加载，也证明了其评论是网页动态加载的。而去爬取动态加载的网页，无非就是查看其 js 请求和使用 selenium 工具，这里使用了前者的方法，比较简单。 查看动态 js 请求先打开开发者工具，然后再打开京东商品 iPhone X 评论页面，可以看到，请求列表中有一个 productPageComments 文件，打开它，点击 Preview 标签，下面的数据是 json 形式，再点击 Comments ，查看其中正是需要的评论数据。 js 文件网址 : https://sclub.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv75398&amp;productId=5089253&amp;score=0&amp;sortType=5&amp;page=0&amp;pageSize=10&amp;isShadowSku=0&amp;rid=0&amp;fold=1 对 js 文件进行请求12345678import requestsheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36'&#125;url = 'https://sclub.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv75398&amp;productId=5089253&amp;score=0&amp;sortType=5&amp;page=0&amp;pageSize=10&amp;isShadowSku=0&amp;fold=1'response = requests.get(url, headers=headers)print(response.text) 在这段代码中，先查看对 js 文件网址请求之后，得到的数据是什么。 输出的数据为 json 形式，但是数据首尾都有一些多余数据，为了把它变成正常的 json 形式，我们用 replace 方法替换掉多余部分。 1234result = response.replace('fetchJSON_comment98vv75398(', '')result = result.replace(');', '')json_so = json.loads(result)print(json_so) 这样我们输出的数据就是正常的 json 格式，可以进行提取信息的操作了。 提取数据并构造翻页网址把上一步输出的数据放入 json在线解析 网页中进行解析，得到如下图。 证明我们上一步操作没问题，接下来提取数据。12345678items = json_so.get('comments')for item in items: id = item.get('id') content = item.get('content') create_time = item.get('creationTime') nick_name = item.get('nickname') client = item.get('userClientShow') print(id, nick_name, content, create_time, client) 这样就可以得到商品评论的 id， 内容，评论时间，评论人昵称和评论人客户端信息，如下图。 接下来要构造翻页网址，通过观察每一评论页网址，发现其参数中有个 page 参数，这个是页码参数，所以我们只要对 url 中传入不同的 page 值，就可以进行翻页。12for page in range(10): url = 'https://sclub.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv75398&amp;productId=5089253&amp;score=0&amp;sortType=5&amp;page=' + str(page) + '&amp;pageSize=10&amp;isShadowSku=0&amp;fold=1' 整理数据最后把评论信息都写入文件中，如下图。 保存至本地文本文件 保存至MongoDB数据库 完整代码123456789101112131415161718192021222324252627282930313233343536373839import requestsimport jsonimport pymongoheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36'&#125;file = open('jd_com.txt', 'w', encoding='utf-8')client = pymongo.MongoClient('localhost', 27017)db = client['jd']collection = db['jdcom_spider']for page in range(10): url = 'https://sclub.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv75398&amp;productId=5089253&amp;score=0&amp;sortType=5&amp;page=' + str(page) + '&amp;pageSize=10&amp;isShadowSku=0&amp;fold=1' response = requests.get(url, headers=headers) html = response.text result = html.replace('fetchJSON_comment98vv75398(', '') result = result.replace(');', '') json_so = json.loads(result) items = json_so.get('comments') for item in items: id = item.get('id') content = item.get('content') create_time = item.get('creationTime') nick_name = item.get('nickname') client = item.get('userClientShow') data = &#123; 'id': id, '内容': content, '评论时间': create_time, '昵称': nick_name, '客户端': client &#125; # 保存至本地文本文件 file.write(str(data) + '\n') # 保存至MongoDB数据库 if collection.insert(data): print('保存至MongoDB成功')]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之今日头条街拍]]></title>
    <url>%2Fjrtt_spider%2F</url>
    <content type="text"><![CDATA[前面的爬虫都是静态页面，遇到动态页面该如何爬取，当时困惑了好久，不知道如何下手，参考了几篇其他大佬的文章，才慢慢有一点懂。 这次的网页是动态加载的今日头条街拍图集网页。看了崔大大的教程，自己动手码一下代码，熟悉一下动态页面的爬虫步骤。 动态页面肯定不能像静态页面一样直接取数据，因为它的数据都是通过 js 渲染进来的，因此先找到对应的数据 js 文件。 首先，先来规划下步骤： 1.观察 js 请求，查看数据通过哪个文件传输。 2.对 js 文件进行请求，获得信息。 3.提取数据，对数据进行处理。 查看 js 请求打开今日头条，搜索框内输入关键词“街拍”，跳转到街拍页面，往下拉，能看到图片一直在加载新的，而网址没有改变，动态的没错了，打开 F12 ，网页继续往下拉，Network 下出现了新的请求，并且这些请求构造都差不多，随便点击一个请求，打开 Preview ，里面是 json 格式的，其中有图片标题，也有图片的 url 信息，看到里面就是想要采集的数据，要找的就是它。 请求方式为 GET , 再来看下 Form Data ，有 offset , format , keyword , autoload , count , cur_tab , from 这几个参数，offset 是偏移量，也就是一共刷新出来的图片数量，keyword 是关键词， count 是每一页刷新出来的图片数量， 其它参数没什么重要意义，构造网址时直接加上去就行了。 对js进行请求想要对 js 进行请求，需要先对网址进行构造。 12345678910111213141516171819from urllib.parse import urlencodeimport requestsimport jsonheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36'&#125;# Form Data 中的参数params = &#123; 'offset': page, 'format': 'json', 'keyword': '街拍', 'autoload': 'true', 'count': '20', 'cur_tab': '3', 'from': 'gallery' &#125;# 对网址进行构造， 使用urlencode方法url = 'https://www.toutiao.com/search_content/?' + urlencode(params) 构造完毕，对网址进行请求。1234# 对网址进行请求，获取到的json格式response = requests.get(url, headers=headers)if response.status_code == 200: return response.json() 提取信息经过上一步对 js 进行请求后，得到和之前看到的 Preview 里面一样的信息，开始对数据进行提取处理。 1234567891011121314151617json = response.json()# 判断json的data，如果存在继续下一步，其实这里用.get方法更好，因为.get方法如果获取一个不存在的属性时不会报错，而直接使用['']这种方法，如果不存在此属性，就会直接报错，影响程序效率if json['data']: # data是个列表，对列表中数据继续遍历 for item in json['data']: # 获取到title信息和图片地址信息 title = item.get('title') image_urls = item.get('image_list') # 因为有多张图片，因此图片信息被装在一个列表中，需要继续遍历 for image_url in image_urls: # 这里把图片url中的list换成large，提取大图 image = 'http://' + image_url.get('url').strip('//').replace('list', 'large') # 返回image和title信息 yield &#123; 'image': image, 'title': title &#125; 保存数据保存到本地文件夹1234567891011121314151617import osfrom hashlib import md5# 判断当前文件路径下是否存在以图片的标题命名的文件夹，如果不存在，就新建文件夹if not os.path.exists(item.get('title')): os.mkdir(item.get('title'))try: # 请求图片的url地址 response = requests.get(item.get('image')) if response.status_code == 200: file_path = '&#123;0&#125;/&#123;1&#125;.&#123;2&#125;'.format(item.get('title'), md5(response.content).hexdigest(), 'jpg') if not os.path.exists(file_path): with open(file_path, 'wb') as f: f.write(response.content) else: print('已经下载过！')except requests.ConnectionError: print('连接失败!') 保存到 MongoDB 数据库123456def save_to_mongo(data): client = pymongo.MongoClient('localhost', 27017) db = client['jrtt'] collection = db['jrtt_spider'] if collection.insert(data): print('保存到MongoDB成功') 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293import osimport pymongoimport requestsfrom hashlib import md5from urllib.parse import urlencodefrom multiprocessing import Poolheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36'&#125;def get_page_source(url): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() except Exception: print('error')def get_content(json): if json['data']: for item in json['data']: title = item.get('title') image_urls = item.get('image_list') for image_url in image_urls: image = image_url.get('url').replace('list', 'large') yield &#123; 'image': image, 'title': title &#125;def save_to_file(item): if not os.path.exists(item.get('title')): os.mkdir(item.get('title')) try: response = requests.get(item.get('image')) if response.status_code == 200: file_path = '&#123;0&#125;/&#123;1&#125;.&#123;2&#125;'.format(item.get('title'), md5(response.content).hexdigest(), 'jpg') if not os.path.exists(file_path): with open(file_path, 'wb') as f: f.write(response.content) else: print('已经下载过！') except requests.ConnectionError: print('连接失败!')def save_to_mongo(data): client = pymongo.MongoClient('localhost', 27017) db = client['jrtt'] collection = db['jrtt_spider'] if collection.insert(data): print('保存到MongoDB成功')def main(page): params = &#123; 'aid': '24', # 新增 'offset': page, 'format': 'json', 'keyword': '街拍', 'autoload': 'true', 'count': '20', 'cur_tab': '3', 'from': 'search_tab', 'pd': 'synthesis' # 新增 &#125; url = 'https://www.toutiao.com/api/search/content/?' + urlencode(params) json = get_page_source(url) items = get_content(json) for item in items: # 本地输出 print(item) # 保存至本地文件夹 save_to_file(item) # 保存标题和链接到MongoDB数据库 save_to_mongo(item)if __name__ == '__main__': START = 1 END = 20 pool = Pool() groups = ([x * 20 for x in range(START, END+1)]) try: pool.map(main, groups) pool.close() pool.join() except Exception as e: print()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>动态网页</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫之猫眼排行榜TOP100]]></title>
    <url>%2Fmaoyan_spider%2F</url>
    <content type="text"><![CDATA[网页分成静态网页和动态网页： 静态网页是相对于动态网页而言，是指没有后台数据库、不含程序和不可交互的网页。静态网页相对更新起来比较麻烦，适用于一般更新较少的展示型网站。一般是网站呈现出来后网站的内容及结构就不会再发生改变了。 而动态网页则不然，页面代码虽然没有变，但是显示的内容却是可以随着时间、环境或者数据库操作的结果而发生改变的能链接数据库，将数据库中的内容展现在页面中，同时允许用户与网站进行交互。我第一个接触的爬虫小项目就是猫眼爬虫。 因为猫眼网站是静态网站，因此我们只要先拿到原网址，获取内容后进行解析，再去分析多页信息等操作就可以了，作为入门项目非常简单。 写每个工程都要有明确思路，对于这个项目，步骤有： 1.获取到网页源代码。 2.使用 bs4 , xpath 等工具解析内容，提取想获取的内容。 3.分析网页如何进行翻页操作，构造翻页的网址，继续提取内容。 4.提取出的信息该如何处理(保存至文件或者数据库) 排行榜网址 http://maoyan.com/board/4 网站首页如下图所示： 获取网页源代码12345678import requestsfrom bs4 import BeautifulSoupheaders = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36 SE 2.X MetaSr 1.0'&#125;# 直接对网址进行请求url = 'http://maoyan.com/board/4' response = requests.get(url, headers=headers) 提取信息要提取各个电影的排名，影片名称，演员，上映时间，评分信息，打开 F12 开发工具，发现电影信息都被放在一个个 dd 标签内，因此使用 bs 的 select 方法，选中 dd 标签，select 输出是列表，可以迭代，所以继续用 for 循环遍历列表，再根据具体信息进行具体筛选，使用 .text 输出文本。 12345678910111213141516soup = Beautifulsoup(response.text, 'lxml')for items in soup.select('dd'): rank = items.select('.board-index')[0].text title = items.select('a')[1].text actor_name = items.select('.star')[0].text.strip() release_time = items.select('.releasetime')[0].text integer = items.select('.integer')[0].text fraction = items.select('.fraction')[0].text goal = integer + fraction yield &#123; 'range': rank, 'title': title, 'actor_name': actor_name, 'release_time': release_time, 'goal': goal &#125; 分析网页的翻页操作这里就直接点击下一页来查看网址的变化来找出页码规律，点击下一页发现网址后面出现了 offset 参数，每点击下一页，offset 后面的数值就+10，所以现在可以开始构造每一页的网址，来进行爬去整个 TOP100 排行榜，这里只爬取了前10页数据。12# 翻页网址的构造final_url = 'http://maoyan.com/board/4?offset=&#123;&#125;'.format(page) 上面是构造出的最终网址，通过传入 page 可以给网址赋予不同的操作，requests 再去请求就可以了。 对提取数据的操作提取数据后，可以存进文件里或者数据库中。 存进文件123# 以追加方式，编码方式为utf-8with open('reslut.txt', 'a', encoding='utf-8') as f: f.write(item) 这样项目下就会生成一个 result.txt 文件，信息以字典方式存放入文件中。 将数据存入 mysql 数据库1234567db = pymysql.connect('localhost', 'root', 'root', 'maoyan')cursor = db.cursor()sql = '''insert into maoyan_spider(rank, title, actor_name, release_time, goal) values(%s,%s,%s,%s,%s)'''cursor.execute(sql, (data['rank'], data['title'], data['actor_name'], data['release_time'], data['goal']))db.commit()db.close()print('存入mysql成功') 将数据存入 mongoDB 数据库12345client = pymongo.MongoClient('localhost', 27017)db = client['maoyan']collection = db['maoyan_spdier']collection.insert(data)print('保存到mongodb成功') 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import requestsfrom bs4 import BeautifulSoupimport pymysqlimport pymongodef get_page_source(url): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.text except Exception: print('error')def get_content(html): soup = BeautifulSoup(html, 'lxml') for items in soup.select('dd'): rank = items.select('.board-index')[0].text title = items.select('a')[1].text name = items.select('.star')[0].text.strip() time = items.select('.releasetime')[0].text integer = items.select('.integer')[0].text fraction = items.select('.fraction')[0].text goal = integer + fraction yield &#123; 'range': rank, 'title': title, 'name': name, 'time': time, 'goal': goal &#125;def write_to_file(data): with open('result.txt', 'a', encoding='utf-8') as f: f.write(str(data) + '\n') print('保存到文本文件成功')def save_to_mysql(data): db = pymysql.connect('localhost', 'root', 'root', 'maoyan') cursor = db.cursor() sql = '''insert into maoyan_spider(rank, title, actor_name, release_time, goal) values(%s,%s,%s,%s,%s)''' cursor.execute(sql, (data['rank'], data['title'], data['actor_name'], data['release_time'], data['goal'])) # cursor.execute('''''', (data['rank'], data['title'], data['mvname'], data['release_time'], data['goal'])) db.commit() db.close() print('存入mysql成功')def save_to_mongo(data): client = pymongo.MongoClient('localhost', 27017) db = client['maoyan'] collection = db['maoyan_spdier'] collection.insert(data) print('保存到mongodb成功')def main(): url = 'http://maoyan.com/board/4?offset=0' html = get_page_source(url) data = get_content(html) for item in data: # 存入文本文件 write_to_file(item) # 存入mysql数据库 save_to_mysql(item) # 存入mongoDB数据库 save_to_mongo(item)if __name__ == '__main__': for page in range(0, 10): main(page*10)]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>静态网页</tag>
      </tags>
  </entry>
</search>
