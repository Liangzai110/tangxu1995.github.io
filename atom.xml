<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>2048</title>
  
  <subtitle>Life is short I use Python</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://tangx1.com/"/>
  <updated>2019-01-21T15:06:26.217Z</updated>
  <id>https://tangx1.com/</id>
  
  <author>
    <name>Tang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Scrapy对接Selenium京东爬虫</title>
    <link href="https://tangx1.com/scrapy-selenium/"/>
    <id>https://tangx1.com/scrapy-selenium/</id>
    <published>2019-01-21T09:45:42.000Z</published>
    <updated>2019-01-21T15:06:26.217Z</updated>
    
    <content type="html"><![CDATA[<h1 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h1><p>闲话不多说，准备工作都已经做好了。确保电脑上 chromedriver 和 selenium 框架都已安装好。新建一个名为 jd_selenium 的项目，然后新建一个爬虫，修改 <code>settings.py</code> 中的 <code>ROBOTSTXT_OBEY = False</code>，最后在项目根目录下新建启动文件 <code>run.py</code>。</p><h1 id="定义-Item"><a href="#定义-Item" class="headerlink" title="定义 Item"></a>定义 Item</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdSeleniumItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    price = scrapy.Field()</span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    comments = scrapy.Field()</span><br></pre></td></tr></table></figure><p>初步实现 Spider 的 start_requests() 方法，如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpiderSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'spider'</span></span><br><span class="line">    allowed_domains = [<span class="string">'jd.com'</span>]</span><br><span class="line">    base_url = <span class="string">'https://search.jd.com/Search?keyword='</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> keyword <span class="keyword">in</span> self.settings.get(<span class="string">'KEYWORDS'</span>):</span><br><span class="line">            <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">100</span>, <span class="number">2</span>):</span><br><span class="line">                url = self.base_url.format(keyword, page)</span><br><span class="line">                <span class="keyword">yield</span> scrapy.Request(url, callback=self.parse, dont_filter=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure></p><p>首先定义了一个 base_url，即商品列表的 URL，其后拼接一个搜索的关键词就是该关键词在京东的搜索结果商品列表页面。</p><p>关键词用 KEYWORDS 标识，定义为一个列表。定义在 settings.py 里面，如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KEYWORDS = [<span class="string">'iPhone XS'</span>]</span><br></pre></td></tr></table></figure></p><p>在 start_requests() 方法里，首先遍历了关键词，构造并生成了 Request。</p><h1 id="对接-Selenium"><a href="#对接-Selenium" class="headerlink" title="对接 Selenium"></a>对接 Selenium</h1><p>接下来需要成功处理这些请求的抓取。这里选择对接 Selenium 进行抓取，采用 Downloader Middleware 来实现。在 Middleware 里面的 process_request() 方法里对每个抓取请求进行处理，启动浏览器并进行页面渲染，再将渲染后的结果构造一个 HtmlResponse 对象返回。代码实现如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.common.exceptions <span class="keyword">import</span> TimeoutException</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="keyword">from</span> logging <span class="keyword">import</span> getLogger</span><br><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> HtmlResponse</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SeleniumMiddleware</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.logger = getLogger(__name__)</span><br><span class="line">        self.timeout = <span class="number">10</span></span><br><span class="line">        self.browser = webdriver.Chrome()</span><br><span class="line">        self.browser.set_page_load_timeout(self.timeout)</span><br><span class="line">        self.wait = WebDriverWait(self.browser, self.timeout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__del__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.browser.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        用 Chrome 抓取页面</span></span><br><span class="line"><span class="string">        :param request: Request 对象</span></span><br><span class="line"><span class="string">        :param spider: Spider 对象</span></span><br><span class="line"><span class="string">        :return: HtmlResponse</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.logger.debug(<span class="string">'Chrome is Starting'</span>)</span><br><span class="line">        page = request.meta.get(<span class="string">'page'</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.browser.get(request.url)</span><br><span class="line">        <span class="keyword">except</span> TimeoutException <span class="keyword">as</span> e:</span><br><span class="line">            print(<span class="string">'请求超时'</span>)</span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> HtmlResponse(url=request.url, body=self.browser.page_source, encoding=<span class="string">'utf-8'</span>, request=request)</span><br></pre></td></tr></table></figure></p><p>这样就定义好了一个 Selenium 中间件，然后在 settings.py 中设置调用 SeleniumMiddleware，如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">   <span class="string">'jd_selenium.middlewares.SeleniumMiddleware'</span>: <span class="number">543</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="解析页面"><a href="#解析页面" class="headerlink" title="解析页面"></a>解析页面</h1><p>Response 对象回传给 Spider 中的回调函数进行解析。因此这步来实现其回调函数，对网页来进行解析，代码如下所示。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    selector = response.xpath(<span class="string">"//*[@id='J_goodsList']/ul//li[@class='gl-item']"</span>)</span><br><span class="line">    print(<span class="string">'---------------------------------------------------'</span>)</span><br><span class="line">    print(len(selector))</span><br><span class="line">    print(<span class="string">'---------------------------------------------------'</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> selector:</span><br><span class="line">        title = <span class="string">''</span>.join(item.xpath(<span class="string">".//div[4]/a/em//text()"</span>).getall())</span><br><span class="line">        price = <span class="string">''</span>.join(item.xpath(<span class="string">".//div[3]/strong//text()"</span>).getall())</span><br><span class="line">        comments = <span class="string">''</span>.join(item.xpath(<span class="string">".//div[5]/strong//text()"</span>).getall())</span><br><span class="line">        item = JdSeleniumItem(title=title, price=price, comments=comments)</span><br><span class="line">        <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure></p><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/scrapy_jd1.png" alt=""></p><p>在上面代码中，对 selector 的长度进行了输出，因为看到的商品数量明显少于直接打开浏览器的商品结果，后来发现在京东输入关键词点击搜索之后，页面的返回时分成两个步骤，它首先会直接返回一个静态的页面，页面的商品信息是30个，之后，当我们鼠标向下滑动时，后台会通过 ajax 技术加载另外的30个商品，因此直接打开浏览器看到的商品列表其实是分两次加载出来的，而且只是在鼠标下滑到一定位置的时候才会加载那另外的30个商品。</p><h1 id="执行-js-脚本"><a href="#执行-js-脚本" class="headerlink" title="执行 js 脚本"></a>执行 js 脚本</h1><p>因此，为了实现鼠标下滑的现象，需要 Selenium 执行一段 js 代码，将网页下拉到一定位置。</p><p>修改后的 SeleniumMiddleware 代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    用 Phantomjs 抓取页面</span></span><br><span class="line"><span class="string">    :param request: Request 对象</span></span><br><span class="line"><span class="string">    :param spider: Spider 对象</span></span><br><span class="line"><span class="string">    :return: HtmlResponse</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    self.logger.debug(<span class="string">'Phantomjs is Starting'</span>)</span><br><span class="line">    page = request.meta.get(<span class="string">'page'</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        self.browser.get(request.url)</span><br><span class="line">        self.browser.execute_script(<span class="string">'window.scrollTo(0, document.body.scrollHeight)'</span>)</span><br><span class="line">    <span class="keyword">except</span> TimeoutException <span class="keyword">as</span> e:</span><br><span class="line">        print(<span class="string">'请求超时'</span>)</span><br><span class="line">        self.browser.execute_script(<span class="string">'window.stop()'</span>)</span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> HtmlResponse(url=request.url, body=self.browser.page_source, encoding=<span class="string">'utf-8'</span>, request=request)</span><br></pre></td></tr></table></figure></p><p>代码修改后选择器的长度变成了59，正常应该是60，可能是哪里出问题了。-.-</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/scrapy_jd2.png" alt=""></p><h1 id="存入数据库"><a href="#存入数据库" class="headerlink" title="存入数据库"></a>存入数据库</h1><p>实现一个 Item Pipeline，将结果保存到 MongoDB，如下所示。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">from</span> .settings <span class="keyword">import</span> mongo_uri, mongo_db</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdSeleniumPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line">        self.collection = self.db[<span class="string">'collection'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.collection.insert(dict(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client.close()</span><br></pre></td></tr></table></figure></p><p>编写完成需要在 settings.py 中开启调用。如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'jd_selenium.pipelines.JdSeleniumPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>其中 mongo_uri 和 mongo_db 分别定义:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mongo_uri = <span class="string">'localhost'</span></span><br><span class="line">mongi_db = <span class="string">'jd_scrapy'</span></span><br></pre></td></tr></table></figure></p><p>如图所示，已经将数据存入了数据库。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/jd_scrapy_pic.png" alt=""></p><h1 id="运行效果"><a href="#运行效果" class="headerlink" title="运行效果"></a>运行效果</h1><hr>]]></content>
    
    <summary type="html">
    
      Scrapy 抓取页面的方式和 requests 库类似，都是直接模拟 HTTP 请求，而 Scrapy 也不能直接抓取 JavaScript 动态渲染的页面。Scrapy 抓取动态渲染的页面有两种方式，其中一种就是此篇要学习的 Scrapy 对接 Selenium 框架进行爬虫。
    
    </summary>
    
      <category term="Scrapy" scheme="https://tangx1.com/categories/Scrapy/"/>
    
    
      <category term="Scrapy" scheme="https://tangx1.com/tags/Scrapy/"/>
    
      <category term="Selenium" scheme="https://tangx1.com/tags/Selenium/"/>
    
  </entry>
  
  <entry>
    <title>Pyppeteer</title>
    <link href="https://tangx1.com/pyppeteer/"/>
    <id>https://tangx1.com/pyppeteer/</id>
    <published>2019-01-18T12:30:20.000Z</published>
    <updated>2019-01-21T06:25:23.724Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Pyppeteer简介"><a href="#Pyppeteer简介" class="headerlink" title="Pyppeteer简介"></a>Pyppeteer简介</h1><h2 id="Puppeteer"><a href="#Puppeteer" class="headerlink" title="Puppeteer"></a>Puppeteer</h2><p>Puppeteer 是 Chrome 团队开发的一个 node 库，可以通过 api 来控制浏览器的行为，比如点击，跳转，刷新，在控制台执行js脚本等等。使用这个神器作为爬虫访问页面收集数据十分方便。</p><h2 id="Pyppeteer"><a href="#Pyppeteer" class="headerlink" title="Pyppeteer"></a>Pyppeteer</h2><p><a href="https://github.com/miyakogi/pyppeteer" target="_blank" rel="noopener">Pyppeteer</a> 是 Puppeteer 的 python 版本。中文资料可以说非常少，只能看官方文档了。不过还是可以看看 Puppeteer 的教程，功能原理相同，只是因为语言差异实现方式不同。</p><p>Pyppeteer文档: <a href="https://miyakogi.github.io/pyppeteer/" target="_blank" rel="noopener">点我</a></p><h1 id="Pyppeteer安装"><a href="#Pyppeteer安装" class="headerlink" title="Pyppeteer安装"></a>Pyppeteer安装</h1><p>以下内容来自 pyppeteer 文档。</p><p>Pyppeteer 需要 python3.6+，安装方式如下。</p><p>使用 pip 方式安装：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 -m pip install pyppeteer</span><br></pre></td></tr></table></figure></p><p>使用来自 <a href="https://github.com/miyakogi/pyppeteer" target="_blank" rel="noopener">github</a> 的最新版本：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 -m pip install -U git+https://github.com/miyakogi/pyppeteer.git@dev</span><br></pre></td></tr></table></figure></p><h1 id="Pyppeteer使用"><a href="#Pyppeteer使用" class="headerlink" title="Pyppeteer使用"></a>Pyppeteer使用</h1><p><strong>注意：</strong> 当你首次运行 pyppeteer 时，它会下载 Chromium 浏览器的最近版本。如果你不想它这么做的话，请在使用 pyppeteer 的脚本之前运行 <code>pyppeteer-install</code> 命令。</p><h2 id="示例-1-打开网页并截图。"><a href="#示例-1-打开网页并截图。" class="headerlink" title="示例 1 打开网页并截图。"></a>示例 1 打开网页并截图。</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">from</span> pyppeteer <span class="keyword">import</span> launch</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    browser = <span class="keyword">await</span> launch()</span><br><span class="line">    page = <span class="keyword">await</span> browser.newPage()</span><br><span class="line">    <span class="keyword">await</span> page.goto(<span class="string">'http://example.com'</span>)</span><br><span class="line">    <span class="keyword">await</span> page.screenshot(&#123;<span class="string">'path'</span>: <span class="string">'example.png'</span>&#125;)</span><br><span class="line">    <span class="keyword">await</span> browser.close()</span><br><span class="line"></span><br><span class="line">asyncio.get_event_loop().run_until_complete(main())</span><br></pre></td></tr></table></figure><h2 id="示例-2-执行-js-脚本"><a href="#示例-2-执行-js-脚本" class="headerlink" title="示例 2 执行 js 脚本"></a>示例 2 执行 js 脚本</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">from</span> pyppeteer <span class="keyword">import</span> launch</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    browser = <span class="keyword">await</span> launch()</span><br><span class="line">    page = <span class="keyword">await</span> browser.newPage()</span><br><span class="line">    <span class="keyword">await</span> page.goto(<span class="string">'http://example.com'</span>)</span><br><span class="line">    <span class="keyword">await</span> page.screenshot(&#123;<span class="string">'path'</span>: <span class="string">'example.png'</span>&#125;)</span><br><span class="line"></span><br><span class="line">    dimensions = <span class="keyword">await</span> page.evaluate(<span class="string">'''() =&gt; &#123;</span></span><br><span class="line"><span class="string">        return &#123;</span></span><br><span class="line"><span class="string">            width: document.documentElement.clientWidth,</span></span><br><span class="line"><span class="string">            height: document.documentElement.clientHeight,</span></span><br><span class="line"><span class="string">            deviceScaleFactor: window.devicePixelRatio,</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">    &#125;'''</span>)</span><br><span class="line"></span><br><span class="line">    print(dimensions)</span><br><span class="line">    <span class="comment"># &gt;&gt;&gt; &#123;'width': 800, 'height': 600, 'deviceScaleFactor': 1&#125;</span></span><br><span class="line">    <span class="keyword">await</span> browser.close()</span><br><span class="line"></span><br><span class="line">asyncio.get_event_loop().run_until_complete(main())</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    <summary type="html">
    
      Puppeteer 是一个 node.js 的库，支持调用 Chrome 的 API 来操纵 Web，相比较 Selenium 或是 PhantomJs，它最大的特点就是它的操作 Dom 可以完全在内存中进行模拟既在V8引擎中处理而不打开浏览器，而且关键是这个是 Chrome 团队在维护，会拥有更好的兼容性和前景。
    
    </summary>
    
      <category term="爬虫" scheme="https://tangx1.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python" scheme="https://tangx1.com/tags/Python/"/>
    
      <category term="自动化" scheme="https://tangx1.com/tags/%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>使用with结构打开n个文件</title>
    <link href="https://tangx1.com/python-with/"/>
    <id>https://tangx1.com/python-with/</id>
    <published>2019-01-18T07:27:09.000Z</published>
    <updated>2019-01-19T04:56:23.402Z</updated>
    
    <content type="html"><![CDATA[<h1 id="对一个文件读写"><a href="#对一个文件读写" class="headerlink" title="对一个文件读写"></a>对一个文件读写</h1><p>有时候在爬虫中对最后数据的处理可能是将其存入某个文件内，那么此时就可以使用 with 语句对某个文件进行操作。</p><h2 id="写入一个文件"><a href="#写入一个文件" class="headerlink" title="写入一个文件"></a>写入一个文件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(filename1, <span class="string">'w'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(item)</span><br></pre></td></tr></table></figure><h2 id="读取一个文件"><a href="#读取一个文件" class="headerlink" title="读取一个文件"></a>读取一个文件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(filename1, <span class="string">'r'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    data = fp.read()</span><br></pre></td></tr></table></figure><h1 id="对多个文件进行读写"><a href="#对多个文件进行读写" class="headerlink" title="对多个文件进行读写"></a>对多个文件进行读写</h1><h2 id="将-A-文件内容写入-B-文件"><a href="#将-A-文件内容写入-B-文件" class="headerlink" title="将 A 文件内容写入 B 文件"></a>将 A 文件内容写入 B 文件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(A,<span class="string">'r'</span>) <span class="keyword">as</span> fp1 , open(B,<span class="string">'a'</span>) <span class="keyword">as</span> fp2:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fp1:</span><br><span class="line">        fp2.write(line)</span><br></pre></td></tr></table></figure><h2 id="同时读取-A-B-C-三个文件"><a href="#同时读取-A-B-C-三个文件" class="headerlink" title="同时读取 A B C 三个文件"></a>同时读取 A B C 三个文件</h2><p>同时打开三个文件，文件行数一样，要求实现每个文件依次读取一行，然后输出，我们先来看比较容易想到的写法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(A, <span class="string">'rb'</span>) <span class="keyword">as</span> fp1:</span><br><span class="line">    <span class="keyword">with</span> open(B, <span class="string">'rb'</span>) <span class="keyword">as</span> fp2:</span><br><span class="line">        <span class="keyword">with</span> open(C, <span class="string">'rb'</span>) <span class="keyword">as</span> fp3:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> fp1:</span><br><span class="line">                j = fp2.readline()</span><br><span class="line">                k = fp3.readline()</span><br><span class="line">                print(i, j, k)</span><br></pre></td></tr></table></figure><p>注意这里只能对单个文件进行for循环读取，不能写成：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, j, k <span class="keyword">in</span> fp1, fp2, fp3:</span><br><span class="line">    print(i, j, k)</span><br></pre></td></tr></table></figure></p><p>但可使用强大的zip操作：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, j, k <span class="keyword">in</span> zip(fp1, fp2, fp3):</span><br><span class="line">    print(i, j, k)</span><br></pre></td></tr></table></figure></p><p>这样层层的嵌套未免啰嗦，with结构支持一种更简洁的写法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(A, <span class="string">'rb'</span>) <span class="keyword">as</span> fp1, open(B, <span class="string">'rb'</span>) <span class="keyword">as</span> fp2, open(C, <span class="string">'rb'</span>) <span class="keyword">as</span> fp3:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> fp1:</span><br><span class="line">        j = fp2.readline()</span><br><span class="line">        k = fp3.readline()</span><br><span class="line">        print(i, j, k)</span><br></pre></td></tr></table></figure></p><p>或者使用更为优雅的写法，此时需要 <a href="https://docs.python.org/3/library/contextlib.html" target="_blank" rel="noopener">contextlib</a> 语法糖：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> contextlib improt ExitStack</span><br><span class="line"><span class="keyword">with</span> ExitStack() <span class="keyword">as</span> stack:</span><br><span class="line">    files = [stack.enter_context(open(fname)) <span class="keyword">for</span> fname <span class="keyword">in</span> [A, B, C]]</span><br><span class="line">    <span class="keyword">for</span> i, j, k <span class="keyword">in</span> zip(files[<span class="number">0</span>], files[<span class="number">1</span>], files[<span class="number">2</span>]):</span><br><span class="line">        print(i, j, k)</span><br></pre></td></tr></table></figure></p><hr>]]></content>
    
    <summary type="html">
    
      with 语句适用于对资源进行访问的场合，确保不管使用过程中是否发生异常都会执行必要的“清理”操作，释放资源，比如文件使用后自动关闭、线程中锁的自动获取和释放等。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>mysql</title>
    <link href="https://tangx1.com/mysql/"/>
    <id>https://tangx1.com/mysql/</id>
    <published>2019-01-16T18:24:07.000Z</published>
    <updated>2019-01-20T10:03:04.411Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>教程节选自 w3school 的 <a href="http://www.w3school.com.cn/sql/index.asp" target="_blank" rel="noopener">sql教程</a></p></blockquote><h1 id="SQL-语法"><a href="#SQL-语法" class="headerlink" title="SQL 语法"></a>SQL 语法</h1><h2 id="数据库表"><a href="#数据库表" class="headerlink" title="数据库表"></a>数据库表</h2><p>一个数据库通常包含一个或多个表。每个表由一个名字标识（例如“客户”或者“订单”）。表包含带有数据的记录（行）。</p><p>下面的例子是一个名为 “Persons” 的表：</p><table><thead><tr><th>Id</th><th>LastName</th><th>FirstName</th><th>Address</th><th>City</th></tr></thead><tbody><tr><td>1</td><td>Adams</td><td>John</td><td>Oxford Street</td><td>London</td></tr><tr><td>2</td><td>Bush</td><td>George</td><td>Fifth Avenue</td><td>New York</td></tr><tr><td>3</td><td>Carter</td><td>Thomas</td><td>Changan Street</td><td>Beijing</td></tr></tbody></table><p>上面的表包含三条记录（每一条对应一个人）和五个列（Id、姓、名、地址和城市）。</p><h2 id="SQL-语句"><a href="#SQL-语句" class="headerlink" title="SQL 语句"></a>SQL 语句</h2><p>您需要在数据库上执行的大部分工作都由 SQL 语句完成。</p><p>下面的语句从表中选取 LastName 列的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> LastName <span class="keyword">FROM</span> Persons</span><br></pre></td></tr></table></figure><p>结果集类似这样：</p><table><thead><tr><th>LastName</th></tr></thead><tbody><tr><td>Adams</td></tr><tr><td>Bush</td></tr><tr><td>Carter</td></tr></tbody></table><h2 id="SQL-DML-和-DDL"><a href="#SQL-DML-和-DDL" class="headerlink" title="SQL DML 和 DDL"></a>SQL DML 和 DDL</h2><p>可以把 SQL 分为两个部分：数据操作语言 (DML) 和 数据定义语言 (DDL)。<br>SQL (结构化查询语言)是用于执行查询的语法。但是 SQL 语言也包含用于更新、插入和删除记录的语法。</p><p>查询和更新指令构成了 SQL 的 DML 部分：</p><ul><li>SELECT - 从数据库表中获取数据</li><li>UPDATE - 更新数据库表中的数据</li><li>DELETE - 从数据库表中删除数据</li><li>INSERT INTO - 向数据库表中插入数据</li></ul><p>SQL 的数据定义语言 (DDL) 部分使我们有能力创建或删除表格。我们也可以定义索引（键），规定表之间的链接，以及施加表间的约束。</p><p>SQL 中最重要的 DDL 语句:</p><ul><li>CREATE DATABASE - 创建新数据库</li><li>ALTER DATABASE - 修改数据库</li><li>CREATE TABLE - 创建新表</li><li>ALTER TABLE - 变更（改变）数据库表</li><li>DROP TABLE - 删除表</li><li>CREATE INDEX - 创建索引（搜索键）</li><li>DROP INDEX - 删除索引</li></ul><h1 id="SQL-SELECT-语句"><a href="#SQL-SELECT-语句" class="headerlink" title="SQL SELECT 语句"></a>SQL SELECT 语句</h1><p>SELECT 语句用于从表中选取数据。</p><p>结果被存储在一个结果表中（称为结果集）。</p><h2 id="SQL-SELECT-语法"><a href="#SQL-SELECT-语法" class="headerlink" title="SQL SELECT 语法"></a>SQL SELECT 语法</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> 列名称 <span class="keyword">FROM</span> 表名称</span><br></pre></td></tr></table></figure><p>以及：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> 表名称</span><br></pre></td></tr></table></figure></p><p><strong>注释</strong>：SQL 语句对大小写不敏感。SELECT 等效于 select。</p><h2 id="SQL-SELECT-实例"><a href="#SQL-SELECT-实例" class="headerlink" title="SQL SELECT 实例"></a>SQL SELECT 实例</h2><p>如需获取名为 “LastName” 和 “FirstName” 的列的内容（从名为 “Persons” 的数据库表），请使用类似这样的 SELECT 语句：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> LastName,FirstName <span class="keyword">FROM</span> Persons</span><br></pre></td></tr></table></figure></p><p><strong>“Persons” 表</strong></p><table><thead><tr><th>Id</th><th>LastName</th><th>FirstName</th><th>Address</th><th>City</th></tr></thead><tbody><tr><td>1</td><td>Adams</td><td>John</td><td>Oxford Street</td><td>London</td></tr><tr><td>2</td><td>Bush</td><td>George</td><td>Fifth Avenue</td><td>New York</td></tr><tr><td>3</td><td>Carter</td><td>Thomas</td><td>Changan Street</td><td>Beijing</td></tr></tbody></table><p><strong>结果：</strong></p><table><thead><tr><th>LastName</th><th>FirstName</th></tr></thead><tbody><tr><td>Adams</td><td>John</td></tr><tr><td>Bush</td><td>George</td></tr><tr><td>Carter</td><td>Thomas</td></tr></tbody></table><h2 id="SQL-SELECT-实例-1"><a href="#SQL-SELECT-实例-1" class="headerlink" title="SQL SELECT * 实例"></a>SQL SELECT * 实例</h2><p>现在我们希望从 “Persons” 表中选取所有的列。</p><p>请使用符号 * 取代列的名称，就像这样：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Persons</span><br></pre></td></tr></table></figure></p><p><strong>提示</strong>：星号（ * ）是选取所有列的快捷方式。</p><p><strong>结果：</strong></p><table><thead><tr><th>Id</th><th>LastName</th><th>FirstName</th><th>Address</th><th>City</th></tr></thead><tbody><tr><td>1</td><td>Adams</td><td>John</td><td>Oxford Street</td><td>London</td></tr><tr><td>2</td><td>Bush</td><td>George</td><td>Fifth Avenue</td><td>New York</td></tr><tr><td>3</td><td>Carter</td><td>Thomas</td><td>Changan Street</td><td>Beijing</td></tr></tbody></table><h1 id="SQL-SELECT-DISTINCT-语句"><a href="#SQL-SELECT-DISTINCT-语句" class="headerlink" title="SQL SELECT DISTINCT 语句"></a>SQL SELECT DISTINCT 语句</h1><p>在表中，可能会包含重复值。这并不成问题，不过，有时您也许希望仅仅列出不同（distinct）的值。</p><p>关键词 DISTINCT 用于返回唯一不同的值。</p><h2 id="语法："><a href="#语法：" class="headerlink" title="语法："></a>语法：</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> 列名称 <span class="keyword">FROM</span> 表名称</span><br></pre></td></tr></table></figure><h2 id="使用-DISTINCT-关键词"><a href="#使用-DISTINCT-关键词" class="headerlink" title="使用 DISTINCT 关键词"></a>使用 DISTINCT 关键词</h2><p>如果要从 “Company” 列中选取所有的值，我们需要使用 SELECT 语句：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> Company <span class="keyword">FROM</span> Orders</span><br></pre></td></tr></table></figure></p><p><strong>“Orders”表</strong></p><table><thead><tr><th>Company</th><th>OrderNumber</th></tr></thead><tbody><tr><td>IBM</td><td>3532</td></tr><tr><td>W3School</td><td>2356</td></tr><tr><td>Apple</td><td>4698</td></tr><tr><td>W3School</td><td>6953</td></tr></tbody></table><p><strong>结果：</strong></p><table><thead><tr><th>Company</th></tr></thead><tbody><tr><td>IBM</td></tr><tr><td>W3School</td></tr><tr><td>Apple</td></tr><tr><td>W3School</td></tr></tbody></table><p>请注意，在结果集中，W3School 被列出了两次。</p><p>如需从 Company” 列中仅选取唯一不同的值，我们需要使用 SELECT DISTINCT 语句：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> Company <span class="keyword">FROM</span> Orders</span><br></pre></td></tr></table></figure></p><p><strong>结果：</strong></p><table><thead><tr><th>Company</th></tr></thead><tbody><tr><td>IBM</td></tr><tr><td>W3School</td></tr><tr><td>Apple</td></tr></tbody></table><p>现在，在结果集中，”W3School” 仅被列出了一次。</p><h1 id="SQL-WHERE-子句"><a href="#SQL-WHERE-子句" class="headerlink" title="SQL WHERE 子句"></a>SQL WHERE 子句</h1><p>如需有条件地从表中选取数据，可将 WHERE 子句添加到 SELECT 语句。</p><h2 id="语法：-1"><a href="#语法：-1" class="headerlink" title="语法："></a>语法：</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> 列名称 <span class="keyword">FROM</span> 表名称 <span class="keyword">WHERE</span> 列 运算符 值</span><br></pre></td></tr></table></figure><p>下面的运算符可在 WHERE 子句中使用：</p><table><thead><tr><th>操作符</th><th>描述</th></tr></thead><tbody><tr><td>=</td><td>等于</td></tr><tr><td>&lt;&gt;</td><td>不等于</td></tr><tr><td>&gt;</td><td>大于</td></tr><tr><td>&lt;</td><td>小于</td></tr><tr><td>&gt;=</td><td>大于等于</td></tr><tr><td>&lt;=</td><td>小于等于</td></tr><tr><td>BETWWEEN</td><td>在某个范围内</td></tr><tr><td>LIKE</td><td>搜索某种模式</td></tr></tbody></table><p><strong>注释</strong>：在某些版本的 SQL 中，操作符 &lt;&gt; 可以写为 !=。</p><h2 id="使用-WHERE-子句"><a href="#使用-WHERE-子句" class="headerlink" title="使用 WHERE 子句"></a>使用 WHERE 子句</h2><p>如果只希望选取居住在城市 “Beijing” 中的人，我们需要向 SELECT 语句添加 WHERE 子句：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Persons <span class="keyword">WHERE</span> City=<span class="string">'Beijing'</span></span><br></pre></td></tr></table></figure></p><p><strong>“Persons” 表</strong></p><table><thead><tr><th>LastName</th><th>FirstName</th><th>Address</th><th>City</th><th>Year</th></tr></thead><tbody><tr><td>Adams</td><td>John</td><td>Oxford Street</td><td>London</td><td>1970</td></tr><tr><td>Bush</td><td>George</td><td>Fifth Avenue</td><td>New York</td><td>1975</td></tr><tr><td>Carter</td><td>Thomas</td><td>Changan Street</td><td>Beijing</td><td>1980</td></tr><tr><td>Gates</td><td>Bill</td><td>Xuanwumen 10</td><td>Beijing</td><td>1985</td></tr></tbody></table><p><strong>结果：</strong></p><table><thead><tr><th>LastName</th><th>FirstName</th><th>Address</th><th>City</th><th>Year</th></tr></thead><tbody><tr><td>Carter</td><td>Thomas</td><td>Changan Street</td><td>Beijing</td><td>1980</td></tr><tr><td>Gates</td><td>Bill</td><td>Xuanwumen 10</td><td>Beijing</td><td>1985</td></tr></tbody></table><h2 id="引号的使用"><a href="#引号的使用" class="headerlink" title="引号的使用"></a>引号的使用</h2><p>请注意，我们在例子中的条件值周围使用的是单引号。</p><p>SQL 使用单引号来环绕<strong>文本值</strong>（大部分数据库系统也接受双引号）。如果是<strong>数值</strong>，请不要使用引号。</p><p><strong>文本值</strong>：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这是正确的：</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Persons <span class="keyword">WHERE</span> FirstName=<span class="string">'Bush'</span></span><br><span class="line"></span><br><span class="line">这是错误的：</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Persons <span class="keyword">WHERE</span> FirstName=Bush</span><br></pre></td></tr></table></figure></p><p><strong>数值</strong>:<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这是正确的：</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Persons <span class="keyword">WHERE</span> <span class="keyword">Year</span>&gt;<span class="number">1965</span></span><br><span class="line"></span><br><span class="line">这是错误的：</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Persons <span class="keyword">WHERE</span> <span class="keyword">Year</span>&gt;<span class="string">'1965'</span></span><br></pre></td></tr></table></figure></p><h1 id="SQL-AND-amp-OR-运算符"><a href="#SQL-AND-amp-OR-运算符" class="headerlink" title="SQL AND &amp; OR 运算符"></a>SQL AND &amp; OR 运算符</h1><p><strong>AND 和 OR 运算符用于基于一个以上的条件对记录进行过滤。</strong></p><p>AND 和 OR 可在 WHERE 子语句中把两个或多个条件结合起来。</p><p>如果第一个条件和第二个条件都成立，则 AND 运算符显示一条记录。</p><p>如果第一个条件和第二个条件中只要有一个成立，则 OR 运算符显示一条记录。</p><p><strong>原始的表 (用在例子中的)：</strong></p><table><thead><tr><th>LastName</th><th>FirstName</th><th>Address</th><th>City</th></tr></thead><tbody><tr><td>Adams</td><td>John</td><td>Oxford Street</td><td>London</td></tr><tr><td>Bush</td><td>George</td><td>Fifth Avenue</td><td>New York</td></tr><tr><td>Carter</td><td>Thomas</td><td>Changan Street</td><td>Beijing</td></tr><tr><td>Carter</td><td>William</td><td>Xuanwumen 10</td><td>Beijing</td></tr></tbody></table><h2 id="AND-运算符实例"><a href="#AND-运算符实例" class="headerlink" title="AND 运算符实例"></a>AND 运算符实例</h2><p>使用 AND 来显示所有姓为 “Carter” 并且名为 “Thomas” 的人：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Persons <span class="keyword">WHERE</span> FirstName=<span class="string">'Thomas'</span> <span class="keyword">AND</span> LastName=<span class="string">'Carter'</span></span><br></pre></td></tr></table></figure></p><p><strong>结果：</strong></p><table><thead><tr><th>LastName</th><th>FirstName</th><th>Address</th><th>City</th></tr></thead><tbody><tr><td>Carter</td><td>Thomas</td><td>Changan Street</td><td>Beijing</td></tr></tbody></table><h2 id="OR-运算符实例"><a href="#OR-运算符实例" class="headerlink" title="OR 运算符实例"></a>OR 运算符实例</h2><p>使用 OR 来显示所有姓为 “Carter” 或者名为 “Thomas” 的人：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Persons <span class="keyword">WHERE</span> firstname=<span class="string">'Thomas'</span> <span class="keyword">OR</span> lastname=<span class="string">'Carter'</span></span><br></pre></td></tr></table></figure></p><p><strong>结果：</strong></p><table><thead><tr><th>LastName</th><th>FirstName</th><th>Address</th><th>City</th></tr></thead><tbody><tr><td>Carter</td><td>Thomas</td><td>Changan Street</td><td>Beijing</td></tr><tr><td>Carter</td><td>William</td><td>Xuanwumen 10</td><td>Beijing</td></tr></tbody></table><h2 id="结合-AND-和-OR-运算符"><a href="#结合-AND-和-OR-运算符" class="headerlink" title="结合 AND 和 OR 运算符"></a>结合 AND 和 OR 运算符</h2><p>我们也可以把 AND 和 OR 结合起来（使用圆括号来组成复杂的表达式）:<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Persons <span class="keyword">WHERE</span> (FirstName=<span class="string">'Thomas'</span> <span class="keyword">OR</span> FirstName=<span class="string">'William'</span>)</span><br><span class="line"><span class="keyword">AND</span> LastName=<span class="string">'Carter'</span></span><br></pre></td></tr></table></figure></p><p><strong>结果：</strong></p><table><thead><tr><th>LastName</th><th>FirstName</th><th>Address</th><th>City</th></tr></thead><tbody><tr><td>Carter</td><td>Thomas</td><td>Changan Street</td><td>Beijing</td></tr><tr><td>Carter</td><td>William</td><td>Xuanwumen 10</td><td>Beijing</td></tr></tbody></table><h1 id="SQL-ORDER-BY-子句"><a href="#SQL-ORDER-BY-子句" class="headerlink" title="SQL ORDER BY 子句"></a>SQL ORDER BY 子句</h1><p><strong>ORDER BY 语句用于对结果集进行排序。</strong></p><p>ORDER BY 语句用于根据指定的列对结果集进行排序。</p><p>ORDER BY 语句默认按照升序对记录进行排序。</p><p>如果您希望按照降序对记录进行排序，可以使用 DESC 关键字。</p><p><strong>原始的表 (用在例子中的)：</strong></p><p>Orders 表:</p><table><thead><tr><th>Company</th><th>OrderNumber</th></tr></thead><tbody><tr><td>IBM</td><td>3532</td></tr><tr><td>W3School</td><td>2356</td></tr><tr><td>Apple</td><td>4698</td></tr><tr><td>W3School</td><td>6953</td></tr></tbody></table><h2 id="实例-1"><a href="#实例-1" class="headerlink" title="实例 1"></a>实例 1</h2><p>以字母顺序显示公司名称：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> Company, OrderNumber <span class="keyword">FROM</span> Orders <span class="keyword">ORDER</span> <span class="keyword">BY</span> Company</span><br></pre></td></tr></table></figure></p><p><strong>结果：</strong></p><table><thead><tr><th>Company</th><th>OrderNumber</th></tr></thead><tbody><tr><td>Apple</td><td>4698</td></tr><tr><td>IBM</td><td>3532</td></tr><tr><td>W3School</td><td>6953</td></tr><tr><td>W3School</td><td>2356</td></tr></tbody></table><h2 id="实例-2"><a href="#实例-2" class="headerlink" title="实例 2"></a>实例 2</h2><p>以字母顺序显示公司名称（Company），并以数字顺序显示顺序号（OrderNumber）：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> Company, OrderNumber <span class="keyword">FROM</span> Orders <span class="keyword">ORDER</span> <span class="keyword">BY</span> Company, OrderNumber</span><br></pre></td></tr></table></figure></p><p><strong>结果：</strong></p><table><thead><tr><th>Company</th><th>OrderNumber</th></tr></thead><tbody><tr><td>Apple</td><td>4698</td></tr><tr><td>IBM</td><td>3532</td></tr><tr><td>W3School</td><td>2356</td></tr><tr><td>W3School</td><td>6953</td></tr></tbody></table><h2 id="实例-3"><a href="#实例-3" class="headerlink" title="实例 3"></a>实例 3</h2><p>以逆字母顺序显示公司名称：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> Company, OrderNumber <span class="keyword">FROM</span> Orders <span class="keyword">ORDER</span> <span class="keyword">BY</span> Company <span class="keyword">DESC</span></span><br></pre></td></tr></table></figure></p><p><strong>结果：</strong></p><table><thead><tr><th>Company</th><th>OrderNumber</th></tr></thead><tbody><tr><td>W3School</td><td>6953</td></tr><tr><td>W3School</td><td>2356</td></tr><tr><td>IBM</td><td>3532</td></tr><tr><td>Apple</td><td>4698</td></tr></tbody></table><h2 id="实例-4"><a href="#实例-4" class="headerlink" title="实例 4"></a>实例 4</h2><p>以逆字母顺序显示公司名称，并以数字顺序显示顺序号：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> Company, OrderNumber <span class="keyword">FROM</span> Orders <span class="keyword">ORDER</span> <span class="keyword">BY</span> Company <span class="keyword">DESC</span>, OrderNumber <span class="keyword">ASC</span></span><br></pre></td></tr></table></figure></p><p><strong>结果：</strong></p><table><thead><tr><th>Company</th><th>OrderNumber</th></tr></thead><tbody><tr><td>W3School</td><td>2356</td></tr><tr><td>W3School</td><td>6953</td></tr><tr><td>IBM</td><td>3532</td></tr><tr><td>Apple</td><td>4698</td></tr></tbody></table><p><strong>注意：</strong> 在以上的结果中有两个相等的公司名称 (W3School)。只有这一次，在第一列中有相同的值时，第二列是以升序排列的。如果第一列中有些值为 nulls 时，情况也是这样的。</p><h1 id="SQL-INSERT-INTO-语句"><a href="#SQL-INSERT-INTO-语句" class="headerlink" title="SQL INSERT INTO 语句"></a>SQL INSERT INTO 语句</h1><p>INSERT INTO 语句用于向表格中插入新的行。</p><h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> 表名称 <span class="keyword">VALUES</span> (值<span class="number">1</span>, 值<span class="number">2</span>,....)</span><br></pre></td></tr></table></figure><p>我们也可以指定所要插入数据的列：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> 表名称 (列<span class="number">1</span>, 列<span class="number">2</span>,...) <span class="keyword">VALUES</span> (值<span class="number">1</span>, 值<span class="number">2</span>,....)</span><br></pre></td></tr></table></figure></p><h2 id="插入新的行"><a href="#插入新的行" class="headerlink" title="插入新的行"></a>插入新的行</h2><p><strong>“Persons” 表</strong></p><table><thead><tr><th>LastName</th><th>FirstName</th><th>Address</th><th>City</th></tr></thead><tbody><tr><td>Carter</td><td>Thomas</td><td>Changan Street</td><td>Beijing</td></tr></tbody></table><p><strong>SQL 语句</strong><br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> Persons <span class="keyword">VALUES</span> (<span class="string">'Gates'</span>, <span class="string">'Bill'</span>, <span class="string">'Xuanwumen 10'</span>, <span class="string">'Beijing'</span>)</span><br></pre></td></tr></table></figure></p><p><strong>结果</strong></p><table><thead><tr><th>LastName</th><th>FirstName</th><th>Address</th><th>City</th></tr></thead><tbody><tr><td>Carter</td><td>Thomas</td><td>Changan Street</td><td>Beijing</td></tr><tr><td>Gates</td><td>Bill</td><td>Xuanwumen 10</td><td>Beijing</td></tr></tbody></table><h2 id="在指定的列中插入数据"><a href="#在指定的列中插入数据" class="headerlink" title="在指定的列中插入数据"></a>在指定的列中插入数据</h2><p><strong>“Persons” 表</strong></p><table><thead><tr><th>LastName</th><th>FirstName</th><th>Address</th><th>City</th></tr></thead><tbody><tr><td>Carter</td><td>Thomas</td><td>Changan Street</td><td>Beijing</td></tr><tr><td>Gates</td><td>Bill</td><td>Xuanwumen 10</td><td>Beijing</td></tr></tbody></table><p><strong>SQL 语句</strong><br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> Persons (LastName, Address) <span class="keyword">VALUES</span> (<span class="string">'Wilson'</span>, <span class="string">'Champs-Elysees'</span>)</span><br></pre></td></tr></table></figure></p><p><strong>结果</strong></p><table><thead><tr><th>LastName</th><th>FirstName</th><th>Address</th><th>City</th></tr></thead><tbody><tr><td>Carter</td><td>Thomas</td><td>Changan Street</td><td>Beijing</td></tr><tr><td>Gates</td><td>Bill</td><td>Xuanwumen 10</td><td>Beijing</td></tr><tr><td>Wilson</td><td></td><td>Champs-Elysees</td><td>——-</td></tr></tbody></table><h1 id="SQL-UPDATE-语句"><a href="#SQL-UPDATE-语句" class="headerlink" title="SQL UPDATE 语句"></a>SQL UPDATE 语句</h1><p>Update 语句用于修改表中的数据。</p><h2 id="语法-1"><a href="#语法-1" class="headerlink" title="语法"></a>语法</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">UPDATE</span> 表名称 <span class="keyword">SET</span> 列名称 = 新值 <span class="keyword">WHERE</span> 列名称 = 某值</span><br></pre></td></tr></table></figure><p><strong>“Persons” 表</strong></p><table><thead><tr><th>LastName</th><th>FirstName</th><th>Address</th><th>City</th></tr></thead><tbody><tr><td>Carter</td><td>Thomas</td><td>Changan Street</td><td>Beijing</td></tr><tr><td>Wilson</td><td></td><td>Champs-Elysees</td><td>——-</td></tr></tbody></table><h2 id="更新某一行中的一个列"><a href="#更新某一行中的一个列" class="headerlink" title="更新某一行中的一个列"></a>更新某一行中的一个列</h2><p>我们为 lastname 是 “Wilson” 的人添加 firstname：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">UPDATE</span> Person <span class="keyword">SET</span> FirstName = <span class="string">'Fred'</span> <span class="keyword">WHERE</span> LastName = <span class="string">'Wilson'</span></span><br></pre></td></tr></table></figure></p><p><strong>结果</strong></p><table><thead><tr><th>LastName</th><th>FirstName</th><th>Address</th><th>City</th></tr></thead><tbody><tr><td>Carter</td><td>Thomas</td><td>Changan Street</td><td>Beijing</td></tr><tr><td>Wilson</td><td>Fred</td><td>Champs-Elysees</td><td>——-</td></tr></tbody></table><h2 id="更新某一行中的若干列"><a href="#更新某一行中的若干列" class="headerlink" title="更新某一行中的若干列"></a>更新某一行中的若干列</h2><p>我们会修改地址（address），并添加城市名称（city）：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">UPDATE</span> Person <span class="keyword">SET</span> Address = <span class="string">'Zhongshan 23'</span>, City = <span class="string">'Nanjing'</span></span><br><span class="line"><span class="keyword">WHERE</span> LastName = <span class="string">'Wilson'</span></span><br></pre></td></tr></table></figure></p><p><strong>结果</strong></p><table><thead><tr><th>LastName</th><th>FirstName</th><th>Address</th><th>City</th></tr></thead><tbody><tr><td>Carter</td><td>Thomas</td><td>Changan Street</td><td>Beijing</td></tr><tr><td>Wilson</td><td>Fred</td><td>Zhongshan 23</td><td>Nanjing</td></tr></tbody></table><h1 id="SQL-DELETE-语句"><a href="#SQL-DELETE-语句" class="headerlink" title="SQL DELETE 语句"></a>SQL DELETE 语句</h1><p>DELETE 语句用于删除表中的行。</p><h2 id="语法-2"><a href="#语法-2" class="headerlink" title="语法"></a>语法</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> 表名称 <span class="keyword">WHERE</span> 列名称 = 值</span><br></pre></td></tr></table></figure><p><strong>“Persons” 表</strong></p><table><thead><tr><th>LastName</th><th>FirstName</th><th>Address</th><th>City</th></tr></thead><tbody><tr><td>Carter</td><td>Thomas</td><td>Changan Street</td><td>Beijing</td></tr><tr><td>Wilson</td><td>Fred</td><td>Zhongshan 23</td><td>Nanjing</td></tr></tbody></table><h2 id="删除某行"><a href="#删除某行" class="headerlink" title="删除某行"></a>删除某行</h2><p>“Fred Wilson” 会被删除：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> Person <span class="keyword">WHERE</span> LastName = <span class="string">'Wilson'</span></span><br></pre></td></tr></table></figure></p><p><strong>结果</strong></p><table><thead><tr><th>LastName</th><th>FirstName</th><th>Address</th><th>City</th></tr></thead><tbody><tr><td>Carter</td><td>Thomas</td><td>Changan Street</td><td>Beijing</td></tr></tbody></table><h2 id="删除所有行"><a href="#删除所有行" class="headerlink" title="删除所有行"></a>删除所有行</h2><p>可以在不删除表的情况下删除所有的行。这意味着表的结构、属性和索引都是完整的：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> table_name</span><br></pre></td></tr></table></figure><p>或者：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DELETE</span> * <span class="keyword">FROM</span> table_name</span><br></pre></td></tr></table></figure></p><hr>]]></content>
    
    <summary type="html">
    
      SQL 是一门 ANSI 的标准计算机语言，用来访问和操作数据库系统。SQL 语句用于取回和更新数据库中的数据。SQL 可与数据库程序协同工作，比如 MS Access、DB2、Informix、MS SQL Server、Oracle、Sybase 以及其他数据库系统。
    
    </summary>
    
      <category term="数据库" scheme="https://tangx1.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://tangx1.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>域名ssl证书部署到服务器</title>
    <link href="https://tangx1.com/ssl/"/>
    <id>https://tangx1.com/ssl/</id>
    <published>2019-01-15T01:11:41.000Z</published>
    <updated>2019-01-21T15:09:33.319Z</updated>
    
    <content type="html"><![CDATA[<h1 id="申请-SSL-证书"><a href="#申请-SSL-证书" class="headerlink" title="申请 SSL 证书"></a>申请 SSL 证书</h1><p>首先进入腾讯云 <a href="https://console.qcloud.com/ssl" target="_blank" rel="noopener">SSL域名管理</a>，点击按钮<code>申请证书</code>，选择第一个免费证书，我这里已经申请过了，简单演示一下，输入绑定域名，邮箱然后选填的都可以不填，直接下一步，如果域名已经绑定了服务器就选择自动，否则选择手动，最后一个不用管。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/nginx/ssl.gif" alt=""></p><p>提交之后耐心等待，审核通过腾讯云官方会发送邮件提醒。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/nginx/ssl_apply.png" alt=""></p><h1 id="下载-SSL-证书"><a href="#下载-SSL-证书" class="headerlink" title="下载 SSL 证书"></a>下载 SSL 证书</h1><p>申请成功后，点击右边的下载按钮下载证书文件。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/nginx/ssl_detail.png" alt=""></p><p>下载解压后，文件中有几个子文件夹，分别是 Apache、IIS、Nginx 服务器的证书文件。<a href="https://cloud.tencent.com/document/product/400/4143" target="_blank" rel="noopener">证书安装指引</a></p><p><img src="https://hexo-pics.nos-eastchina1.126.net/nginx/ssl_down.png" alt=""></p><h1 id="Nginx-证书部署"><a href="#Nginx-证书部署" class="headerlink" title="Nginx 证书部署"></a>Nginx 证书部署</h1><p>Nginx 文件夹内获得 SSL 证书文件 1_<a href="http://www.domain.com_bundle.crt" target="_blank" rel="noopener">www.domain.com_bundle.crt</a> 和私钥文件2_<a href="http://www.domain.com.key" target="_blank" rel="noopener">www.domain.com.key</a> 。 1_<a href="http://www.domain.com_bundle.crt" target="_blank" rel="noopener">www.domain.com_bundle.crt</a> 文件包括两段证书代码 “—–BEGIN CERTIFICATE—–” 和 “—–END CERTIFICATE—–”，2_<a href="http://www.domain.com.key" target="_blank" rel="noopener">www.domain.com.key</a> 文件包括一段私钥代码 “—–BEGIN RSA PRIVATE KEY—–” 和 “—–END RSA PRIVATE KEY—–”。</p><p>将这两个文件上传到服务器中，将域名 <a href="http://www.domain.com" target="_blank" rel="noopener">www.domain.com</a> 的证书文件 1_<a href="http://www.domain.com_bundle.crt" target="_blank" rel="noopener">www.domain.com_bundle.crt</a> 、私钥文件 2_<a href="http://www.domain.com.key" target="_blank" rel="noopener">www.domain.com.key</a> 保存到同一个目录，例如 /usr/local/nginx 目录下。</p><p>修改 Nginx 根目录下 conf/nginx.conf 文件，内容如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">        listen <span class="number">443</span>;</span><br><span class="line">        server_name www.domain.com; <span class="comment">#填写绑定证书的域名</span></span><br><span class="line">        ssl on;</span><br><span class="line">        ssl_certificate <span class="number">1</span>_www.domain.com_bundle.crt;</span><br><span class="line">        ssl_certificate_key <span class="number">2</span>_www.domain.com.key;</span><br><span class="line">        ssl_session_timeout <span class="number">5</span>m;</span><br><span class="line">        ssl_protocols TLSv1 TLSv1<span class="number">.1</span> TLSv1<span class="number">.2</span>; <span class="comment">#按照这个协议配置</span></span><br><span class="line">        ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE;<span class="comment">#按照这个套件配置</span></span><br><span class="line">        ssl_prefer_server_ciphers on;</span><br><span class="line">        location / &#123;</span><br><span class="line">            root   html; <span class="comment">#站点目录</span></span><br><span class="line">            index  index.html index.htm;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><p>配置完成后，请先执行命令 nginx –t 测试 Nginx 配置是否有误。若无报错，重启 Nginx 之后，即可使用 <a href="https://www.domain.com" target="_blank" rel="noopener">https://www.domain.com</a> 来访问。</p><p>如果这里你的 https 域名已经可以正常访问了，那就不用往下看了。</p><h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>我重启了 Nginx 服务器后，重新使用 https 访问我的网址，可以正常访问，但是直接输入我的网址域名，不加 http 或者 https，网页提示“400 Bad Request: The plain HTTP request was sent to HTTPS port”，如图。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/400.png" alt=""></p><blockquote><p>引自 <a href="https://blog.csdn.net/system1024/article/details/52636147" target="_blank" rel="noopener">https://blog.csdn.net/system1024/article/details/52636147</a></p></blockquote><p>解决方法是修改服务器的 /usr/local/nginx/nginx.conf 文件如下。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">        listen       <span class="number">80</span> default_server;</span><br><span class="line">        listen       <span class="number">443</span> ssl; <span class="comment"># 443 后面加上 ssl</span></span><br><span class="line">        <span class="comment">#ssl          on;       # 删除此行</span></span><br><span class="line">        server_name  tangx1.com;</span><br><span class="line">        root         /usr/share/nginx/html;</span><br><span class="line">        ssl_certificate <span class="number">1</span>_tangx1.com_bundle.crt;</span><br><span class="line">        ssl_certificate_key <span class="number">2</span>_tangx1.com.key;</span><br><span class="line">        ssl_session_timeout <span class="number">5</span>m;</span><br><span class="line">        ssl_protocols TLSv1 TLSv1<span class="number">.1</span> TLSv1<span class="number">.2</span>;</span><br><span class="line">        ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE;</span><br><span class="line">        ssl_prefer_server_ciphers on;</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure></p><hr>]]></content>
    
    <summary type="html">
    
      用 Chrome 打开网站总是提示不安全，看着烦心，干脆把域名挂上 https 之后就不提示了。此篇记录了腾讯云的证书部署过程，部署部分参考了腾讯云的官方文档，文章末尾记录了遇到的小问题。
    
    </summary>
    
      <category term="Nginx" scheme="https://tangx1.com/categories/Nginx/"/>
    
    
      <category term="服务器" scheme="https://tangx1.com/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
      <category term="Nginx" scheme="https://tangx1.com/tags/Nginx/"/>
    
  </entry>
  
  <entry>
    <title>Python中的网络编程</title>
    <link href="https://tangx1.com/socket/"/>
    <id>https://tangx1.com/socket/</id>
    <published>2019-01-12T05:48:00.000Z</published>
    <updated>2019-01-12T06:34:49.923Z</updated>
    
    <content type="html"><![CDATA[<h1 id="socket-模块函数"><a href="#socket-模块函数" class="headerlink" title="socket() 模块函数"></a>socket() 模块函数</h1><p>要创建套接字，必须使用 socket.socket() 函数，它一般的语法如下。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">socket(socket_family, socket_type, protocol=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p><p>其中， socket_family 是 AF_UNIX 或 AF_INET， socket_type 是 SOCK_STREAM 或 SOCK_DGRAM 。protocol 通常省略，默认为 0 。</p><p>所以，为了创建 TCP/IP 套接字，可以用下面的方式调用 socket.socket()。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpSock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br></pre></td></tr></table></figure></p><p>同样，为了创建 UDP/IP 套接字，需要执行以下语句。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">udpSock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)</span><br></pre></td></tr></table></figure></p><p>因为有很多 socket 模块属性，所以此时可以使用下面的语句来导入 socket 模块。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> socket <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure></p><p>然后可以创建套接字对象，再之后就可以使用套接字对象的方法进行进一步的交互。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpSock = socket(AF_INET, SOCK_STREAM)</span><br></pre></td></tr></table></figure></p><h1 id="UDP-客户端与服务器的创建"><a href="#UDP-客户端与服务器的创建" class="headerlink" title="UDP 客户端与服务器的创建"></a>UDP 客户端与服务器的创建</h1><h2 id="UDP-客户端的创建"><a href="#UDP-客户端的创建" class="headerlink" title="UDP 客户端的创建"></a>UDP 客户端的创建</h2><p>创建一个套接字对象，代码如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> socket <span class="keyword">import</span> *</span><br><span class="line">udpSock = socket(AF_INET, SOCK_DGRAM)</span><br></pre></td></tr></table></figure></p><p>然后使用 sendto() 方法可以发送信息，代码如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = input(<span class="string">"请输入你想要发送的信息："</span>)</span><br><span class="line"><span class="comment"># sendto(data, (ip, port))</span></span><br><span class="line">udpSock.sendto(data.encode(<span class="string">'gb2312'</span>), (<span class="string">'172.16.217.129'</span>, <span class="number">8080</span>))</span><br></pre></td></tr></table></figure></p><p>这里使用 mac 向 windows 发送一段信息，由于 windows 软件以 gb2312 解码，因此在 Python 代码中需要把要发送的信息编码成 gb2312，如图。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/python/udp.png" alt=""></p><h2 id="UDP-服务器的创建"><a href="#UDP-服务器的创建" class="headerlink" title="UDP 服务器的创建"></a>UDP 服务器的创建</h2><p>同上先创建一个套接字对象，和创建客户端不同，服务器需要绑定到某一个地址上，只有这样客户端才知道如何给服务器发送信息。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> socket <span class="keyword">import</span> *</span><br><span class="line">udpSock = socket(AF_INET, SOCK_DGRAM)</span><br><span class="line"><span class="comment"># 绑定到 7788 端口</span></span><br><span class="line">udpSock.bind((<span class="string">""</span>, <span class="number">7788</span>))</span><br><span class="line">udpSock.recvfrom(<span class="number">1024</span>)</span><br><span class="line">content, ip = udpdata</span><br><span class="line">print(ip, <span class="string">":"</span>, content.decode(<span class="string">"gb2312"</span>))</span><br></pre></td></tr></table></figure></p><p>这次使用 windows 向 mac 发送信息，如图。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/python/udp1.png" alt=""></p><hr>]]></content>
    
    <summary type="html">
    
      Python 网络编程使用的主要模块是 socket 模块，在这个模块中可以找到 socket() 函数，该函数用于创建套接字对象。套接字也有自己的方法集，这些方法可以实现基于套接字的网络通信。
    
    </summary>
    
      <category term="Python" scheme="https://tangx1.com/categories/Python/"/>
    
    
      <category term="Python" scheme="https://tangx1.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>bilibili之弹幕爬虫</title>
    <link href="https://tangx1.com/bilibili_danmu/"/>
    <id>https://tangx1.com/bilibili_danmu/</id>
    <published>2019-01-08T13:10:13.000Z</published>
    <updated>2019-01-20T10:00:50.859Z</updated>
    
    <content type="html"><![CDATA[<p>按照正常流程，打开网页，然后右键网页源代码，网页中并没有类似弹幕的文字出现，猜想应该是通过某些接口或者其它什么途径传过来的数据。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/bilibili/bilibli1.png" alt=""></p><p>打开 F12 开发者工具，查看 Network 下的请求，看看是不是弹幕隐藏在其中某个文件内，找了好久，竟然没有发现，后来发现其中一个前缀为 list.so?oid=xxxx的链接 在 Preview 模式下不显示数据，将其 url 用浏览器打开后却出现了我想要的弹幕数据。如下图所示。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/bilibili/danmu_list.png" alt=""></p><p><img src="https://hexo-pics.nos-eastchina1.126.net/bilibili/api.png" alt=""></p><p>既然找到了源文件，那爬虫就很好写了。</p><h1 id="下载弹幕文件"><a href="#下载弹幕文件" class="headerlink" title="下载弹幕文件"></a>下载弹幕文件</h1><p>创建一个爬虫类，然后定义 get_file 方法来请求并下载弹幕文件，然后使用 parse_danmus 方法来分析弹幕，弹幕信息都是 d 标签下，由此得到弹幕信息。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Danmu_Spider</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.danmu_url = <span class="string">'https://api.bilibili.com/x/v1/dm/list.so?oid=57763167'</span></span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Mobile Safari/537.36'</span>,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_file</span><span class="params">(self, url)</span>:</span></span><br><span class="line">        response = requests.get(url, headers=self.headers)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">with</span> open(<span class="string">'danmu.xml'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(response.content)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">'访问出错'</span>)</span><br></pre></td></tr></table></figure></p><h1 id="分析弹幕文件"><a href="#分析弹幕文件" class="headerlink" title="分析弹幕文件"></a>分析弹幕文件</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_danmus</span><span class="params">(self, file)</span>:</span></span><br><span class="line">    selector = etree.parse(file, etree.HTMLParser())</span><br><span class="line">    items = selector.xpath(<span class="string">"//d//text()"</span>)</span><br><span class="line">    items = set(items)</span><br><span class="line">    <span class="keyword">return</span> items</span><br></pre></td></tr></table></figure><p><img src="https://hexo-pics.nos-eastchina1.126.net/bilibili/danmu.png" alt=""></p><p>在图中可以看到成功爬取到了弹幕信息，数量为3000，和文章开头视频右边弹幕信息的数量一致，但是我这里将弹幕使用了集合去重，保留了有效弹幕。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/bilibili/danmu_quchong.png" alt=""></p><h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Danmu_Spider</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.danmu_url = <span class="string">'https://api.bilibili.com/x/v1/dm/list.so?oid=57763167'</span></span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Mobile Safari/537.36'</span>,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_file</span><span class="params">(self, url)</span>:</span></span><br><span class="line">        response = requests.get(url, headers=self.headers)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">with</span> open(<span class="string">'danmu.xml'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(response.content)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">'访问出错'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_danmus</span><span class="params">(self, file)</span>:</span></span><br><span class="line">        selector = etree.parse(file, etree.HTMLParser())</span><br><span class="line">        items = selector.xpath(<span class="string">"//d//text()"</span>)</span><br><span class="line">        items = set(items)</span><br><span class="line">        <span class="keyword">return</span> items</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.get_file(self.danmu_url)</span><br><span class="line">        danmus = self.parse_danmus(<span class="string">'danmu.xml'</span>)</span><br><span class="line">        print(<span class="string">'弹幕数量: %s'</span>% len(danmus))</span><br><span class="line">        print(danmus)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    spider = Danmu_Spider()</span><br><span class="line">    spider.run()</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    <summary type="html">
    
      有事没事的时候会去逛逛B站，我比较喜欢科技或者汽车频道，突然就想起来给B站视频的弹幕写一个爬虫，故写此篇。
    
    </summary>
    
      <category term="爬虫" scheme="https://tangx1.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="https://tangx1.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy模拟登录豆瓣网初级篇</title>
    <link href="https://tangx1.com/scrapy_login/"/>
    <id>https://tangx1.com/scrapy_login/</id>
    <published>2019-01-03T04:47:38.000Z</published>
    <updated>2019-01-05T08:04:00.958Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/scrapy-logo.png" alt=""></p><hr><p>在进行模拟登录之前，应该先对网站登录的原理有所了解，首先在 Chrome 浏览器中进行一次实际的登录操作，再来观察浏览器和网站服务器是如何交互的。</p><p>在这里我使用 <a href="https://www.douban.com/" target="_blank" rel="noopener">豆瓣网</a> 作为此次模拟登录的示例。</p><p>首先打开 F12 开发者模式，在登录表单中输入用户名和密码(这里我的验证码是因为我尝试次数过多出现)，点击登录按钮，观察控制台中 Network 下第一条请求，其为一条 post 请求，且参数在图中有所展示，那么如果需要模拟登录，就需要对这些参数进行构造。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/douban_form.png" alt=""></p><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/douban_form1.png" alt=""></p><p>登录的核心其实就是向服务器发送含有登录表单数据的 HTTP 请求(通常是 POST)，在 Scrapy 中提供了一个 FormRequest 类(Request的子类)，专门用于构造含有登录表单的请求，FormRequest 的构造器方法有一个 formdata 参数，接收字典形式的表单数据。</p><p>在本篇文章中，我先模拟登录到网站后，跳转至个人中心，然后修改我的个人签名。</p><h1 id="模拟登录"><a href="#模拟登录" class="headerlink" title="模拟登录"></a>模拟登录</h1><p>要构造 post 请求的参数，来看上图参数中 source, redir 和 login 都是固定值，form_email, form_password 分别为用户名和密码，captcha-solution 是图片验证码的字符，captcha-id 就先去网页源代码中寻找，如下图。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/captcha-id.png" alt=""></p><p>在这里我将 start_urls 设置为我的个人详情页，模拟登录这里需要重写 start_requests 方法，因为如果不去重写这个方法，那么 scrapy 就会对我的个人详情页直接进行请求。</p><p>FormRequest 的 from_response 方法需传入一个 Response 对象作为第一个参数，该方法会解析 Response 对象所包含的 form 元素，帮助用户创建 FormRequest 对象，并将隐藏 input 中的信息自动填入表单数据。使用这种方法，只需通过 formdata 参数填写账号和密码即可。这里使用 PIL 的 Image 方法将图片展示出来，人工识别并输入到程序中，程序继续进行登录。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模拟登录</span></span><br><span class="line">login_url = <span class="string">'https://accounts.douban.com/login'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">yield</span> Request(self.login_url, callback=self.login)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    print(<span class="string">'-----登录程序-----'</span>)</span><br><span class="line">    captcha_id = response.xpath(<span class="string">".//input[@name='captcha-id']/@value"</span>).get()</span><br><span class="line">    captcha_url = response.xpath(<span class="string">"//*[@id='captcha_image']/@src"</span>).get()</span><br><span class="line">    <span class="keyword">if</span> captcha_url <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        print(<span class="string">'-----登录时无验证码-----'</span>)</span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">'form_email'</span>: <span class="string">'xxxxxx@qq.com'</span>,</span><br><span class="line">            <span class="string">'form_password'</span>: <span class="string">'xxxxxx'</span></span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'-----登录时有验证码-----'</span>)</span><br><span class="line">        print(<span class="string">'-----即将下载验证码-----'</span>)</span><br><span class="line">        <span class="comment"># 使用urllib 的 urlretrieve 直接下载验证码图片到本地</span></span><br><span class="line">        request.urlretrieve(captcha_url, <span class="string">'captcha.png'</span>,)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            image = Image(<span class="string">'captcha.png'</span>)</span><br><span class="line">            image.show()</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        captcha_solution = input(<span class="string">"请输入图片中的验证码"</span>)</span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">'form_email'</span>: <span class="string">'xxxxxx@qq.com'</span>,</span><br><span class="line">            <span class="string">'form_password'</span>: <span class="string">'xxxxxx'</span>,</span><br><span class="line">            <span class="string">'captcha-solution'</span>: captcha_solution,</span><br><span class="line">            <span class="string">'captcha-id'</span>: captcha_id,</span><br><span class="line">            <span class="string">'login'</span>: <span class="string">'登录'</span></span><br><span class="line">        &#125;</span><br><span class="line">    print(<span class="string">'-----登录中-----'</span>)</span><br><span class="line">    <span class="keyword">yield</span> FormRequest.from_response(response, formdata=data, callback=self.parse_after_login)</span><br></pre></td></tr></table></figure><p>在 login 函数中，最后的 FormRequest 的回调函数是 parse_after_login 函数，代码如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_after_login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">"xxxxxxx的帐号"</span> <span class="keyword">in</span> response.text:</span><br><span class="line">        print(<span class="string">"-----登录成功-----"</span>)</span><br></pre></td></tr></table></figure><h1 id="修改签名"><a href="#修改签名" class="headerlink" title="修改签名"></a>修改签名</h1><p>在登录成功之后，需要先跳转到我的个人详情页面，再进行修改签名操作。</p><p>先手动修改签名一次，观察浏览器的请求过程，如图所示。点击修改后，浏览器中这条 POST 请求的 formdata 只有两个参数，一个 signature 就是我们正在修改的签名。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/sign1.png" alt=""></p><p>另一个是 ck 参数，ck 参数在网页源代码中同样可以找到，如下图。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/ck.png" alt=""></p><p>有了修改签名的两个参数，我们就可以构造修改签名的 FormRequest 了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_after_login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">"xxxxxxx的帐号"</span> <span class="keyword">in</span> response.text:</span><br><span class="line">        <span class="keyword">yield</span> <span class="keyword">from</span> super().start_requests()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> response.url == <span class="string">'https://www.douban.com/people/xxxxxxx/'</span>:</span><br><span class="line">        print(<span class="string">'-----已经进入个人详情页-----'</span>)</span><br><span class="line">        print(<span class="string">'-----正在修改个人签名-----'</span>)</span><br><span class="line">        ck = response.xpath(<span class="string">"//*[@id='edit_signature']/form/div/input/@value"</span>).get()</span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">'ck'</span>: ck,</span><br><span class="line">            <span class="string">'signature'</span>: <span class="string">'我是 scrapy 修改的~~'</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">yield</span> FormRequest(self.edit_signature_url, formdata=data)</span><br></pre></td></tr></table></figure><h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> Request, FormRequest</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LoginSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'login'</span></span><br><span class="line">    allowed_domains = [<span class="string">'douban.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'https://www.douban.com/people/xxxxxx/'</span>]</span><br><span class="line">    edit_signature_url = <span class="string">'https://www.douban.com/j/people/xxxxxx/edit_signature'</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'Connection'</span>: <span class="string">'Keep-Alive'</span>,</span><br><span class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)'</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self ,response)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> response.url == <span class="string">'https://www.douban.com/people/xxxxxx/'</span>:</span><br><span class="line">            print(<span class="string">'-----已经进入个人详情页-----'</span>)</span><br><span class="line">            print(<span class="string">'-----正在修改个人签名-----'</span>)</span><br><span class="line">            ck = response.xpath(<span class="string">"//*[@id='edit_signature']/form/div/input/@value"</span>).get()</span><br><span class="line">            data = &#123;</span><br><span class="line">                <span class="string">'ck'</span>: ck,</span><br><span class="line">                <span class="string">'signature'</span>: <span class="string">'我是 scrapy 修改的~~'</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">yield</span> FormRequest(self.edit_signature_url, formdata=data, callback=self.success)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">success</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        print(<span class="string">'-----个人签名修改成功-----'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模拟登录</span></span><br><span class="line">    login_url = <span class="string">'https://accounts.douban.com/login'</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> Request(self.login_url, callback=self.login)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        print(<span class="string">'-----登录程序-----'</span>)</span><br><span class="line">        captcha_id = response.xpath(<span class="string">".//input[@name='captcha-id']/@value"</span>).get()</span><br><span class="line">        captcha_url = response.xpath(<span class="string">"//*[@id='captcha_image']/@src"</span>).get()</span><br><span class="line">        <span class="keyword">if</span> captcha_url <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            print(<span class="string">'-----登录时无验证码-----'</span>)</span><br><span class="line">            data = &#123;</span><br><span class="line">                <span class="string">'form_email'</span>: <span class="string">'xxxxxx@qq.com'</span>,</span><br><span class="line">                <span class="string">'form_password'</span>: <span class="string">'xxxxxx'</span></span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">'-----登录时有验证码-----'</span>)</span><br><span class="line">            print(<span class="string">'-----即将下载验证码-----'</span>)</span><br><span class="line">            request.urlretrieve(captcha_url, <span class="string">'captcha.png'</span>,)</span><br><span class="line">            image = Image.open(<span class="string">'captcha.png'</span>)</span><br><span class="line">            image.show()</span><br><span class="line">            captcha_solution = input(<span class="string">"请输入验证码:"</span>)</span><br><span class="line">            <span class="comment"># captcha_solution = self.recognize_captcha('captcha.png')</span></span><br><span class="line">            data = &#123;</span><br><span class="line">                <span class="string">'form_email'</span>: <span class="string">'xxxxxx@qq.com'</span>,</span><br><span class="line">                <span class="string">'form_password'</span>: <span class="string">'xxxxxx'</span>,</span><br><span class="line">                <span class="string">'captcha-solution'</span>: captcha_solution,</span><br><span class="line">                <span class="string">'captcha-id'</span>: captcha_id,</span><br><span class="line">                <span class="string">'login'</span>: <span class="string">'登录'</span></span><br><span class="line">            &#125;</span><br><span class="line">        print(<span class="string">'-----登录中-----'</span>)</span><br><span class="line">        <span class="keyword">yield</span> FormRequest.from_response(response, formdata=data, callback=self.parse_after_login)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_after_login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">"xxxxxx的帐号"</span> <span class="keyword">in</span> response.text:</span><br><span class="line">            print(<span class="string">'-----登录成功-----'</span>)</span><br><span class="line">            <span class="keyword">yield</span> <span class="keyword">from</span> super().start_requests()</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    <summary type="html">
    
      Scrapy 是一个强大的爬虫框架，某些网站只有在用户登录后才能获取到有价值的信息，因此 Scrapy 框架也有模拟登录的能力。此篇来学习在 Scrapy 中模拟登录的方法。
    
    </summary>
    
    
      <category term="爬虫" scheme="https://tangx1.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Python" scheme="https://tangx1.com/tags/Python/"/>
    
      <category term="Scrapy" scheme="https://tangx1.com/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy模拟登录豆瓣网进阶篇</title>
    <link href="https://tangx1.com/scrapy-login-captcha/"/>
    <id>https://tangx1.com/scrapy-login-captcha/</id>
    <published>2019-01-03T04:47:38.000Z</published>
    <updated>2019-01-05T15:03:09.697Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/scrapy-logo.png" alt=""></p><hr><p>内容和上一篇大致差不多，重点说一下接入第三方验证码识别平台的问题。</p><p>验证码识别平台我使用的是 <a href="https://www.chaojiying.com/" target="_blank" rel="noopener">超级鹰</a> ，其是全球领先的智能图片分类及识别商家 ，具有安全、准确、高效、稳定、开放的特点，并且拥有强大的技术及校验团队。</p><p>超级鹰的开发文档: <a href="https://www.chaojiying.com/api.html" target="_blank" rel="noopener">API地址</a></p><p>为了方便，这里将 Python 版本的代码贴在下面，以后还用的上。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> md5</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Chaojiying_Client</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, username, password, soft_id)</span>:</span></span><br><span class="line">        self.username = username</span><br><span class="line">password =  password.encode(<span class="string">'utf8'</span>)</span><br><span class="line">        self.password = md5(password).hexdigest()</span><br><span class="line">        self.soft_id = soft_id</span><br><span class="line">        self.base_params = &#123;</span><br><span class="line">            <span class="string">'user'</span>: self.username,</span><br><span class="line">            <span class="string">'pass2'</span>: self.password,</span><br><span class="line">            <span class="string">'softid'</span>: self.soft_id,</span><br><span class="line">        &#125;</span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">'Connection'</span>: <span class="string">'Keep-Alive'</span>,</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)'</span>,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">PostPic</span><span class="params">(self, im, codetype)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        im: 图片字节</span></span><br><span class="line"><span class="string">        codetype: 题目类型 参考 http://www.chaojiying.com/price.html</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        params = &#123;</span><br><span class="line">            <span class="string">'codetype'</span>: codetype,</span><br><span class="line">        &#125;</span><br><span class="line">        params.update(self.base_params)</span><br><span class="line">        files = &#123;<span class="string">'userfile'</span>: (<span class="string">'ccc.jpg'</span>, im)&#125;</span><br><span class="line">        r = requests.post(<span class="string">'http://upload.chaojiying.net/Upload/Processing.php'</span>, data=params, files=files, headers=self.headers)</span><br><span class="line">        <span class="keyword">return</span> r.json()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ReportError</span><span class="params">(self, im_id)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        im_id:报错题目的图片ID</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        params = &#123;</span><br><span class="line">            <span class="string">'id'</span>: im_id,</span><br><span class="line">        &#125;</span><br><span class="line">        params.update(self.base_params)</span><br><span class="line">        r = requests.post(<span class="string">'http://upload.chaojiying.net/Upload/ReportError.php'</span>, data=params, headers=self.headers)</span><br><span class="line">        <span class="keyword">return</span> r.json()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">chaojiying = Chaojiying_Client(<span class="string">'超级鹰用户名'</span>, <span class="string">'超级鹰用户名的密码'</span>, <span class="string">'96001'</span>)<span class="comment">#用户中心&gt;&gt;软件ID 生成一个替换 96001</span></span><br><span class="line">im = open(<span class="string">'a.jpg'</span>, <span class="string">'rb'</span>).read()<span class="comment">#本地图片文件路径 来替换 a.jpg 有时WIN系统须要//</span></span><br><span class="line"><span class="keyword">print</span> chaojiying.PostPic(im, <span class="number">1902</span>) <span class="comment">#1902 验证码类型  官方网站&gt;&gt;价格体系 3.4+版 print 后要加()</span></span><br></pre></td></tr></table></figure></p><p>只要把开发文档中对应的 soft_id 和 codetype 填上去，将验证码图片保存到本地，就可以使用超级鹰识别验证码了。话不多说，代码如下。</p><h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LoginSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'login'</span></span><br><span class="line">    allowed_domains = [<span class="string">'douban.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'https://www.douban.com/people/xxxxxx/'</span>]</span><br><span class="line">    edit_signature_url = <span class="string">'https://www.douban.com/j/people/xxxxxx/edit_signature'</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'Connection'</span>: <span class="string">'Keep-Alive'</span>,</span><br><span class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)'</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self ,response)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> response.url == <span class="string">'https://www.douban.com/people/xxxxxx/'</span>:</span><br><span class="line">            print(<span class="string">'-----已经进入个人详情页-----'</span>)</span><br><span class="line">            print(<span class="string">'-----正在修改个人签名-----'</span>)</span><br><span class="line">            ck = response.xpath(<span class="string">"//*[@id='edit_signature']/form/div/input/@value"</span>).get()</span><br><span class="line">            data = &#123;</span><br><span class="line">                <span class="string">'ck'</span>: ck,</span><br><span class="line">                <span class="string">'signature'</span>: <span class="string">'我可以自动识别验证码啦~~'</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">yield</span> FormRequest(self.edit_signature_url, formdata=data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模拟登录</span></span><br><span class="line">    login_url = <span class="string">'https://accounts.douban.com/login'</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> Request(self.login_url, callback=self.login)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        print(<span class="string">'-----登录程序-----'</span>)</span><br><span class="line">        captcha_id = response.xpath(<span class="string">".//input[@name='captcha-id']/@value"</span>).get()</span><br><span class="line">        captcha_url = response.xpath(<span class="string">"//*[@id='captcha_image']/@src"</span>).get()</span><br><span class="line">        <span class="keyword">if</span> captcha_url <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            print(<span class="string">'-----登录时无验证码-----'</span>)</span><br><span class="line">            data = &#123;</span><br><span class="line">                <span class="string">'form_email'</span>: <span class="string">'xxxxxx@qq.com'</span>,</span><br><span class="line">                <span class="string">'form_password'</span>: <span class="string">'xxxxxx'</span></span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">'-----登录时有验证码-----'</span>)</span><br><span class="line">            print(<span class="string">'-----即将下载验证码-----'</span>)</span><br><span class="line">            request.urlretrieve(captcha_url, <span class="string">'captcha.png'</span>,)</span><br><span class="line"></span><br><span class="line">            captcha_solution = self.recognize_captcha(<span class="string">'captcha.png'</span>)</span><br><span class="line">            data = &#123;</span><br><span class="line">                <span class="string">'form_email'</span>: <span class="string">'xxxxxx@qq.com'</span>,</span><br><span class="line">                <span class="string">'form_password'</span>: <span class="string">'xxxxxx'</span>,</span><br><span class="line">                <span class="string">'captcha-solution'</span>: captcha_solution,</span><br><span class="line">                <span class="string">'captcha-id'</span>: captcha_id,</span><br><span class="line">                <span class="string">'login'</span>: <span class="string">'登录'</span></span><br><span class="line">            &#125;</span><br><span class="line">        print(<span class="string">'-----登录中-----'</span>)</span><br><span class="line">        <span class="keyword">yield</span> FormRequest.from_response(response, formdata=data, callback=self.parse_after_login)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_after_login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">"xxxxxx的帐号"</span> <span class="keyword">in</span> response.text:</span><br><span class="line">            <span class="keyword">yield</span> <span class="keyword">from</span> super().start_requests()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recognize_captcha</span><span class="params">(self, im)</span>:</span></span><br><span class="line">        print(<span class="string">'-----正在进行验证码识别-----'</span>)</span><br><span class="line">        username = <span class="string">'xxxxxx'</span></span><br><span class="line">        password = <span class="string">'xxxxxx'</span>.encode(<span class="string">'utf-8'</span>)</span><br><span class="line">        password = md5(password).hexdigest()</span><br><span class="line">        soft_id = <span class="string">'898320'</span></span><br><span class="line">        codetype = <span class="string">'1007'</span></span><br><span class="line">        base_params = &#123;</span><br><span class="line">            <span class="string">'user'</span>: username,</span><br><span class="line">            <span class="string">'pass2'</span>: password,</span><br><span class="line">            <span class="string">'softid'</span>: soft_id,</span><br><span class="line">            <span class="string">'codetype'</span>: codetype</span><br><span class="line">        &#125;</span><br><span class="line">        im = open(im, <span class="string">'rb'</span>).read()</span><br><span class="line">        files = &#123;<span class="string">'userfile'</span>: (<span class="string">'ccc.jpg'</span>, im)&#125;</span><br><span class="line">        r = requests.post(<span class="string">'http://upload.chaojiying.net/Upload/Processing.php'</span>, data=params, files=files, headers=self.headers)</span><br><span class="line">        captcha = r.json()[<span class="string">'pic_str'</span>]</span><br><span class="line">        print(<span class="string">'-----验证码识别完毕-----'</span>)</span><br><span class="line">        print(<span class="string">'验证码为：%s'</span> % captcha)</span><br><span class="line">        <span class="keyword">return</span> captcha</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      上一篇文章中使用 Scrapy 模拟登录豆瓣网，并且进行人工识别验证码进行登录，这一篇中我们使用第三方验证码识别平台去自动识别 Scrapy 登录过程中的验证码。
    
    </summary>
    
    
      <category term="爬虫" scheme="https://tangx1.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Python" scheme="https://tangx1.com/tags/Python/"/>
    
      <category term="Scrapy" scheme="https://tangx1.com/tags/Scrapy/"/>
    
      <category term="验证码" scheme="https://tangx1.com/tags/%E9%AA%8C%E8%AF%81%E7%A0%81/"/>
    
  </entry>
  
  <entry>
    <title>七牛云数据迁移至网易nos</title>
    <link href="https://tangx1.com/qiniu/"/>
    <id>https://tangx1.com/qiniu/</id>
    <published>2018-10-28T05:52:03.000Z</published>
    <updated>2019-01-16T19:54:06.642Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/qiniu/qiniulogo.png" alt=""></p><hr><h1 id="使用-qshell-将-bucket-文件迁移到新-bucket-中"><a href="#使用-qshell-将-bucket-文件迁移到新-bucket-中" class="headerlink" title="使用 qshell 将 bucket 文件迁移到新 bucket 中"></a>使用 qshell 将 bucket 文件迁移到新 bucket 中</h1><h2 id="安装-qshell-工具。下载地址：qshell-官方文档"><a href="#安装-qshell-工具。下载地址：qshell-官方文档" class="headerlink" title="安装 qshell 工具。下载地址：qshell 官方文档"></a>安装 qshell 工具。下载地址：<a href="http://devtools.qiniu.com/qshell-v2.3.3.zip" target="_blank" rel="noopener">qshell</a> <a href="https://developer.qiniu.com/kodo/tools/1302/qshell" target="_blank" rel="noopener">官方文档</a></h2><p>将下载下来的压缩文件解压到任意目录，将其中的名字为 darwin 的文件重命名为 qshell 并放入 mac 的 /usr/local/bin <a href="https://www.jianshu.com/p/9a6dca7ccfa0" target="_blank" rel="noopener">目录</a>，最后在命令行输入 qshell 。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/qiniu/qshell1.png" alt=""></p><h2 id="密钥设置"><a href="#密钥设置" class="headerlink" title="密钥设置"></a>密钥设置</h2><p>这里需要使用七牛账号中<a href="https://portal.qiniu.com/user/key" target="_blank" rel="noopener">个人密钥管理</a>下的 AccessKey 和 SecretKey 。</p><p>找到这两个值后在命令行中输入如下命令。(其中 ak 和 sk 分别对应 AccessKey 和 SecretKey ， name 为账户名称)<br><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ qshell account ak sk <span class="built_in">name</span></span><br></pre></td></tr></table></figure></p><p>如果没有报错的话，输入以下命令来显示账号信息。<br><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$ </span>qshell account</span><br></pre></td></tr></table></figure></p><h2 id="迁移文件"><a href="#迁移文件" class="headerlink" title="迁移文件"></a>迁移文件</h2><p>使用如下命令进行文件迁移并生成包含文件名的文本文件。</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">$</span> qshell listbucket &#123; bucket_name &#125; | awk -F<span class="string">"\t"</span> <span class="string">'&#123;print $1&#125;'</span> &gt; <span class="keyword">files</span>.txt</span><br><span class="line"><span class="symbol">$</span> qshell batchcopy &#123; bucket_name &#125; &#123; new_bucket_name &#125; -i <span class="keyword">files</span>.txt</span><br></pre></td></tr></table></figure><h1 id="批量下载文件到本地"><a href="#批量下载文件到本地" class="headerlink" title="批量下载文件到本地"></a>批量下载文件到本地</h1><p>上面只是将旧桶内的文件转移到新的桶内，原本无法预览、下载的文件现在都可以正常操作了，可是里面有很多图片总不能从网页上一张一张下载下来然后再上传到其他的储存空间里吧，因此这里就需要思考如何将图片批量下载到本地磁盘。七牛云的 qshell 工具有 <a href="https://github.com/qiniu/qshell/blob/master/docs/qdownload.md" target="_blank" rel="noopener"><code>qdownload</code></a> 方法能批量下载文件，但是尝试多次均以失败告终，后看到 qshell 的 <a href="https://github.com/qiniu/qshell/blob/master/docs/get.md" target="_blank" rel="noopener"><code>get</code></a> 方法只能每次操作一个文件，如果让电脑代替人工去重复操作这一 get 方法，就可以把每一张图片都下载下来。</p><p>python 脚本代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> linecache</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> range(<span class="number">1</span>, total_num):</span><br><span class="line">    result = linecache.getline(<span class="string">'./files.txt'</span>, num).strip()</span><br><span class="line">    print(result)</span><br><span class="line">    os.system(<span class="string">'/usr/local/bin/qshell get test '</span> + result)</span><br></pre></td></tr></table></figure></p><h1 id="上传至网易-nos"><a href="#上传至网易-nos" class="headerlink" title="上传至网易 nos"></a>上传至网易 nos</h1><p><a href="https://c.163yun.com/dashboard#/m/nos/" target="_blank" rel="noopener">网易nos</a> 的注册与创建桶的过程不再过多介绍，需要注意的是在创建储存桶完毕之后要进行两个关键的配置。</p><ul><li><p>存储桶（bucket）访问权限</p></li><li><p>防盗链设置</p></li></ul><p>1、访问权限</p><p>访问权限应设置为公有读。如图中解释：</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/qiniu/rwpro.png" alt=""></p><p>2、防盗链设置</p><p>为了保护自己的免费额度，防止图片被他人盗用，因此需要开启防盗链。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/qiniu/fdl.png" alt=""></p><hr>]]></content>
    
    <summary type="html">
    
      七牛云页面提示：5402 获取bucket域名失败，发现七牛云的测试域名被回收了，想继续用需要绑定一个已备案的域名，太麻烦了。主要是之前的图片现在无法预览，也无法下载。
    
    </summary>
    
      <category term="日常" scheme="https://tangx1.com/categories/%E6%97%A5%E5%B8%B8/"/>
    
    
  </entry>
  
  <entry>
    <title>PHP框架--Laravel</title>
    <link href="https://tangx1.com/laravel_admin/"/>
    <id>https://tangx1.com/laravel_admin/</id>
    <published>2018-10-09T16:10:33.000Z</published>
    <updated>2019-01-20T10:01:40.813Z</updated>
    
    <content type="html"><![CDATA[<p>Laravel-Admin 是一个帮我们快速建立后台管理的工具。它提供了页面组件和表单元素等功能，而且还有很多附加功能，同时也支持我们去自定义一些插件，非常方便。</p><p>由于之前没有接触过 PHP这门语言, 所以一开始接手项目时先看了看框架的代码，果然看不懂。回头看了看 PHP 的基础语法，再去简单做了一下 Laravel 的 demo ，开始慢慢理解了某些语法。</p><p>下面是我遇到的几个小问题，记录下来以作参考。</p><hr><h1 id="switch状态值"><a href="#switch状态值" class="headerlink" title="switch状态值"></a>switch状态值</h1><p>因为 switch 组件默认存入数据库的状态是[ 开-&gt;1, 关-&gt;0 ]，但是由于 0 会影响数据判断的正确性和安全性，因此需要把 0 和 1 的状态改为 1 和 2，由于需要操作的按钮在表格页，也就是 Grid 页，而 Grid 页的操作都是由 Form 页传递过去的，因此只要找到 Form 中对应的 switch 操作数据库的地方就可以修改入库状态值了。经过一层一层的溯源，先找到 Form.php 文件，查看可使用的操作，发现了 SwitchField 关键词，对其进行查找，找到了 SwitchField.php 文件，其中有一段代码是用来写开关入库的状态值的，对此进行修改后通过测试。</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> $states = [</span><br><span class="line">        <span class="string">'on'</span>  =&gt; [<span class="string">'value'</span> =&gt; <span class="number">1</span>, <span class="string">'text'</span> =&gt; <span class="string">'ON'</span>, <span class="string">'color'</span> =&gt; <span class="string">'primary'</span>],</span><br><span class="line">        <span class="string">'off'</span> =&gt; [<span class="string">'value'</span> =&gt; <span class="number">2</span>, <span class="string">'text'</span> =&gt; <span class="string">'OFF'</span>, <span class="string">'color'</span> =&gt; <span class="string">'default'</span>],</span><br><span class="line">    ];</span><br></pre></td></tr></table></figure><h1 id="switch控制多个页面"><a href="#switch控制多个页面" class="headerlink" title="switch控制多个页面"></a>switch控制多个页面</h1><p>有几个相同的表结构，被显示在不同的页面，页面结构完全相同，因此为了节省操作量，需要用一个页面的 swtich 开关去控制多个页面的状态。Google 后发现了<a href="https://github.com/z-song/laravel-admin/issues/375" target="_blank" rel="noopener">解决方案</a>。</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="function"><span class="keyword">function</span> <span class="title">boot</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">parent</span>::boot();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span>::saving(<span class="function"><span class="keyword">function</span> <span class="params">($model)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 从$model取出数据并进行处理</span></span><br><span class="line"></span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    <summary type="html">
    
      在公司工作的时候要用到后台管理工具 Laravel-Admin ，这里简单记录了自己遇到的两个小问题。
    
    </summary>
    
      <category term="PHP" scheme="https://tangx1.com/categories/PHP/"/>
    
    
      <category term="PHP" scheme="https://tangx1.com/tags/PHP/"/>
    
  </entry>
  
  <entry>
    <title>Python算法--快速排序</title>
    <link href="https://tangx1.com/quick_sort/"/>
    <id>https://tangx1.com/quick_sort/</id>
    <published>2018-05-22T16:29:26.000Z</published>
    <updated>2019-01-03T07:24:21.079Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/quick_sortlogo.jpg" alt=""></p><hr><p>快速排序（Quicksort），又称划分交换排序（partition-exchange sort），通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。</p><p>步骤为：</p><p>1、从数列中挑出一个元素，称为”基准”（pivot）</p><p>2、重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作</p><p>3、递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序</p><p>递归的最底部情形，是数列的大小是零或一，也就是永远都已经被排序好了。虽然一直递归下去，但是这个算法总会结束，因为在每次的迭代（iteration）中，它至少会把一个元素摆到它最后的位置去。</p><hr><h1 id="过程分析"><a href="#过程分析" class="headerlink" title="过程分析"></a>过程分析</h1><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/quicksort.jpg" alt=""></p><h1 id="快速排序演示"><a href="#快速排序演示" class="headerlink" title="快速排序演示"></a>快速排序演示</h1><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/quicksort.gif" alt=""></p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort</span><span class="params">(alist, start, end)</span>:</span></span><br><span class="line">    <span class="string">'''快速排序'''</span></span><br><span class="line">    <span class="keyword">if</span> start &gt;= end:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    mid = alist[start]</span><br><span class="line">    low = start</span><br><span class="line">    high = end</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> low &lt; high:</span><br><span class="line">        <span class="keyword">while</span> low &lt; high <span class="keyword">and</span> alist[high] &gt;= mid:</span><br><span class="line">            high -= <span class="number">1</span></span><br><span class="line">        alist[low] = alist[high]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> low &lt; high <span class="keyword">and</span> alist[low] &lt; mid:</span><br><span class="line">            low += <span class="number">1</span></span><br><span class="line">        alist[high] = alist[low]</span><br><span class="line"></span><br><span class="line">    alist[low] = mid</span><br><span class="line"></span><br><span class="line">    quick_sort(alist, start, low<span class="number">-1</span>)</span><br><span class="line">    quick_sort(alist, low+<span class="number">1</span>, end)</span><br><span class="line">    <span class="keyword">return</span> alist</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    alist = [<span class="number">26</span>, <span class="number">54</span>, <span class="number">15</span>, <span class="number">57</span>, <span class="number">6</span>]</span><br><span class="line">    print(alist)</span><br><span class="line">    print(quick_sort(alist, <span class="number">0</span>, len(alist)<span class="number">-1</span>))</span><br></pre></td></tr></table></figure><h1 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h1><blockquote><p>最优时间复杂度：O(nlogn)</p></blockquote><blockquote><p>最坏时间复杂度：O(n2)</p></blockquote><blockquote><p>稳定性：不稳定</p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      快速排序（Quicksort），又称划分交换排序（partition-exchange sort），通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。
    
    </summary>
    
    
      <category term="算法" scheme="https://tangx1.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Python算法--插入排序</title>
    <link href="https://tangx1.com/insert_sort/"/>
    <id>https://tangx1.com/insert_sort/</id>
    <published>2018-05-22T00:07:20.000Z</published>
    <updated>2019-01-20T10:01:26.557Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/insert_sortlogo.jpg" alt=""></p><hr><p>插入排序（英语：Insertion Sort）是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。插入排序在实现上，在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。</p><hr><h1 id="过程分析"><a href="#过程分析" class="headerlink" title="过程分析"></a>过程分析</h1><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/insert.png" alt=""></p><blockquote><p> <img src="https://hexo-pics.nos-eastchina1.126.net/insert-sort/Insertion-sort-example.gif" alt=""></p></blockquote><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_sort</span><span class="params">(alist)</span>:</span></span><br><span class="line">    n = len(alist)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> range(i, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">if</span> alist[j] &lt; alist[j<span class="number">-1</span>]:</span><br><span class="line">          alist[j], alist[j<span class="number">-1</span>] = alist[j<span class="number">-1</span>], alist[j]</span><br><span class="line">    <span class="keyword">return</span> alist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    alist = [<span class="number">26</span>, <span class="number">54</span>, <span class="number">15</span>, <span class="number">57</span>, <span class="number">6</span>]</span><br><span class="line">    print(alist)</span><br><span class="line">    print(insert_sort(alist))</span><br></pre></td></tr></table></figure><h1 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h1><blockquote><p>最优时间复杂度：O(n) （升序排列，序列已经处于升序状态）</p></blockquote><blockquote><p>最坏时间复杂度：O(n2)</p></blockquote><blockquote><p>稳定性：稳定</p></blockquote><h1 id="插入排序演示"><a href="#插入排序演示" class="headerlink" title="插入排序演示"></a>插入排序演示</h1><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/insert.gif" alt=""></p><hr>]]></content>
    
    <summary type="html">
    
      插入排序（英语：Insertion Sort）是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。插入排序在实现上，在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Python算法--选择排序</title>
    <link href="https://tangx1.com/select_sort/"/>
    <id>https://tangx1.com/select_sort/</id>
    <published>2018-05-21T21:58:47.000Z</published>
    <updated>2019-01-15T02:45:15.133Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/select_sortlogo.jpg" alt=""></p><hr><p>选择排序（ Selection sort ）是一种简单直观的排序算法。它的工作原理如下。首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。</p><p>选择排序的主要优点与数据移动有关。如果某个元素位于正确的最终位置上，则它不会被移动。选择排序每次交换一对元素，它们当中至少有一个将被移到其最终位置上，因此对n个元素的表进行排序总共进行至多n-1次交换。在所有的完全依靠交换去移动元素的排序方法中，选择排序属于非常好的一种。</p><hr><h1 id="过程分析"><a href="#过程分析" class="headerlink" title="过程分析"></a>过程分析</h1><blockquote><blockquote><blockquote><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/selectionsort.jpg" alt=""></p></blockquote></blockquote></blockquote><h1 id="选择排序演示"><a href="#选择排序演示" class="headerlink" title="选择排序演示"></a>选择排序演示</h1><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/selection.gif" alt=""></p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_sort</span><span class="params">(alist)</span>:</span></span><br><span class="line">    n = len(alist)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-1</span>):</span><br><span class="line">        min_index = i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, n):</span><br><span class="line">          <span class="keyword">if</span> alist[j] &lt; alist[min_index]:</span><br><span class="line">              min_index = j</span><br><span class="line">        alist[i], alist[min_index] = alist[min_index], alist[i]</span><br><span class="line">    <span class="keyword">return</span> alist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    alist = [<span class="number">26</span>, <span class="number">54</span>, <span class="number">15</span>, <span class="number">57</span>, <span class="number">6</span>]</span><br><span class="line">    print(alist)</span><br><span class="line">    print(selection_sort(alist))</span><br></pre></td></tr></table></figure><h1 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h1><blockquote><p>最优时间复杂度：O(n2)</p></blockquote><blockquote><p>最坏时间复杂度：O(n2)</p></blockquote><blockquote><p>稳定性：不稳定</p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      选择排序（ Selection sort ）是一种简单直观的排序算法。它的工作原理如下。首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。
    
    </summary>
    
      <category term="算法" scheme="https://tangx1.com/categories/%E7%AE%97%E6%B3%95/"/>
    
      <category term="Python" scheme="https://tangx1.com/categories/%E7%AE%97%E6%B3%95/Python/"/>
    
    
      <category term="算法" scheme="https://tangx1.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="Python" scheme="https://tangx1.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>冒泡排序</title>
    <link href="https://tangx1.com/bubble_sort/"/>
    <id>https://tangx1.com/bubble_sort/</id>
    <published>2018-05-21T16:25:42.000Z</published>
    <updated>2019-01-20T10:01:10.824Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/bubble_sortlogo.jpg" alt=""></p><hr><p>冒泡排序（Bubble Sort）是一种简单的排序算法。它重复地遍历要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。遍历数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。</p><p>冒泡排序算法的运作如下：</p><blockquote><p>比较相邻的元素。如果第一个比第二个大（升序），就交换他们两个。</p></blockquote><blockquote><p>对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。</p></blockquote><blockquote><p>针对所有的元素重复以上的步骤，除了最后一个。</p></blockquote><blockquote><p>持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。</p></blockquote><hr><h1 id="过程分析"><a href="#过程分析" class="headerlink" title="过程分析"></a>过程分析</h1><p>交换过程图示(第一次)：</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/bubblesort.jpg" alt=""></p><p>那么我们需要进行 n-1 次冒泡过程，每次对应的比较次数如下图所示：</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/compare.bmp" alt=""></p><h1 id="冒泡排序演示"><a href="#冒泡排序演示" class="headerlink" title="冒泡排序演示"></a>冒泡排序演示</h1><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/bubble.gif" alt=""></p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bubble_sort</span><span class="params">(alist)</span>:</span></span><br><span class="line">    n = len(alist)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n<span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-1</span>-j):</span><br><span class="line">            <span class="keyword">if</span> alist[i] &gt; alist[i+<span class="number">1</span>]:</span><br><span class="line">                alist[i], alist[i+<span class="number">1</span>] = alist[i+<span class="number">1</span>], alist[i]</span><br><span class="line">    <span class="keyword">return</span> alist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    alist = [<span class="number">26</span>, <span class="number">54</span>, <span class="number">15</span>, <span class="number">57</span>, <span class="number">6</span>]</span><br><span class="line">    print(alist)</span><br><span class="line">    print(bubble_sort(alist))</span><br></pre></td></tr></table></figure><h1 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h1><blockquote><p>最优时间复杂度：O(n) （表示遍历一次发现没有任何可以交换的元素，排序结束。）</p></blockquote><blockquote><p>最坏时间复杂度：O(n2)</p></blockquote><blockquote><p>稳定性：稳定</p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      冒泡排序（Bubble Sort）是一种简单的排序算法。它重复地遍历要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。遍历数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。
    
    </summary>
    
    
      <category term="算法" scheme="https://tangx1.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫--Scrapy爬取简书全站文章</title>
    <link href="https://tangx1.com/jianshu/"/>
    <id>https://tangx1.com/jianshu/</id>
    <published>2018-05-18T07:25:17.000Z</published>
    <updated>2019-01-20T10:01:34.599Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/jianshu/jianshu-logo.png" alt=""></p><hr><p>最近学习了 scrapy ，之前刚开始爬虫的时候有接触过这个框架，当时看了下工作原理有点难懂，现在慢慢地接触爬虫多了，回过头来开始了解爬虫框架，现在再来看它的工作流程就明白了很多。</p><p>本篇文章使用 scrapy 来爬取<a href="https://www.jianshu.com/" target="_blank" rel="noopener">简书</a>全站文章。</p><p>scrapy 工程创建与配置步骤(个人习惯)：</p><ol><li>创建 scrapy 工程，创建启动文件 start.py ，修改 settings.py 配置文件</li><li>进入 spider.py 文件开始写爬虫规则</li><li>item.py 中设置存储模板</li><li>写 pipeline 存入数据库</li></ol><hr><h1 id="创建-scrapy-工程"><a href="#创建-scrapy-工程" class="headerlink" title="创建 scrapy 工程"></a>创建 scrapy 工程</h1><p>windows 系统在 scrapy 工程文件根目录：打开命令行工具，输入命令创建工程。<br><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy startproject <span class="string">[工程名字]</span></span><br></pre></td></tr></table></figure></p><p>cd 到工程文件夹下，创建爬虫文件，默认使用 basic 模板，同样在命令行中。输入<br><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy genspider <span class="string">[爬虫名字]</span> <span class="string">[爬虫网址]</span></span><br></pre></td></tr></table></figure></p><p>这样就完成了一个 scrapy 工程的创建。但是这里爬取简书全站我使用的是  crawlspider 爬虫，其有可编写的爬虫规则，使用起来比较方便。</p><p>为了方便启动工程，我都会在创建好 scrapy 后再来创建一个启动文件 start.py 。</p><p>修改 settings.py 文件，将其中的遵守 robots.txt 协议关闭，开启 headers 其它配置等需要的时候再去更改。</p><hr><h1 id="进入-spider-py-写爬虫规则"><a href="#进入-spider-py-写爬虫规则" class="headerlink" title="进入 spider.py 写爬虫规则"></a>进入 spider.py 写爬虫规则</h1><p>这次爬取的是简书全站的文章，因此要找所有文章的链接规则，每篇文章阅读到最底部，简书会推荐给我们一些其它文章，几乎每篇文章下面都会有推荐，因此我们从这里入手，查看了源代码，发现了它们的链接形式都大致相同，<a href="https://www.jianshu.com/p/7a4879ef6f8d" target="_blank" rel="noopener">https://www.jianshu.com/p/7a4879ef6f8d</a> ，<a href="https://www.jianshu.com/p/cde1742518c8" target="_blank" rel="noopener">https://www.jianshu.com/p/cde1742518c8</a> ，如图。    </p><p><img src="https://hexo-pics.nos-eastchina1.126.net/jianshu/jianshu3.jpg" alt=""></p><p>可以看到，都是 p 后面接上一大串数字字母的混合字符串，因此可以写出它的规则，使用正则表达式，如下。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r'.+/p/[a-z0-9].+'</span>), callback=<span class="string">'parse_detail'</span>, follow=<span class="keyword">True</span>),</span><br><span class="line">    )</span><br></pre></td></tr></table></figure></p><p>rules 是一个元组，其中，Rule 写的是爬虫的规则；callback 指的是回调函数，也就是当获取到了前面取到的 url 之后，程序该去调用哪一个函数的操作，而这里就是去调用 parse_detail 这个函数； follow 表示跟进，如果其 ==True 表示要继续跟进，也就是我们进入一片文章之后，要继续跟进下一篇文章。</p><p>在进入一篇文章之后，我们要获取到它的标题，发布者，发布时间，还有内容这四个部分。这里使用 xpath 方法来获取。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/jianshu/jianshu1.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    title = response.xpath(<span class="string">"/html/body/div[1]/div[1]/div[1]/h1/text()"</span>).get()</span><br><span class="line">    pub_name = response.xpath(<span class="string">"/html/body/div[1]/div[1]/div[1]/div[1]/div/span/a/text()"</span>).get()</span><br><span class="line">    release_time = response.xpath(<span class="string">"/html/body/div[1]/div[1]/div[1]/div[1]/div/div/span[1]/text()"</span>).get()</span><br><span class="line">    content = response.xpath(<span class="string">"/html/body/div[1]/div[1]/div[1]/div[2]/div"</span>).get()</span><br></pre></td></tr></table></figure><hr><h1 id="items-py-中设置存储模板"><a href="#items-py-中设置存储模板" class="headerlink" title="items.py 中设置存储模板"></a>items.py 中设置存储模板</h1><p>在上面已经决定了要采集者四个信息，那么在 item.py 中设置好这四项。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JianshuItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    pub_name = scrapy.Field()</span><br><span class="line">    release_time = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br></pre></td></tr></table></figure></p><p>最后在 parse_detail 尾部加入以下代码，再将 item 返回去。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">item = JianshuItem(title=title, pub_name=pub_name, release_time=release_time, content=content)</span><br><span class="line"><span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure></p><hr><h1 id="pipelines-py"><a href="#pipelines-py" class="headerlink" title="pipelines.py"></a>pipelines.py</h1><p>到这里就可以运行一下程序了，看一下是否能正常输出我们采集的信息。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/jianshu/jianshu2.jpg" alt=""></p><p>可以看到这里可以正常爬取数据，接下来需要将其存入数据库。存入数据库需要在 pipelines.py 中编写相应的代码。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JianshuMongoDBPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.DB_URI = <span class="string">'localhost'</span></span><br><span class="line">        self.DB_PORT = <span class="number">27017</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.DB_URI, self.DB_PORT)</span><br><span class="line">        self.db = self.client[<span class="string">'jianshu'</span>]</span><br><span class="line">        self.collection = self.db[<span class="string">'jianshu_spider'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spdier)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">if</span> self.collection.insert(dict(item)):</span><br><span class="line">                print(<span class="string">'保存至MongoDB成功'</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">'保存至MongoDB失败！'</span>)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> error:</span><br><span class="line">            print(error)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure></p><p>在 pipelines.py 中写好 MongoDB 部分后，在 settings.py 中将对应的 pipelines 打开。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/jianshu/jianshusettings.jpg" alt=""></p><p>然后重新运行 start.py ，启动爬虫。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/jianshu/jianshumongo.jpg" alt=""></p><p>启动爬虫后，一直没有遇到反爬措施，运行了大概30分钟， ROBO 3T 管理工具得到的数据有3300条。<del>感觉速度还是慢，有待优化</del>(突然发现在 settings.py 中设置了1s延时….关掉之后快多了。。 )</p><hr>]]></content>
    
    <summary type="html">
    
      最近学习了 scrapy ，之前刚开始爬虫的时候有接触过这个框架，当时看了下工作原理有点难懂，现在慢慢地接触爬虫多了，回过头来开始了解爬虫框架，现在再来看它的工作流程就明白了很多。
    
    </summary>
    
    
      <category term="Scrapy" scheme="https://tangx1.com/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy</title>
    <link href="https://tangx1.com/scrapy/"/>
    <id>https://tangx1.com/scrapy/</id>
    <published>2018-05-10T07:54:09.000Z</published>
    <updated>2019-01-03T07:48:06.672Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/scrapy-logo.png" alt=""></p><hr><p>Scrapy 是一个为了爬取网站数据，提取结构性数据而编写的应用框架。可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。</p><p>其最初是为了<a href="http://en.wikipedia.org/wiki/Web_scraping" target="_blank" rel="noopener">网络抓取</a>所设计的，也可以应用在获取 API 所返回的数据或者通用的网络爬虫。</p><p>Scrapy 是使用 Python 语言(基于 Twisted 框架)编写的开源网络爬虫框架。其简单易用、灵活易扩展、开发社区活跃，并且是跨平台的(支持Linux, MacOS, Windows)。</p><p>安装方式：pip 安装<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip <span class="keyword">install</span> scrapy</span><br></pre></td></tr></table></figure></p><p>为了确认 Scrapy 被成功安装，可尝试在 python 命令行中将 scrapy 导入，如未报错，则安装成功。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; import scrapy</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; scrapy.version_info</span><br></pre></td></tr></table></figure><hr><h1 id="scrapy-框架结构"><a href="#scrapy-框架结构" class="headerlink" title="scrapy 框架结构"></a>scrapy 框架结构</h1><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/scrapyfluent.jpg" alt=""></p><p><code>ENGINE</code> 引擎，框架的核心，用于协调其它组件间工作</p><p><code>SCHEDULER</code> 调度器，负责对 SPIDERS 提交的请求进行调度</p><p><code>DOWNLOADER</code> 下载器，负责下载页面</p><p><code>SPIDERS</code> 爬虫，负责提取页面中的数据，并产生对新页面的下载请求</p><p><code>MIDDLEWARES</code> 中间件，负责对 Request 对象和 Response 对象进行处理</p><p><code>ITEM PIPELINE</code> 数据管道，负责对爬取到的数据进行处理</p><h1 id="scrapy-工作流程"><a href="#scrapy-工作流程" class="headerlink" title="scrapy 工作流程"></a>scrapy 工作流程</h1><p>1、引擎 ENGINE 从调度器中取出一个链接 URL 用于接下来的抓取</p><p>2、引擎 ENGINE 把 URL 封装成一个请求 Request 传给下载器 Downloader</p><p>3、下载器 Downloader 把资源下载下来，并封装成 Response</p><p>4、爬虫解析 Response</p><p>5、若解析出实体 Item，交给数据管道 Item Pipeline 做进一步处理</p><p>6、若解析出链接 URL，就把 URL 交给调度器等待抓取</p><h1 id="scrapy-简单爬虫示例"><a href="#scrapy-简单爬虫示例" class="headerlink" title="scrapy 简单爬虫示例"></a>scrapy 简单爬虫示例</h1><p>专供爬虫初学者训练爬虫技术的网站 <a href="http://books.toscrape.com" target="_blank" rel="noopener">http://books.toscrape.com</a> ，从这里开始。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/scrapydemo.jpg" alt=""></p><h2 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h2><p>首先，来创建一个 scrapy 项目，在新建好的 scrapy_demo 文件夹下打开命令行工具，输入如下命令,在这里我创建好了名为 demo 的工程文件夹。<br><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$ </span>scrapy startproject projectName</span><br></pre></td></tr></table></figure></p><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/scrapynew.jpg" alt=""></p><p>接下来再切换到项目文件夹根目录下，使用命令创建爬虫文件。可以看到提示已经使用 basic 模板创建了一个名为 books 的爬虫文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> projectName</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> scrapy genspider spiderName[爬虫名称] url[爬虫网址]</span></span><br></pre></td></tr></table></figure><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/scrapy_book.jpg" alt=""></p><p>在此之后，使用 Pycharm 将整个 scrapy 项目导入。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/scrapypycharm.jpg" alt=""></p><p><code>scrapy.cfg</code> 项目的配置信息，主要为Scrapy命令行工具提供一个基础的配置信息。（真正爬虫相关的配置信息在settings.py文件中）</p><p><code>items.py</code> 设置数据存储模板，用于结构化数据，如：Django的Model</p><p><code>pipelines.py</code> 数据处理行为，如：一般结构化的数据持久化</p><p><code>settings.py</code> 配置文件，如：递归的层数、并发数，延迟下载等</p><p><code>spiders</code> 爬虫目录，如：创建文件，编写爬虫规则</p><h2 id="设置数据存储模板"><a href="#设置数据存储模板" class="headerlink" title="设置数据存储模板"></a>设置数据存储模板</h2><p>要爬取的是各个图书的标题，价格信息。需要在 items.py 中设置数据存储模板。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DemoItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    price = scrapy.Field()</span><br></pre></td></tr></table></figure><h2 id="编写爬虫"><a href="#编写爬虫" class="headerlink" title="编写爬虫"></a>编写爬虫</h2><p>爬虫规则在 books.py 中编写。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> demo.items <span class="keyword">import</span> DemoItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'books'</span></span><br><span class="line">    allowed_domains = [<span class="string">'toscrape.com'</span>]</span><br><span class="line">    <span class="comment"># 爬虫起始页</span></span><br><span class="line">    start_urls = [<span class="string">'http://books.toscrape.com/catalogue/page-1.html'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        items = response.xpath(<span class="string">"//article[@class='product_pod']"</span>)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">            title = item.xpath(<span class="string">"./h3/a/text()"</span>).get()</span><br><span class="line">            price = item.xpath(<span class="string">"./div[2]/p/text()"</span>).get()</span><br><span class="line">            item = DemoItem(title=title, price=price)</span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure></p><p>在运行爬虫之前，先在 settings.py 中设置好相关项，把 ROBOTSTXT_OBEY 置为 False ，并且打开浏览器模拟。</p><p>一切都设置好后，就可以运行爬虫了。为了方便运行，我习惯在项目根目录下新建一个 start.py 用来启动项目，其中代码如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> cmdline</span><br><span class="line"></span><br><span class="line">cmdline.execute(<span class="string">'scrapy crawl books'</span>.split())</span><br></pre></td></tr></table></figure><p>直接运行 start.py，得到如下结果。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">36</span> [scrapy.utils.log] INFO: Scrapy <span class="number">1.5</span><span class="number">.0</span> started (bot: demo)</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">36</span> [scrapy.utils.log] INFO: Versions: lxml <span class="number">4.2</span><span class="number">.1</span><span class="number">.0</span>, libxml2 <span class="number">2.9</span><span class="number">.8</span>, cssselect <span class="number">1.0</span><span class="number">.3</span>, parsel <span class="number">1.4</span><span class="number">.0</span>, w3lib <span class="number">1.19</span><span class="number">.0</span>, Twisted <span class="number">17.5</span><span class="number">.0</span>, Python <span class="number">3.6</span><span class="number">.5</span> |Anaconda, Inc.| (default, Mar <span class="number">29</span> <span class="number">2018</span>, <span class="number">13</span>:<span class="number">32</span>:<span class="number">41</span>) [MSC v<span class="number">.1900</span> <span class="number">64</span> bit (AMD64)], pyOpenSSL <span class="number">18.0</span><span class="number">.0</span> (OpenSSL <span class="number">1.0</span><span class="number">.2</span>o  <span class="number">27</span> Mar <span class="number">2018</span>), cryptography <span class="number">2.2</span><span class="number">.2</span>, Platform Windows<span class="number">-7</span><span class="number">-6.1</span><span class="number">.7601</span>-SP1</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">36</span> [scrapy.crawler] INFO: Overridden settings: &#123;<span class="string">'BOT_NAME'</span>: <span class="string">'demo'</span>, <span class="string">'NEWSPIDER_MODULE'</span>: <span class="string">'demo.spiders'</span>, <span class="string">'SPIDER_MODULES'</span>: [<span class="string">'demo.spiders'</span>]&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">36</span> [scrapy.middleware] INFO: Enabled extensions:</span><br><span class="line">[<span class="string">'scrapy.extensions.corestats.CoreStats'</span>,</span><br><span class="line"> <span class="string">'scrapy.extensions.telnet.TelnetConsole'</span>,</span><br><span class="line"> <span class="string">'scrapy.extensions.logstats.LogStats'</span>]</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.middleware] INFO: Enabled downloader middlewares:</span><br><span class="line">[<span class="string">'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.retry.RetryMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.cookies.CookiesMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.stats.DownloaderStats'</span>]</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.middleware] INFO: Enabled spider middlewares:</span><br><span class="line">[<span class="string">'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.offsite.OffsiteMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.referer.RefererMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.depth.DepthMiddleware'</span>]</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.middleware] INFO: Enabled item pipelines:</span><br><span class="line">[]</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.engine] INFO: Spider opened</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.extensions.logstats] INFO: Crawled <span class="number">0</span> pages (at <span class="number">0</span> pages/min), scraped <span class="number">0</span> items (at <span class="number">0</span> items/min)</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [py.warnings] WARNING: C:\ProgramData\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py:<span class="number">59</span>: URLWarning: allowed_domains accepts only domains, <span class="keyword">not</span> URLs. Ignoring URL entry http://toscrape.com/ <span class="keyword">in</span> allowed_domains.</span><br><span class="line">  warnings.warn(<span class="string">"allowed_domains accepts only domains, not URLs. Ignoring URL entry %s in allowed_domains."</span> % domain, URLWarning)</span><br><span class="line"></span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.extensions.telnet] DEBUG: Telnet console listening on <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">6023</span></span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.engine] DEBUG: Crawled (<span class="number">200</span>) &lt;GET http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt; (referer: <span class="keyword">None</span>)</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£51.77'</span>, <span class="string">'title'</span>: <span class="string">'A Light in the ...'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£53.74'</span>, <span class="string">'title'</span>: <span class="string">'Tipping the Velvet'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£50.10'</span>, <span class="string">'title'</span>: <span class="string">'Soumission'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£47.82'</span>, <span class="string">'title'</span>: <span class="string">'Sharp Objects'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£54.23'</span>, <span class="string">'title'</span>: <span class="string">'Sapiens: A Brief History ...'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£22.65'</span>, <span class="string">'title'</span>: <span class="string">'The Requiem Red'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£33.34'</span>, <span class="string">'title'</span>: <span class="string">'The Dirty Little Secrets ...'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£17.93'</span>, <span class="string">'title'</span>: <span class="string">'The Coming Woman: A ...'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£22.60'</span>, <span class="string">'title'</span>: <span class="string">'The Boys in the ...'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£52.15'</span>, <span class="string">'title'</span>: <span class="string">'The Black Maria'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£13.99'</span>, <span class="string">'title'</span>: <span class="string">'Starving Hearts (Triangular Trade ...'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£20.66'</span>, <span class="string">'title'</span>: <span class="string">"Shakespeare's Sonnets"</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£17.46'</span>, <span class="string">'title'</span>: <span class="string">'Set Me Free'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£52.29'</span>, <span class="string">'title'</span>: <span class="string">"Scott Pilgrim's Precious Little ..."</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£35.02'</span>, <span class="string">'title'</span>: <span class="string">'Rip it Up and ...'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£57.25'</span>, <span class="string">'title'</span>: <span class="string">'Our Band Could Be ...'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£23.88'</span>, <span class="string">'title'</span>: <span class="string">'Olio'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£37.59'</span>, <span class="string">'title'</span>: <span class="string">'Mesaerion: The Best Science ...'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£51.33'</span>, <span class="string">'title'</span>: <span class="string">'Libertarianism for Beginners'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£45.17'</span>, <span class="string">'title'</span>: <span class="string">"It's Only the Himalayas"</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.engine] INFO: Closing spider (finished)</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.statscollectors] INFO: Dumping Scrapy stats:</span><br><span class="line">&#123;<span class="string">'downloader/request_bytes'</span>: <span class="number">315</span>,</span><br><span class="line"> <span class="string">'downloader/request_count'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'downloader/request_method_count/GET'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'downloader/response_bytes'</span>: <span class="number">5889</span>,</span><br><span class="line"> <span class="string">'downloader/response_count'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'downloader/response_status_count/200'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'finish_reason'</span>: <span class="string">'finished'</span>,</span><br><span class="line"> <span class="string">'finish_time'</span>: datetime.datetime(<span class="number">2018</span>, <span class="number">8</span>, <span class="number">20</span>, <span class="number">9</span>, <span class="number">30</span>, <span class="number">37</span>, <span class="number">857663</span>),</span><br><span class="line"> <span class="string">'item_scraped_count'</span>: <span class="number">20</span>,</span><br><span class="line"> <span class="string">'log_count/DEBUG'</span>: <span class="number">22</span>,</span><br><span class="line"> <span class="string">'log_count/INFO'</span>: <span class="number">7</span>,</span><br><span class="line"> <span class="string">'log_count/WARNING'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'response_received_count'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'scheduler/dequeued'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'scheduler/dequeued/memory'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'scheduler/enqueued'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'scheduler/enqueued/memory'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'start_time'</span>: datetime.datetime(<span class="number">2018</span>, <span class="number">8</span>, <span class="number">20</span>, <span class="number">9</span>, <span class="number">30</span>, <span class="number">37</span>, <span class="number">96620</span>)&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.engine] INFO: Spider closed (finished)</span><br></pre></td></tr></table></figure></p><p>可以看到，scrapy 在经过初始化之后开始爬虫，并且输出了所需的价格和标题信息。</p><h2 id="保存数据"><a href="#保存数据" class="headerlink" title="保存数据"></a>保存数据</h2><p>暂不做介绍。</p><hr>]]></content>
    
    <summary type="html">
    
      Scrapy 是一个为了爬取网站数据，提取结构性数据而编写的应用框架。可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。
    
    </summary>
    
    
      <category term="爬虫" scheme="https://tangx1.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Python" scheme="https://tangx1.com/tags/Python/"/>
    
      <category term="Scrapy" scheme="https://tangx1.com/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Github模拟登陆</title>
    <link href="https://tangx1.com/github/"/>
    <id>https://tangx1.com/github/</id>
    <published>2018-05-01T09:20:41.000Z</published>
    <updated>2019-01-20T10:01:21.637Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/github/github-logo.png" alt=""></p><hr><p>github 的登录页面没有验证码，而且比较简单，所以先拿 github 练练手。</p><p>按照惯例，先规划一下步骤：</p><p>1.进入登录页，F12 查看请求，查看必要参数。</p><p>2.构造登录函数。</p><p>3.爬虫成功登陆后采集信息。</p><hr><h1 id="进入登录页真实登录"><a href="#进入登录页真实登录" class="headerlink" title="进入登录页真实登录"></a>进入登录页真实登录</h1><p>登录页网址: <a href="https://github.com/login" target="_blank" rel="noopener">https://github.com/login</a></p><p>页面如下：</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/github/github1.jpg" alt=""></p><p>现在先输入正确的账号密码进行登录。</p><p>打开 F12 开发工具，打开 Network 标签，先输入正确的账号密码，点击 <code>Sign in</code>，然后观察请求。可以看到，第一条请求是 <a href="https://github.com/session" target="_blank" rel="noopener">https://github.com/session</a> , 请求方式是 post 方式，而且 Form Data 里面存在账号密码信息，ok，就是这个请求，如下图。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/github/github2.jpg" alt=""></p><p>能看到请求方式为 post 然后看下 Form Data ，其中有这么几个参数，<code>commit</code>，<code>utf8</code>，<code>authenticity_token</code>，<code>login</code>，<code>password</code>，经过观察，<code>commit</code>，<code>utf8</code>，<code>login</code>，<code>password</code> 这几个都是固定值，而 <code>authenticity_token</code> 每次登录都会发生改变，先找到这个参数是怎么出来的，才方便构造登录函数，经查找后发现在登录页，也就是 <code>https://github.com/login</code> 这个页面，再查看源代码可以查找到这个 <code>token</code> 值，如下图所示。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/github/github3.jpg" alt=""></p><h1 id="构造登录函数"><a href="#构造登录函数" class="headerlink" title="构造登录函数"></a>构造登录函数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">github_login</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Mobile Safari/537.36'</span>,</span><br><span class="line">        &#125;</span><br><span class="line">        self.login_url = <span class="string">'https://github.com/login'</span></span><br><span class="line">        self.post_url = <span class="string">'https://github.com/session'</span></span><br><span class="line">        self.logined_url = <span class="string">'https://github.com/settings/profile'</span></span><br><span class="line">        <span class="comment"># 为了保持会话，需要用到 requests 的 session 服务</span></span><br><span class="line">        self.session = requests.Session()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取token值</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_token</span><span class="params">(self)</span>:</span></span><br><span class="line">        html = self.session.get(self.login_url, headers=self.headers)</span><br><span class="line">        response = etree.HTML(html.text)</span><br><span class="line">        token = response.xpath(<span class="string">"//input[@name='authenticity_token']/@value"</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> token</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 上面已经获取到了token参数，那么开始写登录函数</span></span><br><span class="line">    <span class="comment"># 把username和password作为变量，后期传入</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self, token, email, password)</span>:</span></span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">'commit'</span>: <span class="string">'Sign in'</span>,</span><br><span class="line">            <span class="string">'utf8'</span>: <span class="string">'✓'</span>,</span><br><span class="line">            <span class="string">'authenticity_token'</span>: token,</span><br><span class="line">            <span class="string">'login'</span>: email,</span><br><span class="line">            <span class="string">'password'</span>: password</span><br><span class="line">        &#125;</span><br><span class="line">        response = self.session.post(self.post_url, data=data, headers=self.headers)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            print(<span class="string">'登录成功，正在跳转到个人信息...'</span>)</span><br></pre></td></tr></table></figure><h1 id="采集信息"><a href="#采集信息" class="headerlink" title="采集信息"></a>采集信息</h1><p>上面得到 <code>token</code> 值，也成功登录了 github 。现在我们就可以开始采集信息了。在这里我选择进入个人详情页来获取用户名和地址信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, html)</span>:</span></span><br><span class="line">    response = etree.HTML(html)</span><br><span class="line">    name = response.xpath(<span class="string">"//dl[@class='form-group'][1]/dd[1]/input/@value"</span>)[<span class="number">0</span>]</span><br><span class="line">    location = response.xpath(<span class="string">"//dl[@class='form-group'][6]/dd[1]/input/@value"</span>)[<span class="number">0</span>]</span><br><span class="line">    print(<span class="string">'姓名: %s, 地址: %s'</span> % (name, location))</span><br></pre></td></tr></table></figure><p>获取到用户名和地址信息。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/github/github_info.png" alt=""></p><h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">github_login</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Mobile Safari/537.36'</span>,</span><br><span class="line">        &#125;</span><br><span class="line">        self.login_url = <span class="string">'https://github.com/login'</span></span><br><span class="line">        self.post_url = <span class="string">'https://github.com/session'</span></span><br><span class="line">        self.logined_url = <span class="string">'https://github.com/settings/profile'</span></span><br><span class="line">        self.session = requests.Session()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_token</span><span class="params">(self)</span>:</span></span><br><span class="line">        html = self.session.get(self.login_url, headers=self.headers)</span><br><span class="line">        response = etree.HTML(html.text)</span><br><span class="line">        token = response.xpath(<span class="string">"//input[@name='authenticity_token']/@value"</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> token</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, html)</span>:</span></span><br><span class="line">        response = etree.HTML(html)</span><br><span class="line">        name = response.xpath(<span class="string">"//dl[@class='form-group'][1]/dd[1]/input/@value"</span>)[<span class="number">0</span>]</span><br><span class="line">        location = response.xpath(<span class="string">"//dl[@class='form-group'][6]/dd[1]/input/@value"</span>)[<span class="number">0</span>]</span><br><span class="line">        print(<span class="string">'姓名: %s, 地址: %s'</span> % (name, location))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self, token, email, password)</span>:</span></span><br><span class="line"></span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">'commit'</span>: <span class="string">'Sign in'</span>,</span><br><span class="line">            <span class="string">'utf8'</span>: <span class="string">'✓'</span>,</span><br><span class="line">            <span class="string">'authenticity_token'</span>: token,</span><br><span class="line">            <span class="string">'login'</span>: email,</span><br><span class="line">            <span class="string">'password'</span>: password</span><br><span class="line">        &#125;</span><br><span class="line">        response = self.session.post(self.post_url, data=data, headers=self.headers)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            print(<span class="string">'登录成功，正在跳转到个人信息...'</span>)</span><br><span class="line"></span><br><span class="line">        response = self.session.get(self.logined_url, headers=self.headers)</span><br><span class="line">        self.parse(response.text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    login = github_login()</span><br><span class="line">    token = login.get_token()</span><br><span class="line">    login.login(token, <span class="string">'username'</span>, <span class="string">'password'</span>)</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    <summary type="html">
    
      目前，大部分网站都具有用户登录功能，其中的某些网站只有在用户登陆后才能获取到有价值的信息，因此爬虫也就需要有模拟真实用户登录的功能。
    
    </summary>
    
    
      <category term="爬虫" scheme="https://tangx1.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫--美团美食信息</title>
    <link href="https://tangx1.com/meituanspider/"/>
    <id>https://tangx1.com/meituanspider/</id>
    <published>2018-04-25T09:43:37.000Z</published>
    <updated>2019-01-15T02:47:05.254Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/meituan/meituan-logo.png" alt=""></p><hr><p>美团网也是动态加载的网页，但是我看了下，其美食信息也在源代码中有，因此可以按照<a href="http://tangx1.com/2018/08/15/taobao2/">Python爬虫–淘宝(2)</a>其中的方式去爬美食信息。</p><p>步骤照搬：</p><p>1.获取网页源代码。</p><p>2.使用网页解析工具进行解析，构造翻页网址，提取所需信息。</p><p>3.整理并保存信息。</p><hr><h1 id="获取网页源代码"><a href="#获取网页源代码" class="headerlink" title="获取网页源代码"></a>获取网页源代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以爬取北京美团美食列表为例</span></span><br><span class="line">url = <span class="string">'http://bj.meituan.com/meishi/'</span></span><br><span class="line"><span class="comment"># 请求网页</span></span><br><span class="line">response = requests.get(url, headers=headers)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure><p><img src="https://hexo-pics.nos-eastchina1.126.net/meituan/meituan1-1.jpg" alt=""></p><h1 id="提取信息"><a href="#提取信息" class="headerlink" title="提取信息"></a>提取信息</h1><p>在上一步操作得到源代码之后，仔细观察，可以看到带有商家信息的部分为一段 json 数据，这里稍有不同的是，我们可以直接在源代码中使用正则表达式匹配出含有商家信息的 json 信息。  </p><h2 id="匹配-json-数据段"><a href="#匹配-json-数据段" class="headerlink" title="匹配 json 数据段"></a>匹配 json 数据段</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="comment"># findall获取得到的是一个列表</span></span><br><span class="line">items = re.findall(<span class="string">r'"poiLists":(.`?),"comHeader"'</span>, response.text, re.S)</span><br><span class="line"><span class="comment"># 而列表中只有一个元素</span></span><br><span class="line">item = json.loads(items[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p><img src="https://hexo-pics.nos-eastchina1.126.net/meituan/meituan1-2.jpg" alt=""></p><h2 id="提取信息-1"><a href="#提取信息-1" class="headerlink" title="提取信息"></a>提取信息</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> item.get(<span class="string">'poiInfos'</span>):</span><br><span class="line">    <span class="keyword">for</span> info <span class="keyword">in</span> item.get(<span class="string">'poiInfos'</span>):</span><br><span class="line">        id = info.get(<span class="string">'poiId'</span>)</span><br><span class="line">        title = info.get(<span class="string">'title'</span>)</span><br><span class="line">        addr = info.get(<span class="string">'address'</span>)</span><br><span class="line">        avgScore = <span class="string">'平均分'</span> + str(info.get(<span class="string">'avgScore'</span>))</span><br><span class="line">        allCommentNum = str(info.get(<span class="string">'allCommentNum'</span>)) + <span class="string">'条评论'</span></span><br><span class="line">        avgPrice = str(info.get(<span class="string">'avgPrice'</span>)) + <span class="string">'元/人'</span></span><br><span class="line">        print(id, title, addr, avgPrice, avgScore, allCommentNum)</span><br></pre></td></tr></table></figure><p><img src="https://hexo-pics.nos-eastchina1.126.net/meituan/meituan1-3.jpg" alt=""></p><h1 id="构造翻页网址"><a href="#构造翻页网址" class="headerlink" title="构造翻页网址"></a>构造翻页网址</h1><p>通过翻页找到页码规律，每个网页最后面 <code>pn</code> 后面的数字就是页码。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 爬取 1-9 页美食信息</span></span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">10</span>):</span><br><span class="line">    url = <span class="string">'http://bj.meituan.com/meishi/pn&#123;&#125;'</span>.format(page)</span><br></pre></td></tr></table></figure></p><p><img src="https://hexo-pics.nos-eastchina1.126.net/meituan/meituan1-4.jpg" alt=""></p><h1 id="保存数据"><a href="#保存数据" class="headerlink" title="保存数据"></a>保存数据</h1><p>保存到MongoDB数据库。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/meituan/meituanmongo1.jpg" alt=""></p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">10</span>):</span><br><span class="line">    url = <span class="string">'http://bj.meituan.com/meishi/pn&#123;&#125;'</span>.format(page)</span><br><span class="line">    response = requests.get(url, headers=headers)</span><br><span class="line">    items = re.findall(<span class="string">r'"poiLists":(.`?),"comHeader"'</span>, response.text, re.S)</span><br><span class="line">    item = json.loads(items[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> item.get(<span class="string">'poiInfos'</span>):</span><br><span class="line">        <span class="keyword">for</span> info <span class="keyword">in</span> item.get(<span class="string">'poiInfos'</span>):</span><br><span class="line">            id = info.get(<span class="string">'poiId'</span>)</span><br><span class="line">            title = info.get(<span class="string">'title'</span>)</span><br><span class="line">            addr = info.get(<span class="string">'address'</span>)</span><br><span class="line">            avgScore = <span class="string">'平均分'</span> + str(info.get(<span class="string">'avgScore'</span>))</span><br><span class="line">            allCommentNum = str(info.get(<span class="string">'allCommentNum'</span>)) + <span class="string">'条评论'</span></span><br><span class="line">            avgPrice = str(info.get(<span class="string">'avgPrice'</span>)) + <span class="string">'元/人'</span></span><br><span class="line">            print(id, title, addr, avgPrice, avgScore, allCommentNum)</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    <summary type="html">
    
      美团网的美食信息，
    
    </summary>
    
    
      <category term="爬虫" scheme="https://tangx1.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫之淘宝(二)</title>
    <link href="https://tangx1.com/taobaospider2/"/>
    <id>https://tangx1.com/taobaospider2/</id>
    <published>2018-04-18T08:00:33.000Z</published>
    <updated>2019-01-15T02:49:22.537Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/tb/tb-logo.png" alt=""></p><hr><p>上一篇介绍了如何用 selenium 自动化工具去帮助爬取淘宝商品的各项数据，但是，我发现，淘宝的网页源代码中就包含有宝贝信息，信息被放在了 script 标签中，既然信息被包含在源代码中，那就证明可以通过正则或者其它的网页解析方式可以获取到需要的信息，因此这篇就来写一写对于淘宝商品来说更为简单的小爬虫。</p><p>这次的流程就比较简单了，先做好准备工作：</p><p>1.获取网页源代码。</p><p>2.使用网页解析工具进行解析，构造翻页网址，提取所需信息。</p><p>3.整理并保存信息。</p><hr><h1 id="获取网页源代码"><a href="#获取网页源代码" class="headerlink" title="获取网页源代码"></a>获取网页源代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 搜索关键词为python的商品列表页</span></span><br><span class="line">url = <span class="string">'https://s.taobao.com/search?q=python'</span></span><br><span class="line"><span class="comment"># 请求网页</span></span><br><span class="line">response = requests.get(url, headers=headers)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure><p>通过 requests 的 get 方法请求网页之后，得到网页源代码。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/tb/taobao2-1.jpg" alt=""></p><h1 id="提取信息"><a href="#提取信息" class="headerlink" title="提取信息"></a>提取信息</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">title = re.findall(<span class="string">r'"raw_title":"(.`?)"'</span>, response.text, re.I)</span><br><span class="line">price = re.findall(<span class="string">r'"view_price":"(.`?)"'</span>, response.text, re.I)</span><br><span class="line">location = re.findall(<span class="string">r'"item_loc":"(.`?)"'</span>, response.text, re.I)</span><br><span class="line">n = len(title)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    print(title[i], price[i], location[i])</span><br></pre></td></tr></table></figure><p>在这里只对商品的标题，价格和发货地进行了正则匹配，因为它们都是列表，要想让宝贝信息都一一对应的显示出来，就要进行遍历，最后把宝贝信息都进行输出。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/tb/taobao2-2.jpg" alt=""></p><h1 id="整理信息"><a href="#整理信息" class="headerlink" title="整理信息"></a>整理信息</h1><p>获取到了宝贝信息之后我们就可以把它们存入文件或者数据库了。这里我们先把它们存入文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">file = open(<span class="string">'taobao.txt'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">n = len(title)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    info = str(page ` <span class="number">44</span> + i + <span class="number">1</span>) + <span class="string">'标题：'</span>+ title[i] + <span class="string">'\n'</span> + <span class="string">'价格：'</span> + price[i] + <span class="string">'\n'</span> + <span class="string">'发货地：'</span> + location[i] + <span class="string">'\n'</span></span><br><span class="line">    file.write(info)</span><br><span class="line">file.close()</span><br></pre></td></tr></table></figure><p><img src="https://hexo-pics.nos-eastchina1.126.net/tb/taobao2-3.jpg" alt=""></p><h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">payload = &#123;<span class="string">'q'</span>:<span class="string">'python'</span>, <span class="string">'s'</span>:<span class="string">'1'</span>, <span class="string">'ie'</span>:<span class="string">'utf8'</span>&#125;</span><br><span class="line">file = open(<span class="string">'taobao.txt'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    payload[<span class="string">'s'</span>] = <span class="number">44</span> ` page + <span class="number">1</span></span><br><span class="line">    url = <span class="string">'https://s.taobao.com/search?q=python'</span></span><br><span class="line">    response = requests.get(url, params=payload, headers=headers)</span><br><span class="line">    title = re.findall(<span class="string">r'"raw_title":"(.`?)"'</span>, response.text, re.I)</span><br><span class="line">    price = re.findall(<span class="string">r'"view_price":"(.`?)"'</span>, response.text, re.I)</span><br><span class="line">    location = re.findall(<span class="string">r'"item_loc":"(.`?)"'</span>, response.text, re.I)</span><br><span class="line">    n = len(title)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        info = str(page ` <span class="number">44</span> + i + <span class="number">1</span>) + <span class="string">'标题：'</span>+ title[i] + <span class="string">'\n'</span> + <span class="string">'价格：'</span> + price[i] + <span class="string">'\n'</span> + <span class="string">'发货地：'</span> + location[i] + <span class="string">'\n'</span></span><br><span class="line">        file.write(info)</span><br><span class="line">file.close()</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    <summary type="html">
    
      上一篇介绍了如何用 selenium 自动化工具去帮助爬取淘宝商品的各项数据，但是，我发现，淘宝的网页源代码中就包含有宝贝信息，信息被放在了 script 标签中，既然信息被包含在源代码中，那就证明可以通过正则或者其它的网页解析方式可以获取到需要的信息，因此这篇就来写一写对于淘宝商品来说更为简单的小爬虫。
    
    </summary>
    
      <category term="Python" scheme="https://tangx1.com/categories/Python/"/>
    
    
      <category term="爬虫" scheme="https://tangx1.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
</feed>
