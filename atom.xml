<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>2048</title>
  
  <subtitle>Life is short I use Python</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-01-15T02:44:05.303Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Tang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>域名ssl证书部署到服务器</title>
    <link href="http://yoursite.com/nginx/"/>
    <id>http://yoursite.com/nginx/</id>
    <published>2019-01-15T01:11:41.000Z</published>
    <updated>2019-01-15T02:44:05.303Z</updated>
    
    <content type="html"><![CDATA[<h1 id="申请-SSL-证书"><a href="#申请-SSL-证书" class="headerlink" title="申请 SSL 证书"></a>申请 SSL 证书</h1><p>首先进入腾讯云 <a href="https://console.qcloud.com/ssl" target="_blank" rel="noopener">SSL域名管理</a>，点击按钮<code>申请证书</code>，选择第一个免费证书，我这里已经申请过了，简单演示一下，输入绑定域名，邮箱然后选填的都可以不填，直接下一步，如果域名已经绑定了服务器就选择自动，否则选择手动，最后一个不用管。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/nginx/ssl.gif" alt=""></p><p>提交之后耐心等待，审核通过腾讯云官方会发送邮件提醒。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/nginx/ssl_apply.png" alt=""></p><h1 id="下载-SSL-证书"><a href="#下载-SSL-证书" class="headerlink" title="下载 SSL 证书"></a>下载 SSL 证书</h1><p>申请成功后，点击右边的下载按钮下载证书文件。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/nginx/ssl_detail.png" alt=""></p><p>下载解压后，文件中有几个子文件夹，分别是 Apache、IIS、Nginx 服务器的证书文件。<a href="https://cloud.tencent.com/document/product/400/4143" target="_blank" rel="noopener">证书安装指引</a></p><p><img src="https://hexo-pics.nos-eastchina1.126.net/nginx/ssl_down.png" alt=""></p><h1 id="Nginx-证书部署"><a href="#Nginx-证书部署" class="headerlink" title="Nginx 证书部署"></a>Nginx 证书部署</h1><p>Nginx 文件夹内获得 SSL 证书文件 1_<a href="http://www.domain.com_bundle.crt" target="_blank" rel="noopener">www.domain.com_bundle.crt</a> 和私钥文件2_<a href="http://www.domain.com.key" target="_blank" rel="noopener">www.domain.com.key</a> 。 1_<a href="http://www.domain.com_bundle.crt" target="_blank" rel="noopener">www.domain.com_bundle.crt</a> 文件包括两段证书代码 “—–BEGIN CERTIFICATE—–” 和 “—–END CERTIFICATE—–”，2_<a href="http://www.domain.com.key" target="_blank" rel="noopener">www.domain.com.key</a> 文件包括一段私钥代码 “—–BEGIN RSA PRIVATE KEY—–” 和 “—–END RSA PRIVATE KEY—–”。</p><p>将这两个文件上传到服务器中，将域名 <a href="http://www.domain.com" target="_blank" rel="noopener">www.domain.com</a> 的证书文件 1_<a href="http://www.domain.com_bundle.crt" target="_blank" rel="noopener">www.domain.com_bundle.crt</a> 、私钥文件 2_<a href="http://www.domain.com.key" target="_blank" rel="noopener">www.domain.com.key</a> 保存到同一个目录，例如 /usr/local/nginx 目录下。</p><p>修改 Nginx 根目录下 conf/nginx.conf 文件，内容如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">        listen <span class="number">443</span>;</span><br><span class="line">        server_name www.domain.com; <span class="comment">#填写绑定证书的域名</span></span><br><span class="line">        ssl on;</span><br><span class="line">        ssl_certificate <span class="number">1</span>_www.domain.com_bundle.crt;</span><br><span class="line">        ssl_certificate_key <span class="number">2</span>_www.domain.com.key;</span><br><span class="line">        ssl_session_timeout <span class="number">5</span>m;</span><br><span class="line">        ssl_protocols TLSv1 TLSv1<span class="number">.1</span> TLSv1<span class="number">.2</span>; <span class="comment">#按照这个协议配置</span></span><br><span class="line">        ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE;<span class="comment">#按照这个套件配置</span></span><br><span class="line">        ssl_prefer_server_ciphers on;</span><br><span class="line">        location / &#123;</span><br><span class="line">            root   html; <span class="comment">#站点目录</span></span><br><span class="line">            index  index.html index.htm;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><p>配置完成后，请先执行命令 nginx –t 测试 Nginx 配置是否有误。若无报错，重启 Nginx 之后，即可使用 <a href="https://www.domain.com" target="_blank" rel="noopener">https://www.domain.com</a> 来访问。</p><p>如果这里你的 https 域名已经可以正常访问了，那就不用往下看了。</p><h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>我重启了 Nginx 服务器后，重新使用 https 访问我的网址，可以正常访问，但是直接输入我的网址域名，不加 http 或者 https，网页提示“400 Bad Request: The plain HTTP request was sent to HTTPS port”，如图。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/400.png" alt=""></p><blockquote><p>文章引自 <a href="https://blog.csdn.net/system1024/article/details/52636147" target="_blank" rel="noopener">https://blog.csdn.net/system1024/article/details/52636147</a></p></blockquote><p>解决方法是修改服务器的 /usr/local/nginx/nginx.conf 文件如下。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">        listen       <span class="number">80</span> default_server;</span><br><span class="line">        listen       <span class="number">443</span> ssl; <span class="comment"># 443 后面加上 ssl</span></span><br><span class="line">        <span class="comment">#ssl          on;       # 删除此行</span></span><br><span class="line">        server_name  tangx1.com;</span><br><span class="line">        root         /usr/share/nginx/html;</span><br><span class="line">        ssl_certificate <span class="number">1</span>_tangx1.com_bundle.crt;</span><br><span class="line">        ssl_certificate_key <span class="number">2</span>_tangx1.com.key;</span><br><span class="line">        ssl_session_timeout <span class="number">5</span>m;</span><br><span class="line">        ssl_protocols TLSv1 TLSv1<span class="number">.1</span> TLSv1<span class="number">.2</span>;</span><br><span class="line">        ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE;</span><br><span class="line">        ssl_prefer_server_ciphers on;</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure></p><hr>]]></content>
    
    <summary type="html">
    
      用 Chrome 打开网站总是提示不安全，看着烦心，干脆把域名挂上 https 之后就不提示了。此篇记录了腾讯云的证书部署过程，部署部分参考了腾讯云的官方文档，文章末尾记录了遇到的小问题。
    
    </summary>
    
      <category term="Nginx" scheme="http://yoursite.com/categories/Nginx/"/>
    
    
      <category term="服务器" scheme="http://yoursite.com/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
      <category term="Nginx" scheme="http://yoursite.com/tags/Nginx/"/>
    
  </entry>
  
  <entry>
    <title>Python中的网络编程</title>
    <link href="http://yoursite.com/socket/"/>
    <id>http://yoursite.com/socket/</id>
    <published>2019-01-12T05:48:00.000Z</published>
    <updated>2019-01-12T06:34:49.923Z</updated>
    
    <content type="html"><![CDATA[<h1 id="socket-模块函数"><a href="#socket-模块函数" class="headerlink" title="socket() 模块函数"></a>socket() 模块函数</h1><p>要创建套接字，必须使用 socket.socket() 函数，它一般的语法如下。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">socket(socket_family, socket_type, protocol=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p><p>其中， socket_family 是 AF_UNIX 或 AF_INET， socket_type 是 SOCK_STREAM 或 SOCK_DGRAM 。protocol 通常省略，默认为 0 。</p><p>所以，为了创建 TCP/IP 套接字，可以用下面的方式调用 socket.socket()。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpSock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br></pre></td></tr></table></figure></p><p>同样，为了创建 UDP/IP 套接字，需要执行以下语句。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">udpSock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)</span><br></pre></td></tr></table></figure></p><p>因为有很多 socket 模块属性，所以此时可以使用下面的语句来导入 socket 模块。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> socket <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure></p><p>然后可以创建套接字对象，再之后就可以使用套接字对象的方法进行进一步的交互。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpSock = socket(AF_INET, SOCK_STREAM)</span><br></pre></td></tr></table></figure></p><h1 id="UDP-客户端与服务器的创建"><a href="#UDP-客户端与服务器的创建" class="headerlink" title="UDP 客户端与服务器的创建"></a>UDP 客户端与服务器的创建</h1><h2 id="UDP-客户端的创建"><a href="#UDP-客户端的创建" class="headerlink" title="UDP 客户端的创建"></a>UDP 客户端的创建</h2><p>创建一个套接字对象，代码如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> socket <span class="keyword">import</span> *</span><br><span class="line">udpSock = socket(AF_INET, SOCK_DGRAM)</span><br></pre></td></tr></table></figure></p><p>然后使用 sendto() 方法可以发送信息，代码如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = input(<span class="string">"请输入你想要发送的信息："</span>)</span><br><span class="line"><span class="comment"># sendto(data, (ip, port))</span></span><br><span class="line">udpSock.sendto(data.encode(<span class="string">'gb2312'</span>), (<span class="string">'172.16.217.129'</span>, <span class="number">8080</span>))</span><br></pre></td></tr></table></figure></p><p>这里使用 mac 向 windows 发送一段信息，由于 windows 软件以 gb2312 解码，因此在 Python 代码中需要把要发送的信息编码成 gb2312，如图。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/python/udp.png" alt=""></p><h2 id="UDP-服务器的创建"><a href="#UDP-服务器的创建" class="headerlink" title="UDP 服务器的创建"></a>UDP 服务器的创建</h2><p>同上先创建一个套接字对象，和创建客户端不同，服务器需要绑定到某一个地址上，只有这样客户端才知道如何给服务器发送信息。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> socket <span class="keyword">import</span> *</span><br><span class="line">udpSock = socket(AF_INET, SOCK_DGRAM)</span><br><span class="line"><span class="comment"># 绑定到 7788 端口</span></span><br><span class="line">udpSock.bind((<span class="string">""</span>, <span class="number">7788</span>))</span><br><span class="line">udpSock.recvfrom(<span class="number">1024</span>)</span><br><span class="line">content, ip = udpdata</span><br><span class="line">print(ip, <span class="string">":"</span>, content.decode(<span class="string">"gb2312"</span>))</span><br></pre></td></tr></table></figure></p><p>这次使用 windows 向 mac 发送信息，如图。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/python/udp1.png" alt=""></p><hr>]]></content>
    
    <summary type="html">
    
      Python 网络编程使用的主要模块是 socket 模块，在这个模块中可以找到 socket() 函数，该函数用于创建套接字对象。套接字也有自己的方法集，这些方法可以实现基于套接字的网络通信。
    
    </summary>
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>bilibili之弹幕爬虫</title>
    <link href="http://yoursite.com/bilibili_danmu/"/>
    <id>http://yoursite.com/bilibili_danmu/</id>
    <published>2019-01-08T13:10:13.000Z</published>
    <updated>2019-01-14T09:50:21.729Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/bilibili/bilibili-logo.png" alt=""></p><hr><p>按照正常流程，打开网页，然后右键网页源代码，网页中并没有类似弹幕的文字出现，猜想应该是通过某些接口或者其它什么途径传过来的数据。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/bilibili/bilibli1.png" alt=""></p><p>打开 F12 开发者工具，查看 Network 下的请求，看看是不是弹幕隐藏在其中某个文件内，找了好久，竟然没有发现，后来发现其中一个前缀为 list.so?oid=xxxx的链接 在 Preview 模式下不显示数据，将其 url 用浏览器打开后却出现了我想要的弹幕数据。如下图所示。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/bilibili/danmu_list.png" alt=""></p><p><img src="https://hexo-pics.nos-eastchina1.126.net/bilibili/api.png" alt=""></p><p>既然找到了源文件，那爬虫就很好写了。</p><h1 id="下载并分析弹幕-xml-文件"><a href="#下载并分析弹幕-xml-文件" class="headerlink" title="下载并分析弹幕 xml 文件"></a>下载并分析弹幕 xml 文件</h1><p>创建一个爬虫类，然后定义 get_file 方法来请求并下载弹幕文件，然后使用 parse_danmus 方法来分析弹幕，弹幕信息都是 d 标签下，由此得到弹幕信息。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Danmu_Spider</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.danmu_url = <span class="string">'https://api.bilibili.com/x/v1/dm/list.so?oid=57763167'</span></span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Mobile Safari/537.36'</span>,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_file</span><span class="params">(self, url)</span>:</span></span><br><span class="line">        response = requests.get(url, headers=self.headers)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">with</span> open(<span class="string">'danmu.xml'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(response.content)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">'访问出错'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_danmus</span><span class="params">(self, file)</span>:</span></span><br><span class="line">        selector = etree.parse(file, etree.HTMLParser())</span><br><span class="line">        items = selector.xpath(<span class="string">"//d//text()"</span>)</span><br><span class="line">        <span class="keyword">return</span> items</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.get_file(self.danmu_url)</span><br><span class="line">        danmus = self.parse_danmus(<span class="string">'danmu.xml'</span>)</span><br><span class="line">        print(danmus)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">  spider = Danmu_Spider()</span><br><span class="line">  spider.run()</span><br></pre></td></tr></table></figure></p><p><img src="https://hexo-pics.nos-eastchina1.126.net/bilibili/danmu.png" alt=""></p><p>在图中可以看到成功爬取到了弹幕信息，数量为3000，和文章开头视频右边弹幕信息的数量一致，但是我这里将弹幕使用了集合去重，保留了有效弹幕。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/bilibili/danmu_quchong.png" alt=""></p><h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Danmu_Spider</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.danmu_url = <span class="string">'https://api.bilibili.com/x/v1/dm/list.so?oid=57763167'</span></span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Mobile Safari/537.36'</span>,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_file</span><span class="params">(self, url)</span>:</span></span><br><span class="line">        response = requests.get(url, headers=self.headers)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">with</span> open(<span class="string">'danmu.xml'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(response.content)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">'访问出错'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_danmus</span><span class="params">(self, file)</span>:</span></span><br><span class="line">        selector = etree.parse(file, etree.HTMLParser())</span><br><span class="line">        items = selector.xpath(<span class="string">"//d//text()"</span>)</span><br><span class="line">        items = set(items)</span><br><span class="line">        <span class="keyword">return</span> items</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.get_file(self.danmu_url)</span><br><span class="line">        danmus = self.parse_danmus(<span class="string">'danmu.xml'</span>)</span><br><span class="line">        print(<span class="string">'弹幕数量: %s'</span>% len(danmus))</span><br><span class="line">        print(danmus)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    spider = Danmu_Spider()</span><br><span class="line">    spider.run()</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    <summary type="html">
    
      有事没事的时候会去逛逛B站，我比较喜欢科技或者汽车频道，突然就想起来给B站视频的弹幕写一个爬虫，没接触这类的爬虫，借此机会学习一下。^_^
    
    </summary>
    
      <category term="爬虫" scheme="http://yoursite.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy模拟登录豆瓣网进阶篇</title>
    <link href="http://yoursite.com/scrapy-login-captcha/"/>
    <id>http://yoursite.com/scrapy-login-captcha/</id>
    <published>2019-01-03T04:47:38.000Z</published>
    <updated>2019-01-05T15:03:09.697Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/scrapy-logo.png" alt=""></p><hr><p>内容和上一篇大致差不多，重点说一下接入第三方验证码识别平台的问题。</p><p>验证码识别平台我使用的是 <a href="https://www.chaojiying.com/" target="_blank" rel="noopener">超级鹰</a> ，其是全球领先的智能图片分类及识别商家 ，具有安全、准确、高效、稳定、开放的特点，并且拥有强大的技术及校验团队。</p><p>超级鹰的开发文档: <a href="https://www.chaojiying.com/api.html" target="_blank" rel="noopener">API地址</a></p><p>为了方便，这里将 Python 版本的代码贴在下面，以后还用的上。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> md5</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Chaojiying_Client</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, username, password, soft_id)</span>:</span></span><br><span class="line">        self.username = username</span><br><span class="line">password =  password.encode(<span class="string">'utf8'</span>)</span><br><span class="line">        self.password = md5(password).hexdigest()</span><br><span class="line">        self.soft_id = soft_id</span><br><span class="line">        self.base_params = &#123;</span><br><span class="line">            <span class="string">'user'</span>: self.username,</span><br><span class="line">            <span class="string">'pass2'</span>: self.password,</span><br><span class="line">            <span class="string">'softid'</span>: self.soft_id,</span><br><span class="line">        &#125;</span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">'Connection'</span>: <span class="string">'Keep-Alive'</span>,</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)'</span>,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">PostPic</span><span class="params">(self, im, codetype)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        im: 图片字节</span></span><br><span class="line"><span class="string">        codetype: 题目类型 参考 http://www.chaojiying.com/price.html</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        params = &#123;</span><br><span class="line">            <span class="string">'codetype'</span>: codetype,</span><br><span class="line">        &#125;</span><br><span class="line">        params.update(self.base_params)</span><br><span class="line">        files = &#123;<span class="string">'userfile'</span>: (<span class="string">'ccc.jpg'</span>, im)&#125;</span><br><span class="line">        r = requests.post(<span class="string">'http://upload.chaojiying.net/Upload/Processing.php'</span>, data=params, files=files, headers=self.headers)</span><br><span class="line">        <span class="keyword">return</span> r.json()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ReportError</span><span class="params">(self, im_id)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        im_id:报错题目的图片ID</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        params = &#123;</span><br><span class="line">            <span class="string">'id'</span>: im_id,</span><br><span class="line">        &#125;</span><br><span class="line">        params.update(self.base_params)</span><br><span class="line">        r = requests.post(<span class="string">'http://upload.chaojiying.net/Upload/ReportError.php'</span>, data=params, headers=self.headers)</span><br><span class="line">        <span class="keyword">return</span> r.json()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">chaojiying = Chaojiying_Client(<span class="string">'超级鹰用户名'</span>, <span class="string">'超级鹰用户名的密码'</span>, <span class="string">'96001'</span>)<span class="comment">#用户中心&gt;&gt;软件ID 生成一个替换 96001</span></span><br><span class="line">im = open(<span class="string">'a.jpg'</span>, <span class="string">'rb'</span>).read()<span class="comment">#本地图片文件路径 来替换 a.jpg 有时WIN系统须要//</span></span><br><span class="line"><span class="keyword">print</span> chaojiying.PostPic(im, <span class="number">1902</span>) <span class="comment">#1902 验证码类型  官方网站&gt;&gt;价格体系 3.4+版 print 后要加()</span></span><br></pre></td></tr></table></figure></p><p>只要把开发文档中对应的 soft_id 和 codetype 填上去，将验证码图片保存到本地，就可以使用超级鹰识别验证码了。话不多说，代码如下。</p><h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LoginSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'login'</span></span><br><span class="line">    allowed_domains = [<span class="string">'douban.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'https://www.douban.com/people/xxxxxx/'</span>]</span><br><span class="line">    edit_signature_url = <span class="string">'https://www.douban.com/j/people/xxxxxx/edit_signature'</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'Connection'</span>: <span class="string">'Keep-Alive'</span>,</span><br><span class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)'</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self ,response)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> response.url == <span class="string">'https://www.douban.com/people/xxxxxx/'</span>:</span><br><span class="line">            print(<span class="string">'-----已经进入个人详情页-----'</span>)</span><br><span class="line">            print(<span class="string">'-----正在修改个人签名-----'</span>)</span><br><span class="line">            ck = response.xpath(<span class="string">"//*[@id='edit_signature']/form/div/input/@value"</span>).get()</span><br><span class="line">            data = &#123;</span><br><span class="line">                <span class="string">'ck'</span>: ck,</span><br><span class="line">                <span class="string">'signature'</span>: <span class="string">'我可以自动识别验证码啦~~'</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">yield</span> FormRequest(self.edit_signature_url, formdata=data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模拟登录</span></span><br><span class="line">    login_url = <span class="string">'https://accounts.douban.com/login'</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> Request(self.login_url, callback=self.login)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        print(<span class="string">'-----登录程序-----'</span>)</span><br><span class="line">        captcha_id = response.xpath(<span class="string">".//input[@name='captcha-id']/@value"</span>).get()</span><br><span class="line">        captcha_url = response.xpath(<span class="string">"//*[@id='captcha_image']/@src"</span>).get()</span><br><span class="line">        <span class="keyword">if</span> captcha_url <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            print(<span class="string">'-----登录时无验证码-----'</span>)</span><br><span class="line">            data = &#123;</span><br><span class="line">                <span class="string">'form_email'</span>: <span class="string">'xxxxxx@qq.com'</span>,</span><br><span class="line">                <span class="string">'form_password'</span>: <span class="string">'xxxxxx'</span></span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">'-----登录时有验证码-----'</span>)</span><br><span class="line">            print(<span class="string">'-----即将下载验证码-----'</span>)</span><br><span class="line">            request.urlretrieve(captcha_url, <span class="string">'captcha.png'</span>,)</span><br><span class="line"></span><br><span class="line">            captcha_solution = self.recognize_captcha(<span class="string">'captcha.png'</span>)</span><br><span class="line">            data = &#123;</span><br><span class="line">                <span class="string">'form_email'</span>: <span class="string">'xxxxxx@qq.com'</span>,</span><br><span class="line">                <span class="string">'form_password'</span>: <span class="string">'xxxxxx'</span>,</span><br><span class="line">                <span class="string">'captcha-solution'</span>: captcha_solution,</span><br><span class="line">                <span class="string">'captcha-id'</span>: captcha_id,</span><br><span class="line">                <span class="string">'login'</span>: <span class="string">'登录'</span></span><br><span class="line">            &#125;</span><br><span class="line">        print(<span class="string">'-----登录中-----'</span>)</span><br><span class="line">        <span class="keyword">yield</span> FormRequest.from_response(response, formdata=data, callback=self.parse_after_login)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_after_login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">"xxxxxx的帐号"</span> <span class="keyword">in</span> response.text:</span><br><span class="line">            <span class="keyword">yield</span> <span class="keyword">from</span> super().start_requests()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recognize_captcha</span><span class="params">(self, im)</span>:</span></span><br><span class="line">        print(<span class="string">'-----正在进行验证码识别-----'</span>)</span><br><span class="line">        username = <span class="string">'xxxxxx'</span></span><br><span class="line">        password = <span class="string">'xxxxxx'</span>.encode(<span class="string">'utf-8'</span>)</span><br><span class="line">        password = md5(password).hexdigest()</span><br><span class="line">        soft_id = <span class="string">'898320'</span></span><br><span class="line">        codetype = <span class="string">'1007'</span></span><br><span class="line">        base_params = &#123;</span><br><span class="line">            <span class="string">'user'</span>: username,</span><br><span class="line">            <span class="string">'pass2'</span>: password,</span><br><span class="line">            <span class="string">'softid'</span>: soft_id,</span><br><span class="line">            <span class="string">'codetype'</span>: codetype</span><br><span class="line">        &#125;</span><br><span class="line">        im = open(im, <span class="string">'rb'</span>).read()</span><br><span class="line">        files = &#123;<span class="string">'userfile'</span>: (<span class="string">'ccc.jpg'</span>, im)&#125;</span><br><span class="line">        r = requests.post(<span class="string">'http://upload.chaojiying.net/Upload/Processing.php'</span>, data=params, files=files, headers=self.headers)</span><br><span class="line">        captcha = r.json()[<span class="string">'pic_str'</span>]</span><br><span class="line">        print(<span class="string">'-----验证码识别完毕-----'</span>)</span><br><span class="line">        print(<span class="string">'验证码为：%s'</span> % captcha)</span><br><span class="line">        <span class="keyword">return</span> captcha</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      上一篇文章中使用 Scrapy 模拟登录豆瓣网，并且进行人工识别验证码进行登录，这一篇中我们使用第三方验证码识别平台去自动识别 Scrapy 登录过程中的验证码。
    
    </summary>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Scrapy" scheme="http://yoursite.com/tags/Scrapy/"/>
    
      <category term="验证码" scheme="http://yoursite.com/tags/%E9%AA%8C%E8%AF%81%E7%A0%81/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy模拟登录豆瓣网初级篇</title>
    <link href="http://yoursite.com/scrapy_login/"/>
    <id>http://yoursite.com/scrapy_login/</id>
    <published>2019-01-03T04:47:38.000Z</published>
    <updated>2019-01-05T08:04:00.958Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/scrapy-logo.png" alt=""></p><hr><p>在进行模拟登录之前，应该先对网站登录的原理有所了解，首先在 Chrome 浏览器中进行一次实际的登录操作，再来观察浏览器和网站服务器是如何交互的。</p><p>在这里我使用 <a href="https://www.douban.com/" target="_blank" rel="noopener">豆瓣网</a> 作为此次模拟登录的示例。</p><p>首先打开 F12 开发者模式，在登录表单中输入用户名和密码(这里我的验证码是因为我尝试次数过多出现)，点击登录按钮，观察控制台中 Network 下第一条请求，其为一条 post 请求，且参数在图中有所展示，那么如果需要模拟登录，就需要对这些参数进行构造。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/douban_form.png" alt=""></p><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/douban_form1.png" alt=""></p><p>登录的核心其实就是向服务器发送含有登录表单数据的 HTTP 请求(通常是 POST)，在 Scrapy 中提供了一个 FormRequest 类(Request的子类)，专门用于构造含有登录表单的请求，FormRequest 的构造器方法有一个 formdata 参数，接收字典形式的表单数据。</p><p>在本篇文章中，我先模拟登录到网站后，跳转至个人中心，然后修改我的个人签名。</p><h1 id="模拟登录"><a href="#模拟登录" class="headerlink" title="模拟登录"></a>模拟登录</h1><p>要构造 post 请求的参数，来看上图参数中 source, redir 和 login 都是固定值，form_email, form_password 分别为用户名和密码，captcha-solution 是图片验证码的字符，captcha-id 就先去网页源代码中寻找，如下图。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/captcha-id.png" alt=""></p><p>在这里我将 start_urls 设置为我的个人详情页，模拟登录这里需要重写 start_requests 方法，因为如果不去重写这个方法，那么 scrapy 就会对我的个人详情页直接进行请求。</p><p>FormRequest 的 from_response 方法需传入一个 Response 对象作为第一个参数，该方法会解析 Response 对象所包含的 form 元素，帮助用户创建 FormRequest 对象，并将隐藏 input 中的信息自动填入表单数据。使用这种方法，只需通过 formdata 参数填写账号和密码即可。这里使用 PIL 的 Image 方法将图片展示出来，人工识别并输入到程序中，程序继续进行登录。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模拟登录</span></span><br><span class="line">login_url = <span class="string">'https://accounts.douban.com/login'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">yield</span> Request(self.login_url, callback=self.login)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    print(<span class="string">'-----登录程序-----'</span>)</span><br><span class="line">    captcha_id = response.xpath(<span class="string">".//input[@name='captcha-id']/@value"</span>).get()</span><br><span class="line">    captcha_url = response.xpath(<span class="string">"//*[@id='captcha_image']/@src"</span>).get()</span><br><span class="line">    <span class="keyword">if</span> captcha_url <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        print(<span class="string">'-----登录时无验证码-----'</span>)</span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">'form_email'</span>: <span class="string">'xxxxxx@qq.com'</span>,</span><br><span class="line">            <span class="string">'form_password'</span>: <span class="string">'xxxxxx'</span></span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'-----登录时有验证码-----'</span>)</span><br><span class="line">        print(<span class="string">'-----即将下载验证码-----'</span>)</span><br><span class="line">        <span class="comment"># 使用urllib 的 urlretrieve 直接下载验证码图片到本地</span></span><br><span class="line">        request.urlretrieve(captcha_url, <span class="string">'captcha.png'</span>,)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            image = Image(<span class="string">'captcha.png'</span>)</span><br><span class="line">            image.show()</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        captcha_solution = input(<span class="string">"请输入图片中的验证码"</span>)</span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">'form_email'</span>: <span class="string">'xxxxxx@qq.com'</span>,</span><br><span class="line">            <span class="string">'form_password'</span>: <span class="string">'xxxxxx'</span>,</span><br><span class="line">            <span class="string">'captcha-solution'</span>: captcha_solution,</span><br><span class="line">            <span class="string">'captcha-id'</span>: captcha_id,</span><br><span class="line">            <span class="string">'login'</span>: <span class="string">'登录'</span></span><br><span class="line">        &#125;</span><br><span class="line">    print(<span class="string">'-----登录中-----'</span>)</span><br><span class="line">    <span class="keyword">yield</span> FormRequest.from_response(response, formdata=data, callback=self.parse_after_login)</span><br></pre></td></tr></table></figure><p>在 login 函数中，最后的 FormRequest 的回调函数是 parse_after_login 函数，代码如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_after_login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">"xxxxxxx的帐号"</span> <span class="keyword">in</span> response.text:</span><br><span class="line">        print(<span class="string">"-----登录成功-----"</span>)</span><br></pre></td></tr></table></figure><h1 id="修改签名"><a href="#修改签名" class="headerlink" title="修改签名"></a>修改签名</h1><p>在登录成功之后，需要先跳转到我的个人详情页面，再进行修改签名操作。</p><p>先手动修改签名一次，观察浏览器的请求过程，如图所示。点击修改后，浏览器中这条 POST 请求的 formdata 只有两个参数，一个 signature 就是我们正在修改的签名。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/sign1.png" alt=""></p><p>另一个是 ck 参数，ck 参数在网页源代码中同样可以找到，如下图。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/ck.png" alt=""></p><p>有了修改签名的两个参数，我们就可以构造修改签名的 FormRequest 了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_after_login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">"xxxxxxx的帐号"</span> <span class="keyword">in</span> response.text:</span><br><span class="line">        <span class="keyword">yield</span> <span class="keyword">from</span> super().start_requests()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> response.url == <span class="string">'https://www.douban.com/people/xxxxxxx/'</span>:</span><br><span class="line">        print(<span class="string">'-----已经进入个人详情页-----'</span>)</span><br><span class="line">        print(<span class="string">'-----正在修改个人签名-----'</span>)</span><br><span class="line">        ck = response.xpath(<span class="string">"//*[@id='edit_signature']/form/div/input/@value"</span>).get()</span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">'ck'</span>: ck,</span><br><span class="line">            <span class="string">'signature'</span>: <span class="string">'我是 scrapy 修改的~~'</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">yield</span> FormRequest(self.edit_signature_url, formdata=data)</span><br></pre></td></tr></table></figure><h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> Request, FormRequest</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LoginSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'login'</span></span><br><span class="line">    allowed_domains = [<span class="string">'douban.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'https://www.douban.com/people/xxxxxx/'</span>]</span><br><span class="line">    edit_signature_url = <span class="string">'https://www.douban.com/j/people/xxxxxx/edit_signature'</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'Connection'</span>: <span class="string">'Keep-Alive'</span>,</span><br><span class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)'</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self ,response)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> response.url == <span class="string">'https://www.douban.com/people/xxxxxx/'</span>:</span><br><span class="line">            print(<span class="string">'-----已经进入个人详情页-----'</span>)</span><br><span class="line">            print(<span class="string">'-----正在修改个人签名-----'</span>)</span><br><span class="line">            ck = response.xpath(<span class="string">"//*[@id='edit_signature']/form/div/input/@value"</span>).get()</span><br><span class="line">            data = &#123;</span><br><span class="line">                <span class="string">'ck'</span>: ck,</span><br><span class="line">                <span class="string">'signature'</span>: <span class="string">'我是 scrapy 修改的~~'</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">yield</span> FormRequest(self.edit_signature_url, formdata=data, callback=self.success)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">success</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        print(<span class="string">'-----个人签名修改成功-----'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模拟登录</span></span><br><span class="line">    login_url = <span class="string">'https://accounts.douban.com/login'</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> Request(self.login_url, callback=self.login)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        print(<span class="string">'-----登录程序-----'</span>)</span><br><span class="line">        captcha_id = response.xpath(<span class="string">".//input[@name='captcha-id']/@value"</span>).get()</span><br><span class="line">        captcha_url = response.xpath(<span class="string">"//*[@id='captcha_image']/@src"</span>).get()</span><br><span class="line">        <span class="keyword">if</span> captcha_url <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            print(<span class="string">'-----登录时无验证码-----'</span>)</span><br><span class="line">            data = &#123;</span><br><span class="line">                <span class="string">'form_email'</span>: <span class="string">'xxxxxx@qq.com'</span>,</span><br><span class="line">                <span class="string">'form_password'</span>: <span class="string">'xxxxxx'</span></span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">'-----登录时有验证码-----'</span>)</span><br><span class="line">            print(<span class="string">'-----即将下载验证码-----'</span>)</span><br><span class="line">            request.urlretrieve(captcha_url, <span class="string">'captcha.png'</span>,)</span><br><span class="line">            image = Image.open(<span class="string">'captcha.png'</span>)</span><br><span class="line">            image.show()</span><br><span class="line">            captcha_solution = input(<span class="string">"请输入验证码:"</span>)</span><br><span class="line">            <span class="comment"># captcha_solution = self.recognize_captcha('captcha.png')</span></span><br><span class="line">            data = &#123;</span><br><span class="line">                <span class="string">'form_email'</span>: <span class="string">'xxxxxx@qq.com'</span>,</span><br><span class="line">                <span class="string">'form_password'</span>: <span class="string">'xxxxxx'</span>,</span><br><span class="line">                <span class="string">'captcha-solution'</span>: captcha_solution,</span><br><span class="line">                <span class="string">'captcha-id'</span>: captcha_id,</span><br><span class="line">                <span class="string">'login'</span>: <span class="string">'登录'</span></span><br><span class="line">            &#125;</span><br><span class="line">        print(<span class="string">'-----登录中-----'</span>)</span><br><span class="line">        <span class="keyword">yield</span> FormRequest.from_response(response, formdata=data, callback=self.parse_after_login)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_after_login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">"xxxxxx的帐号"</span> <span class="keyword">in</span> response.text:</span><br><span class="line">            print(<span class="string">'-----登录成功-----'</span>)</span><br><span class="line">            <span class="keyword">yield</span> <span class="keyword">from</span> super().start_requests()</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    <summary type="html">
    
      Scrapy 是一个强大的爬虫框架，某些网站只有在用户登录后才能获取到有价值的信息，因此 Scrapy 框架也有模拟登录的能力。此篇来学习在 Scrapy 中模拟登录的方法。
    
    </summary>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Scrapy" scheme="http://yoursite.com/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Pycharm</title>
    <link href="http://yoursite.com/Pycharm/"/>
    <id>http://yoursite.com/Pycharm/</id>
    <published>2018-11-26T07:23:13.000Z</published>
    <updated>2019-01-03T07:52:24.707Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/pycharm/pycharmlogo.png" alt=""></p><hr><a id="more"></a><h1 id="内存报错-The-IDE-is-running-low-on-memory"><a href="#内存报错-The-IDE-is-running-low-on-memory" class="headerlink" title="内存报错 The IDE is running low on memory"></a>内存报错 The IDE is running low on memory</h1><p>有时候在 mac 和 windows 系统上使用 PyCharm 时，会提醒 The IDE is running low on memory ，然后看了下内存占用，明明内存够，百度后发现是软件本身设置的内存较小。在 mac 系统中，PyCharm 操作如下：</p><p>1.打开 Help -&gt; Find Action</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/pycharm/pyfind.png" alt=""></p><p>2.在搜索框中输入 VM Options , 点击进入第一条结果</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/pycharm/vm_options.jpg" alt=""></p><p>3.将这里的 -Xmx750m 中的 750 改成想设置的内存大小</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/pycharm/vm.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      212
    
    </summary>
    
      <category term="Pycharm" scheme="http://yoursite.com/categories/Pycharm/"/>
    
      <category term="日常" scheme="http://yoursite.com/categories/Pycharm/%E6%97%A5%E5%B8%B8/"/>
    
    
      <category term="Pycharm" scheme="http://yoursite.com/tags/Pycharm/"/>
    
  </entry>
  
  <entry>
    <title>七牛云数据迁移至网易nos</title>
    <link href="http://yoursite.com/qiniu/"/>
    <id>http://yoursite.com/qiniu/</id>
    <published>2018-10-28T05:52:03.000Z</published>
    <updated>2019-01-14T15:37:36.233Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/qiniu/qiniulogo.png" alt=""></p><hr><h1 id="使用-qshell-将-bucket-文件迁移到新-bucket-中"><a href="#使用-qshell-将-bucket-文件迁移到新-bucket-中" class="headerlink" title="使用 qshell 将 bucket 文件迁移到新 bucket 中"></a>使用 qshell 将 bucket 文件迁移到新 bucket 中</h1><h2 id="安装-qshell-工具。下载地址：qshell-官方文档"><a href="#安装-qshell-工具。下载地址：qshell-官方文档" class="headerlink" title="安装 qshell 工具。下载地址：qshell 官方文档"></a>安装 qshell 工具。下载地址：<a href="http://devtools.qiniu.com/qshell-v2.3.3.zip" target="_blank" rel="noopener">qshell</a> <a href="https://developer.qiniu.com/kodo/tools/1302/qshell" target="_blank" rel="noopener">官方文档</a></h2><p>将下载下来的压缩文件解压到任意目录，将其中的名字为 darwin 的文件重命名为 qshell 并放入 mac 的 /usr/local/bin <a href="https://www.jianshu.com/p/9a6dca7ccfa0" target="_blank" rel="noopener">目录</a>，最后在命令行输入 qshell 。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/qiniu/qshell1.png" alt=""></p><h2 id="密钥设置"><a href="#密钥设置" class="headerlink" title="密钥设置"></a>密钥设置</h2><p>这里需要使用七牛账号中<a href="https://portal.qiniu.com/user/key" target="_blank" rel="noopener">个人密钥管理</a>下的 AccessKey 和 SecretKey 。</p><p>找到这两个值后在命令行中输入如下命令。(其中 ak 和 sk 分别对应 AccessKey 和 SecretKey ， name 为账户名称)<br><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ qshell account ak sk <span class="built_in">name</span></span><br></pre></td></tr></table></figure></p><p>如果没有报错的话，输入以下命令来显示账号信息。<br><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$ </span>qshell account</span><br></pre></td></tr></table></figure></p><h2 id="迁移文件"><a href="#迁移文件" class="headerlink" title="迁移文件"></a>迁移文件</h2><p>使用如下命令进行文件迁移并生成包含文件名的文本文件。</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">$</span> qshell listbucket &#123; bucket_name &#125; | awk -F<span class="string">"\t"</span> <span class="string">'&#123;print $1&#125;'</span> &gt; <span class="keyword">files</span>.txt</span><br><span class="line"><span class="symbol">$</span> qshell batchcopy &#123; bucket_name &#125; &#123; new_bucket_name &#125; -i <span class="keyword">files</span>.txt</span><br></pre></td></tr></table></figure><h1 id="批量下载文件到本地"><a href="#批量下载文件到本地" class="headerlink" title="批量下载文件到本地"></a>批量下载文件到本地</h1><p>上面只是将旧桶内的文件转移到新的桶内，原本无法预览、下载的文件现在都可以正常操作了，可是里面有很多图片总不能从网页上一张一张下载下来然后再上传到其他的储存空间里吧，因此这里就需要思考如何将图片批量下载到本地磁盘。七牛云的 qshell 工具有 <a href="https://github.com/qiniu/qshell/blob/master/docs/qdownload.md" target="_blank" rel="noopener"><code>qdownload</code></a> 方法能批量下载文件，但是尝试多次均以失败告终，后看到 qshell 的 <a href="https://github.com/qiniu/qshell/blob/master/docs/get.md" target="_blank" rel="noopener"><code>get</code></a> 方法只能每次操作一个文件，如果让电脑代替人工去重复操作这一 get 方法，就可以把每一张图片都下载下来。</p><p>python 脚本代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> linecache</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> range(<span class="number">1</span>, total_num):</span><br><span class="line">    result = linecache.getline(<span class="string">'./files.txt'</span>, num).strip()</span><br><span class="line">    print(result)</span><br><span class="line">    os.system(<span class="string">'/usr/local/bin/qshell get test '</span> + result)</span><br></pre></td></tr></table></figure></p><h1 id="上传至网易-nos"><a href="#上传至网易-nos" class="headerlink" title="上传至网易 nos"></a>上传至网易 nos</h1><p><a href="https://c.163yun.com/dashboard#/m/nos/" target="_blank" rel="noopener">网易nos</a> 的注册与创建桶的过程不再过多介绍，需要注意的是在创建储存桶完毕之后要进行两个关键的配置。</p><ul><li><p>存储桶（bucket）访问权限</p></li><li><p>防盗链设置</p></li></ul><p>1、访问权限</p><p>访问权限应设置为公有读。如图中解释：</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/qiniu/rwpro.png" alt=""></p><p>2、防盗链设置</p><p>为了保护自己的免费额度，防止图片被他人盗用，因此需要开启防盗链。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/qiniu/fdl.png" alt=""></p><hr>]]></content>
    
    <summary type="html">
    
      今天想去七牛云上传个图片，上传几次都没成功，之前的图片也查看不了，发现页面提示：5402 获取bucket域名失败，才发现七牛云的测试域名被回收了，想继续用需要绑定一个已备案的域名，太麻烦了。主要是之前的图片现在无法预览，也无法下载。不过经实践发现可以进行数据迁移，写此篇做记录。
    
    </summary>
    
      <category term="日常" scheme="http://yoursite.com/categories/%E6%97%A5%E5%B8%B8/"/>
    
    
  </entry>
  
  <entry>
    <title>PHP框架--Laravel</title>
    <link href="http://yoursite.com/laravel_admin/"/>
    <id>http://yoursite.com/laravel_admin/</id>
    <published>2018-10-09T16:10:33.000Z</published>
    <updated>2019-01-14T09:56:20.134Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/laravel-admin/laravel-logo.png" alt=""></p><hr><p>Laravel-Admin 是一个帮我们快速建立后台管理的工具。它提供了页面组件和表单元素等功能，而且还有很多附加功能，同时也支持我们去自定义一些插件，非常方便。</p><p>由于之前没有接触过 PHP这门语言, 所以一开始接手项目时先看了看框架的代码，果然看不懂。回头看了看 PHP 的基础语法，再去简单做了一下 Laravel 的 demo ，开始慢慢理解了某些语法。</p><p>下面是我遇到的几个小问题，记录下来以作参考。</p><hr><h1 id="switch状态值"><a href="#switch状态值" class="headerlink" title="switch状态值"></a><code>switch</code>状态值</h1><p>因为 switch 组件默认存入数据库的状态是[ 开-&gt;1, 关-&gt;0 ]，但是由于 0 会影响数据判断的正确性和安全性，因此需要把 0 和 1 的状态改为 1 和 2，由于需要操作的按钮在表格页，也就是 Grid 页，而 Grid 页的操作都是由 Form 页传递过去的，因此只要找到 Form 中对应的 switch 操作数据库的地方就可以修改入库状态值了。经过一层一层的溯源，先找到 Form.php 文件，查看可使用的操作，发现了 SwitchField 关键词，对其进行查找，找到了 SwitchField.php 文件，其中有一段代码是用来写开关入库的状态值的，对此进行修改后通过测试。</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> $states = [</span><br><span class="line">        <span class="string">'on'</span>  =&gt; [<span class="string">'value'</span> =&gt; <span class="number">1</span>, <span class="string">'text'</span> =&gt; <span class="string">'ON'</span>, <span class="string">'color'</span> =&gt; <span class="string">'primary'</span>],</span><br><span class="line">        <span class="string">'off'</span> =&gt; [<span class="string">'value'</span> =&gt; <span class="number">2</span>, <span class="string">'text'</span> =&gt; <span class="string">'OFF'</span>, <span class="string">'color'</span> =&gt; <span class="string">'default'</span>],</span><br><span class="line">    ];</span><br></pre></td></tr></table></figure><h1 id="switch控制多个页面"><a href="#switch控制多个页面" class="headerlink" title="switch控制多个页面"></a><code>switch</code>控制多个页面</h1><p>有几个相同的表结构，被显示在不同的页面，页面结构完全相同，因此为了节省操作量，需要用一个页面的 swtich 开关去控制多个页面的状态。Google 后发现了<a href="https://github.com/z-song/laravel-admin/issues/375" target="_blank" rel="noopener">解决方案</a>。</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="function"><span class="keyword">function</span> <span class="title">boot</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">parent</span>::boot();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span>::saving(<span class="function"><span class="keyword">function</span> <span class="params">($model)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 从$model取出数据并进行处理</span></span><br><span class="line"></span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    <summary type="html">
    
      在公司工作的时候要用到后台管理工具 Laravel-Admin ，这里简单记录了自己遇到的两个小问题。
    
    </summary>
    
    
      <category term="PHP" scheme="http://yoursite.com/tags/PHP/"/>
    
  </entry>
  
  <entry>
    <title>Python算法--快速排序</title>
    <link href="http://yoursite.com/quick_sort/"/>
    <id>http://yoursite.com/quick_sort/</id>
    <published>2018-05-22T16:29:26.000Z</published>
    <updated>2019-01-03T07:24:21.079Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/quick_sortlogo.jpg" alt=""></p><hr><p>快速排序（Quicksort），又称划分交换排序（partition-exchange sort），通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。</p><p>步骤为：</p><p>1、从数列中挑出一个元素，称为”基准”（pivot）</p><p>2、重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作</p><p>3、递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序</p><p>递归的最底部情形，是数列的大小是零或一，也就是永远都已经被排序好了。虽然一直递归下去，但是这个算法总会结束，因为在每次的迭代（iteration）中，它至少会把一个元素摆到它最后的位置去。</p><hr><h1 id="过程分析"><a href="#过程分析" class="headerlink" title="过程分析"></a>过程分析</h1><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/quicksort.jpg" alt=""></p><h1 id="快速排序演示"><a href="#快速排序演示" class="headerlink" title="快速排序演示"></a>快速排序演示</h1><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/quicksort.gif" alt=""></p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort</span><span class="params">(alist, start, end)</span>:</span></span><br><span class="line">    <span class="string">'''快速排序'''</span></span><br><span class="line">    <span class="keyword">if</span> start &gt;= end:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    mid = alist[start]</span><br><span class="line">    low = start</span><br><span class="line">    high = end</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> low &lt; high:</span><br><span class="line">        <span class="keyword">while</span> low &lt; high <span class="keyword">and</span> alist[high] &gt;= mid:</span><br><span class="line">            high -= <span class="number">1</span></span><br><span class="line">        alist[low] = alist[high]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> low &lt; high <span class="keyword">and</span> alist[low] &lt; mid:</span><br><span class="line">            low += <span class="number">1</span></span><br><span class="line">        alist[high] = alist[low]</span><br><span class="line"></span><br><span class="line">    alist[low] = mid</span><br><span class="line"></span><br><span class="line">    quick_sort(alist, start, low<span class="number">-1</span>)</span><br><span class="line">    quick_sort(alist, low+<span class="number">1</span>, end)</span><br><span class="line">    <span class="keyword">return</span> alist</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    alist = [<span class="number">26</span>, <span class="number">54</span>, <span class="number">15</span>, <span class="number">57</span>, <span class="number">6</span>]</span><br><span class="line">    print(alist)</span><br><span class="line">    print(quick_sort(alist, <span class="number">0</span>, len(alist)<span class="number">-1</span>))</span><br></pre></td></tr></table></figure><h1 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h1><blockquote><p>最优时间复杂度：O(nlogn)</p></blockquote><blockquote><p>最坏时间复杂度：O(n2)</p></blockquote><blockquote><p>稳定性：不稳定</p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      快速排序（Quicksort），又称划分交换排序（partition-exchange sort），通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。
    
    </summary>
    
    
      <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Python算法--插入排序</title>
    <link href="http://yoursite.com/insert_sort/"/>
    <id>http://yoursite.com/insert_sort/</id>
    <published>2018-05-22T00:07:20.000Z</published>
    <updated>2019-01-03T07:15:23.268Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/insert_sortlogo.jpg" alt=""></p><hr><p>插入排序（英语：Insertion Sort）是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。插入排序在实现上，在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。</p><hr><h1 id="过程分析"><a href="#过程分析" class="headerlink" title="过程分析"></a>过程分析</h1><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/insert.png" alt=""></p><blockquote><p> <img src="https://hexo-pics.nos-eastchina1.126.net/insert-sort/Insertion-sort-example.gif" alt=""></p></blockquote><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_sort</span><span class="params">(alist)</span>:</span></span><br><span class="line">    n = len(alist)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> range(i, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">if</span> alist[j] &lt; alist[j<span class="number">-1</span>]:</span><br><span class="line">          alist[j], alist[j<span class="number">-1</span>] = alist[j<span class="number">-1</span>], alist[j]</span><br><span class="line">    <span class="keyword">return</span> alist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    alist = [<span class="number">26</span>, <span class="number">54</span>, <span class="number">15</span>, <span class="number">57</span>, <span class="number">6</span>]</span><br><span class="line">    print(alist)</span><br><span class="line">    print(insert_sort(alist))</span><br></pre></td></tr></table></figure><h1 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h1><blockquote><p>最优时间复杂度：O(n) （升序排列，序列已经处于升序状态）</p></blockquote><blockquote><p>最坏时间复杂度：O(n2)</p></blockquote><blockquote><p>稳定性：稳定</p></blockquote><h1 id="插入排序演示"><a href="#插入排序演示" class="headerlink" title="插入排序演示"></a>插入排序演示</h1><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/insert.gif" alt=""></p><hr>]]></content>
    
    <summary type="html">
    
      插入排序（英语：Insertion Sort）是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。插入排序在实现上，在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Python算法--选择排序</title>
    <link href="http://yoursite.com/select_sort/"/>
    <id>http://yoursite.com/select_sort/</id>
    <published>2018-05-21T21:58:47.000Z</published>
    <updated>2019-01-15T02:45:15.133Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/select_sortlogo.jpg" alt=""></p><hr><p>选择排序（ Selection sort ）是一种简单直观的排序算法。它的工作原理如下。首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。</p><p>选择排序的主要优点与数据移动有关。如果某个元素位于正确的最终位置上，则它不会被移动。选择排序每次交换一对元素，它们当中至少有一个将被移到其最终位置上，因此对n个元素的表进行排序总共进行至多n-1次交换。在所有的完全依靠交换去移动元素的排序方法中，选择排序属于非常好的一种。</p><hr><h1 id="过程分析"><a href="#过程分析" class="headerlink" title="过程分析"></a>过程分析</h1><blockquote><blockquote><blockquote><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/selectionsort.jpg" alt=""></p></blockquote></blockquote></blockquote><h1 id="选择排序演示"><a href="#选择排序演示" class="headerlink" title="选择排序演示"></a>选择排序演示</h1><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/selection.gif" alt=""></p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_sort</span><span class="params">(alist)</span>:</span></span><br><span class="line">    n = len(alist)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-1</span>):</span><br><span class="line">        min_index = i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, n):</span><br><span class="line">          <span class="keyword">if</span> alist[j] &lt; alist[min_index]:</span><br><span class="line">              min_index = j</span><br><span class="line">        alist[i], alist[min_index] = alist[min_index], alist[i]</span><br><span class="line">    <span class="keyword">return</span> alist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    alist = [<span class="number">26</span>, <span class="number">54</span>, <span class="number">15</span>, <span class="number">57</span>, <span class="number">6</span>]</span><br><span class="line">    print(alist)</span><br><span class="line">    print(selection_sort(alist))</span><br></pre></td></tr></table></figure><h1 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h1><blockquote><p>最优时间复杂度：O(n2)</p></blockquote><blockquote><p>最坏时间复杂度：O(n2)</p></blockquote><blockquote><p>稳定性：不稳定</p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      选择排序（ Selection sort ）是一种简单直观的排序算法。它的工作原理如下。首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。
    
    </summary>
    
      <category term="算法" scheme="http://yoursite.com/categories/%E7%AE%97%E6%B3%95/"/>
    
      <category term="Python" scheme="http://yoursite.com/categories/%E7%AE%97%E6%B3%95/Python/"/>
    
    
      <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>冒泡排序</title>
    <link href="http://yoursite.com/bubble_sort/"/>
    <id>http://yoursite.com/bubble_sort/</id>
    <published>2018-05-21T16:25:42.000Z</published>
    <updated>2019-01-03T07:27:29.359Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/bubble_sortlogo.jpg" alt=""></p><hr><p>冒泡排序（Bubble Sort）是一种简单的排序算法。它重复地遍历要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。遍历数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。</p><p>冒泡排序算法的运作如下：</p><blockquote><p>比较相邻的元素。如果第一个比第二个大（升序），就交换他们两个。</p></blockquote><blockquote><p>对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。</p></blockquote><blockquote><p>针对所有的元素重复以上的步骤，除了最后一个。</p></blockquote><blockquote><p>持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。</p></blockquote><hr><h1 id="过程分析"><a href="#过程分析" class="headerlink" title="过程分析"></a>过程分析</h1><p>交换过程图示(第一次)：</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/bubblesort.jpg" alt=""></p><p>那么我们需要进行 n-1 次冒泡过程，每次对应的比较次数如下图所示：</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/compare.bmp" alt=""></p><h1 id="冒泡排序演示"><a href="#冒泡排序演示" class="headerlink" title="冒泡排序演示"></a>冒泡排序演示</h1><p><img src="https://hexo-pics.nos-eastchina1.126.net/sort/bubble.gif" alt=""></p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bubble_sort</span><span class="params">(alist)</span>:</span></span><br><span class="line">    n = len(alist)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n<span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-1</span>-j):</span><br><span class="line">            <span class="keyword">if</span> alist[i] &gt; alist[i+<span class="number">1</span>]:</span><br><span class="line">                alist[i], alist[i+<span class="number">1</span>] = alist[i+<span class="number">1</span>], alist[i]</span><br><span class="line">    <span class="keyword">return</span> alist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    alist = [<span class="number">26</span>, <span class="number">54</span>, <span class="number">15</span>, <span class="number">57</span>, <span class="number">6</span>]</span><br><span class="line">    print(alist)</span><br><span class="line">    print(bubble_sort(alist))</span><br></pre></td></tr></table></figure><h1 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h1><blockquote><p>最优时间复杂度：O(n) （表示遍历一次发现没有任何可以交换的元素，排序结束。）</p></blockquote><blockquote><p>最坏时间复杂度：O(n2)</p></blockquote><blockquote><p>稳定性：稳定</p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      冒泡排序（Bubble Sort）是一种简单的排序算法。它重复地遍历要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。遍历数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。
    
    </summary>
    
    
      <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫--Scrapy爬取简书全站文章</title>
    <link href="http://yoursite.com/jianshu/"/>
    <id>http://yoursite.com/jianshu/</id>
    <published>2018-05-18T07:25:17.000Z</published>
    <updated>2019-01-15T02:50:35.767Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/jianshu/jianshu-logo.png" alt=""></p><hr><p>最近学习了 scrapy ，之前刚开始爬虫的时候有接触过这个框架，当时看了下工作原理有点难懂，现在慢慢地接触爬虫多了，回过头来开始了解爬虫框架，现在再来看它的工作流程就明白了很多。</p><p>本篇文章使用 scrapy 来爬取<a href="https://www.jianshu.com/" target="_blank" rel="noopener">简书</a>全站文章。</p><p>scrapy 工程创建与配置步骤(个人习惯)：</p><ol><li>创建 scrapy 工程，创建启动文件 start.py ，修改 settings.py 配置文件</li><li>进入 spider.py 文件开始写爬虫规则</li><li>item.py 中设置存储模板</li><li>写 pipeline 存入数据库</li></ol><hr><h1 id="创建-scrapy-工程"><a href="#创建-scrapy-工程" class="headerlink" title="创建 scrapy 工程"></a>创建 scrapy 工程</h1><p>windows 系统在 scrapy 工程文件根目录：打开命令行工具，输入命令创建工程。<br><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy startproject <span class="string">[工程名字]</span></span><br></pre></td></tr></table></figure></p><p>cd 到工程文件夹下，创建爬虫文件，默认使用 basic 模板，同样在命令行中。输入<br><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy genspider <span class="string">[爬虫名字]</span> <span class="string">[爬虫网址]</span></span><br></pre></td></tr></table></figure></p><p>这样就完成了一个 scrapy 工程的创建。但是这里爬取简书全站我使用的是  crawlspider 爬虫，其有可编写的爬虫规则，使用起来比较方便。</p><p>为了方便启动工程，我都会在创建好 scrapy 后再来创建一个启动文件 start.py 。</p><p>修改 settings.py 文件，将其中的遵守 robots.txt 协议关闭，开启 headers 其它配置等需要的时候再去更改。</p><hr><h1 id="进入-spider-py-写爬虫规则"><a href="#进入-spider-py-写爬虫规则" class="headerlink" title="进入 spider.py 写爬虫规则"></a>进入 spider.py 写爬虫规则</h1><p>这次爬取的是简书全站的文章，因此要找所有文章的链接规则，每篇文章阅读到最底部，简书会推荐给我们一些其它文章，几乎每篇文章下面都会有推荐，因此我们从这里入手，查看了源代码，发现了它们的链接形式都大致相同，<a href="https://www.jianshu.com/p/7a4879ef6f8d" target="_blank" rel="noopener">https://www.jianshu.com/p/7a4879ef6f8d</a> ，<a href="https://www.jianshu.com/p/cde1742518c8" target="_blank" rel="noopener">https://www.jianshu.com/p/cde1742518c8</a> ，如图。    </p><p><img src="https://hexo-pics.nos-eastchina1.126.net/jianshu/jianshu3.jpg" alt=""></p><p>可以看到，都是 p 后面接上一大串数字字母的混合字符串，因此可以写出它的规则，使用正则表达式，如下。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r'.+/p/[a-z0-9].+'</span>), callback=<span class="string">'parse_detail'</span>, follow=<span class="keyword">True</span>),</span><br><span class="line">    )</span><br></pre></td></tr></table></figure></p><p>rules 是一个元组，其中，Rule 写的是爬虫的规则；callback 指的是回调函数，也就是当获取到了前面取到的 url 之后，程序该去调用哪一个函数的操作，而这里就是去调用 parse_detail 这个函数； follow 表示跟进，如果其 ==True 表示要继续跟进，也就是我们进入一片文章之后，要继续跟进下一篇文章。</p><p>在进入一篇文章之后，我们要获取到它的标题，发布者，发布时间，还有内容这四个部分。这里使用 xpath 方法来获取。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/jianshu/jianshu1.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    title = response.xpath(<span class="string">"/html/body/div[1]/div[1]/div[1]/h1/text()"</span>).get()</span><br><span class="line">    pub_name = response.xpath(<span class="string">"/html/body/div[1]/div[1]/div[1]/div[1]/div/span/a/text()"</span>).get()</span><br><span class="line">    release_time = response.xpath(<span class="string">"/html/body/div[1]/div[1]/div[1]/div[1]/div/div/span[1]/text()"</span>).get()</span><br><span class="line">    content = response.xpath(<span class="string">"/html/body/div[1]/div[1]/div[1]/div[2]/div"</span>).get()</span><br></pre></td></tr></table></figure><hr><h1 id="items-py-中设置存储模板"><a href="#items-py-中设置存储模板" class="headerlink" title="items.py 中设置存储模板"></a>items.py 中设置存储模板</h1><p>在上面已经决定了要采集者四个信息，那么在 item.py 中设置好这四项。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JianshuItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    pub_name = scrapy.Field()</span><br><span class="line">    release_time = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br></pre></td></tr></table></figure></p><p>最后在 parse_detail 尾部加入以下代码，再将 item 返回去。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">item = JianshuItem(title=title, pub_name=pub_name, release_time=release_time, content=content)</span><br><span class="line"><span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure></p><hr><h1 id="pipelines-py"><a href="#pipelines-py" class="headerlink" title="pipelines.py"></a>pipelines.py</h1><p>到这里就可以运行一下程序了，看一下是否能正常输出我们采集的信息。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/jianshu/jianshu2.jpg" alt=""></p><p>可以看到这里可以正常爬取数据，接下来需要将其存入数据库。存入数据库需要在 pipelines.py 中编写相应的代码。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JianshuMongoDBPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.DB_URI = <span class="string">'localhost'</span></span><br><span class="line">        self.DB_PORT = <span class="number">27017</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.DB_URI, self.DB_PORT)</span><br><span class="line">        self.db = self.client[<span class="string">'jianshu'</span>]</span><br><span class="line">        self.collection = self.db[<span class="string">'jianshu_spider'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spdier)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">if</span> self.collection.insert(dict(item)):</span><br><span class="line">                print(<span class="string">'保存至MongoDB成功'</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">'保存至MongoDB失败！'</span>)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> error:</span><br><span class="line">            print(error)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure></p><p>在 pipelines.py 中写好 MongoDB 部分后，在 settings.py 中将对应的 pipelines 打开。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/jianshu/jianshusettings.jpg" alt=""></p><p>然后重新运行 start.py ，启动爬虫。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/jianshu/jianshumongo.jpg" alt=""></p><p>启动爬虫后，一直没有遇到反爬措施，运行了大概30分钟， ROBO 3T 管理工具得到的数据有3300条。<del>感觉速度还是慢，有待优化</del>(突然发现在 settings.py 中设置了1s延时….关掉之后快多了。。 )</p><hr>]]></content>
    
    <summary type="html">
    
      最近学习了 scrapy ，之前刚开始爬虫的时候有接触过这个框架，当时看了下工作原理有点难懂，现在慢慢地接触爬虫多了，回过头来开始了解爬虫框架，现在再来看它的工作流程就明白了很多。
    
    </summary>
    
    
      <category term="Scrapy" scheme="http://yoursite.com/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy</title>
    <link href="http://yoursite.com/scrapy/"/>
    <id>http://yoursite.com/scrapy/</id>
    <published>2018-05-10T07:54:09.000Z</published>
    <updated>2019-01-03T07:48:06.672Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/scrapy-logo.png" alt=""></p><hr><p>Scrapy 是一个为了爬取网站数据，提取结构性数据而编写的应用框架。可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。</p><p>其最初是为了<a href="http://en.wikipedia.org/wiki/Web_scraping" target="_blank" rel="noopener">网络抓取</a>所设计的，也可以应用在获取 API 所返回的数据或者通用的网络爬虫。</p><p>Scrapy 是使用 Python 语言(基于 Twisted 框架)编写的开源网络爬虫框架。其简单易用、灵活易扩展、开发社区活跃，并且是跨平台的(支持Linux, MacOS, Windows)。</p><p>安装方式：pip 安装<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip <span class="keyword">install</span> scrapy</span><br></pre></td></tr></table></figure></p><p>为了确认 Scrapy 被成功安装，可尝试在 python 命令行中将 scrapy 导入，如未报错，则安装成功。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; import scrapy</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; scrapy.version_info</span><br></pre></td></tr></table></figure><hr><h1 id="scrapy-框架结构"><a href="#scrapy-框架结构" class="headerlink" title="scrapy 框架结构"></a>scrapy 框架结构</h1><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/scrapyfluent.jpg" alt=""></p><p><code>ENGINE</code> 引擎，框架的核心，用于协调其它组件间工作</p><p><code>SCHEDULER</code> 调度器，负责对 SPIDERS 提交的请求进行调度</p><p><code>DOWNLOADER</code> 下载器，负责下载页面</p><p><code>SPIDERS</code> 爬虫，负责提取页面中的数据，并产生对新页面的下载请求</p><p><code>MIDDLEWARES</code> 中间件，负责对 Request 对象和 Response 对象进行处理</p><p><code>ITEM PIPELINE</code> 数据管道，负责对爬取到的数据进行处理</p><h1 id="scrapy-工作流程"><a href="#scrapy-工作流程" class="headerlink" title="scrapy 工作流程"></a>scrapy 工作流程</h1><p>1、引擎 ENGINE 从调度器中取出一个链接 URL 用于接下来的抓取</p><p>2、引擎 ENGINE 把 URL 封装成一个请求 Request 传给下载器 Downloader</p><p>3、下载器 Downloader 把资源下载下来，并封装成 Response</p><p>4、爬虫解析 Response</p><p>5、若解析出实体 Item，交给数据管道 Item Pipeline 做进一步处理</p><p>6、若解析出链接 URL，就把 URL 交给调度器等待抓取</p><h1 id="scrapy-简单爬虫示例"><a href="#scrapy-简单爬虫示例" class="headerlink" title="scrapy 简单爬虫示例"></a>scrapy 简单爬虫示例</h1><p>专供爬虫初学者训练爬虫技术的网站 <a href="http://books.toscrape.com" target="_blank" rel="noopener">http://books.toscrape.com</a> ，从这里开始。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/scrapydemo.jpg" alt=""></p><h2 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h2><p>首先，来创建一个 scrapy 项目，在新建好的 scrapy_demo 文件夹下打开命令行工具，输入如下命令,在这里我创建好了名为 demo 的工程文件夹。<br><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$ </span>scrapy startproject projectName</span><br></pre></td></tr></table></figure></p><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/scrapynew.jpg" alt=""></p><p>接下来再切换到项目文件夹根目录下，使用命令创建爬虫文件。可以看到提示已经使用 basic 模板创建了一个名为 books 的爬虫文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> projectName</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> scrapy genspider spiderName[爬虫名称] url[爬虫网址]</span></span><br></pre></td></tr></table></figure><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/scrapy_book.jpg" alt=""></p><p>在此之后，使用 Pycharm 将整个 scrapy 项目导入。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/scrapy/scrapypycharm.jpg" alt=""></p><p><code>scrapy.cfg</code> 项目的配置信息，主要为Scrapy命令行工具提供一个基础的配置信息。（真正爬虫相关的配置信息在settings.py文件中）</p><p><code>items.py</code> 设置数据存储模板，用于结构化数据，如：Django的Model</p><p><code>pipelines.py</code> 数据处理行为，如：一般结构化的数据持久化</p><p><code>settings.py</code> 配置文件，如：递归的层数、并发数，延迟下载等</p><p><code>spiders</code> 爬虫目录，如：创建文件，编写爬虫规则</p><h2 id="设置数据存储模板"><a href="#设置数据存储模板" class="headerlink" title="设置数据存储模板"></a>设置数据存储模板</h2><p>要爬取的是各个图书的标题，价格信息。需要在 items.py 中设置数据存储模板。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DemoItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    price = scrapy.Field()</span><br></pre></td></tr></table></figure><h2 id="编写爬虫"><a href="#编写爬虫" class="headerlink" title="编写爬虫"></a>编写爬虫</h2><p>爬虫规则在 books.py 中编写。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> demo.items <span class="keyword">import</span> DemoItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'books'</span></span><br><span class="line">    allowed_domains = [<span class="string">'toscrape.com'</span>]</span><br><span class="line">    <span class="comment"># 爬虫起始页</span></span><br><span class="line">    start_urls = [<span class="string">'http://books.toscrape.com/catalogue/page-1.html'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        items = response.xpath(<span class="string">"//article[@class='product_pod']"</span>)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">            title = item.xpath(<span class="string">"./h3/a/text()"</span>).get()</span><br><span class="line">            price = item.xpath(<span class="string">"./div[2]/p/text()"</span>).get()</span><br><span class="line">            item = DemoItem(title=title, price=price)</span><br><span class="line">            <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure></p><p>在运行爬虫之前，先在 settings.py 中设置好相关项，把 ROBOTSTXT_OBEY 置为 False ，并且打开浏览器模拟。</p><p>一切都设置好后，就可以运行爬虫了。为了方便运行，我习惯在项目根目录下新建一个 start.py 用来启动项目，其中代码如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> cmdline</span><br><span class="line"></span><br><span class="line">cmdline.execute(<span class="string">'scrapy crawl books'</span>.split())</span><br></pre></td></tr></table></figure><p>直接运行 start.py，得到如下结果。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">36</span> [scrapy.utils.log] INFO: Scrapy <span class="number">1.5</span><span class="number">.0</span> started (bot: demo)</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">36</span> [scrapy.utils.log] INFO: Versions: lxml <span class="number">4.2</span><span class="number">.1</span><span class="number">.0</span>, libxml2 <span class="number">2.9</span><span class="number">.8</span>, cssselect <span class="number">1.0</span><span class="number">.3</span>, parsel <span class="number">1.4</span><span class="number">.0</span>, w3lib <span class="number">1.19</span><span class="number">.0</span>, Twisted <span class="number">17.5</span><span class="number">.0</span>, Python <span class="number">3.6</span><span class="number">.5</span> |Anaconda, Inc.| (default, Mar <span class="number">29</span> <span class="number">2018</span>, <span class="number">13</span>:<span class="number">32</span>:<span class="number">41</span>) [MSC v<span class="number">.1900</span> <span class="number">64</span> bit (AMD64)], pyOpenSSL <span class="number">18.0</span><span class="number">.0</span> (OpenSSL <span class="number">1.0</span><span class="number">.2</span>o  <span class="number">27</span> Mar <span class="number">2018</span>), cryptography <span class="number">2.2</span><span class="number">.2</span>, Platform Windows<span class="number">-7</span><span class="number">-6.1</span><span class="number">.7601</span>-SP1</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">36</span> [scrapy.crawler] INFO: Overridden settings: &#123;<span class="string">'BOT_NAME'</span>: <span class="string">'demo'</span>, <span class="string">'NEWSPIDER_MODULE'</span>: <span class="string">'demo.spiders'</span>, <span class="string">'SPIDER_MODULES'</span>: [<span class="string">'demo.spiders'</span>]&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">36</span> [scrapy.middleware] INFO: Enabled extensions:</span><br><span class="line">[<span class="string">'scrapy.extensions.corestats.CoreStats'</span>,</span><br><span class="line"> <span class="string">'scrapy.extensions.telnet.TelnetConsole'</span>,</span><br><span class="line"> <span class="string">'scrapy.extensions.logstats.LogStats'</span>]</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.middleware] INFO: Enabled downloader middlewares:</span><br><span class="line">[<span class="string">'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.retry.RetryMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.cookies.CookiesMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.stats.DownloaderStats'</span>]</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.middleware] INFO: Enabled spider middlewares:</span><br><span class="line">[<span class="string">'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.offsite.OffsiteMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.referer.RefererMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.depth.DepthMiddleware'</span>]</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.middleware] INFO: Enabled item pipelines:</span><br><span class="line">[]</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.engine] INFO: Spider opened</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.extensions.logstats] INFO: Crawled <span class="number">0</span> pages (at <span class="number">0</span> pages/min), scraped <span class="number">0</span> items (at <span class="number">0</span> items/min)</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [py.warnings] WARNING: C:\ProgramData\Anaconda3\lib\site-packages\scrapy\spidermiddlewares\offsite.py:<span class="number">59</span>: URLWarning: allowed_domains accepts only domains, <span class="keyword">not</span> URLs. Ignoring URL entry http://toscrape.com/ <span class="keyword">in</span> allowed_domains.</span><br><span class="line">  warnings.warn(<span class="string">"allowed_domains accepts only domains, not URLs. Ignoring URL entry %s in allowed_domains."</span> % domain, URLWarning)</span><br><span class="line"></span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.extensions.telnet] DEBUG: Telnet console listening on <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">6023</span></span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.engine] DEBUG: Crawled (<span class="number">200</span>) &lt;GET http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt; (referer: <span class="keyword">None</span>)</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£51.77'</span>, <span class="string">'title'</span>: <span class="string">'A Light in the ...'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£53.74'</span>, <span class="string">'title'</span>: <span class="string">'Tipping the Velvet'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£50.10'</span>, <span class="string">'title'</span>: <span class="string">'Soumission'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£47.82'</span>, <span class="string">'title'</span>: <span class="string">'Sharp Objects'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£54.23'</span>, <span class="string">'title'</span>: <span class="string">'Sapiens: A Brief History ...'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£22.65'</span>, <span class="string">'title'</span>: <span class="string">'The Requiem Red'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£33.34'</span>, <span class="string">'title'</span>: <span class="string">'The Dirty Little Secrets ...'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£17.93'</span>, <span class="string">'title'</span>: <span class="string">'The Coming Woman: A ...'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£22.60'</span>, <span class="string">'title'</span>: <span class="string">'The Boys in the ...'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£52.15'</span>, <span class="string">'title'</span>: <span class="string">'The Black Maria'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£13.99'</span>, <span class="string">'title'</span>: <span class="string">'Starving Hearts (Triangular Trade ...'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£20.66'</span>, <span class="string">'title'</span>: <span class="string">"Shakespeare's Sonnets"</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£17.46'</span>, <span class="string">'title'</span>: <span class="string">'Set Me Free'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£52.29'</span>, <span class="string">'title'</span>: <span class="string">"Scott Pilgrim's Precious Little ..."</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£35.02'</span>, <span class="string">'title'</span>: <span class="string">'Rip it Up and ...'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£57.25'</span>, <span class="string">'title'</span>: <span class="string">'Our Band Could Be ...'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£23.88'</span>, <span class="string">'title'</span>: <span class="string">'Olio'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£37.59'</span>, <span class="string">'title'</span>: <span class="string">'Mesaerion: The Best Science ...'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£51.33'</span>, <span class="string">'title'</span>: <span class="string">'Libertarianism for Beginners'</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.scraper] DEBUG: Scraped <span class="keyword">from</span> &lt;<span class="number">200</span> http://books.toscrape.com/catalogue/page<span class="number">-1.</span>html&gt;</span><br><span class="line">&#123;<span class="string">'price'</span>: <span class="string">'£45.17'</span>, <span class="string">'title'</span>: <span class="string">"It's Only the Himalayas"</span>&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.engine] INFO: Closing spider (finished)</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.statscollectors] INFO: Dumping Scrapy stats:</span><br><span class="line">&#123;<span class="string">'downloader/request_bytes'</span>: <span class="number">315</span>,</span><br><span class="line"> <span class="string">'downloader/request_count'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'downloader/request_method_count/GET'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'downloader/response_bytes'</span>: <span class="number">5889</span>,</span><br><span class="line"> <span class="string">'downloader/response_count'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'downloader/response_status_count/200'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'finish_reason'</span>: <span class="string">'finished'</span>,</span><br><span class="line"> <span class="string">'finish_time'</span>: datetime.datetime(<span class="number">2018</span>, <span class="number">8</span>, <span class="number">20</span>, <span class="number">9</span>, <span class="number">30</span>, <span class="number">37</span>, <span class="number">857663</span>),</span><br><span class="line"> <span class="string">'item_scraped_count'</span>: <span class="number">20</span>,</span><br><span class="line"> <span class="string">'log_count/DEBUG'</span>: <span class="number">22</span>,</span><br><span class="line"> <span class="string">'log_count/INFO'</span>: <span class="number">7</span>,</span><br><span class="line"> <span class="string">'log_count/WARNING'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'response_received_count'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'scheduler/dequeued'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'scheduler/dequeued/memory'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'scheduler/enqueued'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'scheduler/enqueued/memory'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'start_time'</span>: datetime.datetime(<span class="number">2018</span>, <span class="number">8</span>, <span class="number">20</span>, <span class="number">9</span>, <span class="number">30</span>, <span class="number">37</span>, <span class="number">96620</span>)&#125;</span><br><span class="line"><span class="number">2018</span><span class="number">-08</span><span class="number">-20</span> <span class="number">17</span>:<span class="number">30</span>:<span class="number">37</span> [scrapy.core.engine] INFO: Spider closed (finished)</span><br></pre></td></tr></table></figure></p><p>可以看到，scrapy 在经过初始化之后开始爬虫，并且输出了所需的价格和标题信息。</p><h2 id="保存数据"><a href="#保存数据" class="headerlink" title="保存数据"></a>保存数据</h2><p>暂不做介绍。</p><hr>]]></content>
    
    <summary type="html">
    
      Scrapy 是一个为了爬取网站数据，提取结构性数据而编写的应用框架。可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。
    
    </summary>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Scrapy" scheme="http://yoursite.com/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Github模拟登陆</title>
    <link href="http://yoursite.com/github/"/>
    <id>http://yoursite.com/github/</id>
    <published>2018-05-01T09:20:41.000Z</published>
    <updated>2019-01-03T10:39:56.290Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/github/github-logo.png" alt=""></p><hr><p>github 的登录页面没有验证码，而且比较简单，所以先拿 github 练练手。</p><p>按照惯例，先规划一下步骤：</p><p>1.进入登录页，F12 查看请求，查看必要参数。</p><p>2.构造登录函数。</p><p>3.爬虫成功登陆后采集信息。</p><hr><h1 id="进入登录页真实登录"><a href="#进入登录页真实登录" class="headerlink" title="进入登录页真实登录"></a>进入登录页真实登录</h1><p>登录页网址: <a href="https://github.com/login" target="_blank" rel="noopener">https://github.com/login</a></p><p>页面如下：</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/github/github1.jpg" alt=""></p><p>现在先输入正确的账号密码进行登录。</p><p>打开 F12 开发工具，打开 Network 标签，先输入正确的账号密码，点击 <code>Sign in</code>，然后观察请求。可以看到，第一条请求是 <a href="https://github.com/session" target="_blank" rel="noopener">https://github.com/session</a> , 请求方式是 post 方式，而且 Form Data 里面存在账号密码信息，ok，就是这个请求，如下图。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/github/github2.jpg" alt=""></p><p>能看到请求方式为 post 然后看下 Form Data ，其中有这么几个参数，<code>commit</code>，<code>utf8</code>，<code>authenticity_token</code>，<code>login</code>，<code>password</code>，经过观察，<code>commit</code>，<code>utf8</code>，<code>login</code>，<code>password</code> 这几个都是固定值，而 <code>authenticity_token</code> 每次登录都会发生改变，先找到这个参数是怎么出来的，才方便构造登录函数，经查找后发现在登录页，也就是 <code>https://github.com/login</code> 这个页面，再查看源代码可以查找到这个 <code>token</code> 值，如下图所示。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/github/github3.jpg" alt=""></p><h1 id="构造登录函数"><a href="#构造登录函数" class="headerlink" title="构造登录函数"></a>构造登录函数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">github_login</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Mobile Safari/537.36'</span>,</span><br><span class="line">        &#125;</span><br><span class="line">        self.login_url = <span class="string">'https://github.com/login'</span></span><br><span class="line">        self.post_url = <span class="string">'https://github.com/session'</span></span><br><span class="line">        self.logined_url = <span class="string">'https://github.com/settings/profile'</span></span><br><span class="line">        <span class="comment"># 为了保持会话，需要用到 requests 的 session 服务</span></span><br><span class="line">        self.session = requests.Session()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取token值</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_token</span><span class="params">(self)</span>:</span></span><br><span class="line">        html = self.session.get(self.login_url, headers=self.headers)</span><br><span class="line">        response = etree.HTML(html.text)</span><br><span class="line">        token = response.xpath(<span class="string">"//input[@name='authenticity_token']/@value"</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> token</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 上面已经获取到了token参数，那么开始写登录函数</span></span><br><span class="line">    <span class="comment"># 把username和password作为变量，后期传入</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self, token, email, password)</span>:</span></span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">'commit'</span>: <span class="string">'Sign in'</span>,</span><br><span class="line">            <span class="string">'utf8'</span>: <span class="string">'✓'</span>,</span><br><span class="line">            <span class="string">'authenticity_token'</span>: token,</span><br><span class="line">            <span class="string">'login'</span>: email,</span><br><span class="line">            <span class="string">'password'</span>: password</span><br><span class="line">        &#125;</span><br><span class="line">        response = self.session.post(self.post_url, data=data, headers=self.headers)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            print(<span class="string">'登录成功，正在跳转到个人信息...'</span>)</span><br></pre></td></tr></table></figure><h1 id="采集信息"><a href="#采集信息" class="headerlink" title="采集信息"></a>采集信息</h1><p>上面得到 <code>token</code> 值，也成功登录了 github 。现在我们就可以开始采集信息了。在这里我选择进入个人详情页来获取用户名和地址信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, html)</span>:</span></span><br><span class="line">    response = etree.HTML(html)</span><br><span class="line">    name = response.xpath(<span class="string">"//dl[@class='form-group'][1]/dd[1]/input/@value"</span>)[<span class="number">0</span>]</span><br><span class="line">    location = response.xpath(<span class="string">"//dl[@class='form-group'][6]/dd[1]/input/@value"</span>)[<span class="number">0</span>]</span><br><span class="line">    print(<span class="string">'姓名: %s, 地址: %s'</span> % (name, location))</span><br></pre></td></tr></table></figure><p>获取到用户名和地址信息。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/github/github_info.png" alt=""></p><h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">github_login</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Mobile Safari/537.36'</span>,</span><br><span class="line">        &#125;</span><br><span class="line">        self.login_url = <span class="string">'https://github.com/login'</span></span><br><span class="line">        self.post_url = <span class="string">'https://github.com/session'</span></span><br><span class="line">        self.logined_url = <span class="string">'https://github.com/settings/profile'</span></span><br><span class="line">        self.session = requests.Session()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_token</span><span class="params">(self)</span>:</span></span><br><span class="line">        html = self.session.get(self.login_url, headers=self.headers)</span><br><span class="line">        response = etree.HTML(html.text)</span><br><span class="line">        token = response.xpath(<span class="string">"//input[@name='authenticity_token']/@value"</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> token</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, html)</span>:</span></span><br><span class="line">        response = etree.HTML(html)</span><br><span class="line">        name = response.xpath(<span class="string">"//dl[@class='form-group'][1]/dd[1]/input/@value"</span>)[<span class="number">0</span>]</span><br><span class="line">        location = response.xpath(<span class="string">"//dl[@class='form-group'][6]/dd[1]/input/@value"</span>)[<span class="number">0</span>]</span><br><span class="line">        print(<span class="string">'姓名: %s, 地址: %s'</span> % (name, location))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self, token, email, password)</span>:</span></span><br><span class="line"></span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">'commit'</span>: <span class="string">'Sign in'</span>,</span><br><span class="line">            <span class="string">'utf8'</span>: <span class="string">'✓'</span>,</span><br><span class="line">            <span class="string">'authenticity_token'</span>: token,</span><br><span class="line">            <span class="string">'login'</span>: email,</span><br><span class="line">            <span class="string">'password'</span>: password</span><br><span class="line">        &#125;</span><br><span class="line">        response = self.session.post(self.post_url, data=data, headers=self.headers)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            print(<span class="string">'登录成功，正在跳转到个人信息...'</span>)</span><br><span class="line"></span><br><span class="line">        response = self.session.get(self.logined_url, headers=self.headers)</span><br><span class="line">        self.parse(response.text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    login = github_login()</span><br><span class="line">    token = login.get_token()</span><br><span class="line">    login.login(token, <span class="string">'username'</span>, <span class="string">'password'</span>)</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    <summary type="html">
    
      目前，大部分网站都具有用户登录功能，其中的某些网站只有在用户登陆后才能获取到有价值的信息，因此爬虫也就需要有模拟真实用户登录的功能。
    
    </summary>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫--美团美食信息</title>
    <link href="http://yoursite.com/meituanspider/"/>
    <id>http://yoursite.com/meituanspider/</id>
    <published>2018-04-25T09:43:37.000Z</published>
    <updated>2019-01-15T02:47:05.254Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/meituan/meituan-logo.png" alt=""></p><hr><p>美团网也是动态加载的网页，但是我看了下，其美食信息也在源代码中有，因此可以按照<a href="http://tangx1.com/2018/08/15/taobao2/" target="_blank" rel="noopener">Python爬虫–淘宝(2)</a>其中的方式去爬美食信息。</p><p>步骤照搬：</p><p>1.获取网页源代码。</p><p>2.使用网页解析工具进行解析，构造翻页网址，提取所需信息。</p><p>3.整理并保存信息。</p><hr><h1 id="获取网页源代码"><a href="#获取网页源代码" class="headerlink" title="获取网页源代码"></a>获取网页源代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以爬取北京美团美食列表为例</span></span><br><span class="line">url = <span class="string">'http://bj.meituan.com/meishi/'</span></span><br><span class="line"><span class="comment"># 请求网页</span></span><br><span class="line">response = requests.get(url, headers=headers)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure><p><img src="https://hexo-pics.nos-eastchina1.126.net/meituan/meituan1-1.jpg" alt=""></p><h1 id="提取信息"><a href="#提取信息" class="headerlink" title="提取信息"></a>提取信息</h1><p>在上一步操作得到源代码之后，仔细观察，可以看到带有商家信息的部分为一段 json 数据，这里稍有不同的是，我们可以直接在源代码中使用正则表达式匹配出含有商家信息的 json 信息。  </p><h2 id="匹配-json-数据段"><a href="#匹配-json-数据段" class="headerlink" title="匹配 json 数据段"></a>匹配 json 数据段</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="comment"># findall获取得到的是一个列表</span></span><br><span class="line">items = re.findall(<span class="string">r'"poiLists":(.`?),"comHeader"'</span>, response.text, re.S)</span><br><span class="line"><span class="comment"># 而列表中只有一个元素</span></span><br><span class="line">item = json.loads(items[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p><img src="https://hexo-pics.nos-eastchina1.126.net/meituan/meituan1-2.jpg" alt=""></p><h2 id="提取信息-1"><a href="#提取信息-1" class="headerlink" title="提取信息"></a>提取信息</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> item.get(<span class="string">'poiInfos'</span>):</span><br><span class="line">    <span class="keyword">for</span> info <span class="keyword">in</span> item.get(<span class="string">'poiInfos'</span>):</span><br><span class="line">        id = info.get(<span class="string">'poiId'</span>)</span><br><span class="line">        title = info.get(<span class="string">'title'</span>)</span><br><span class="line">        addr = info.get(<span class="string">'address'</span>)</span><br><span class="line">        avgScore = <span class="string">'平均分'</span> + str(info.get(<span class="string">'avgScore'</span>))</span><br><span class="line">        allCommentNum = str(info.get(<span class="string">'allCommentNum'</span>)) + <span class="string">'条评论'</span></span><br><span class="line">        avgPrice = str(info.get(<span class="string">'avgPrice'</span>)) + <span class="string">'元/人'</span></span><br><span class="line">        print(id, title, addr, avgPrice, avgScore, allCommentNum)</span><br></pre></td></tr></table></figure><p><img src="https://hexo-pics.nos-eastchina1.126.net/meituan/meituan1-3.jpg" alt=""></p><h1 id="构造翻页网址"><a href="#构造翻页网址" class="headerlink" title="构造翻页网址"></a>构造翻页网址</h1><p>通过翻页找到页码规律，每个网页最后面 <code>pn</code> 后面的数字就是页码。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 爬取 1-9 页美食信息</span></span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">10</span>):</span><br><span class="line">    url = <span class="string">'http://bj.meituan.com/meishi/pn&#123;&#125;'</span>.format(page)</span><br></pre></td></tr></table></figure></p><p><img src="https://hexo-pics.nos-eastchina1.126.net/meituan/meituan1-4.jpg" alt=""></p><h1 id="保存数据"><a href="#保存数据" class="headerlink" title="保存数据"></a>保存数据</h1><p>保存到MongoDB数据库。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/meituan/meituanmongo1.jpg" alt=""></p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">10</span>):</span><br><span class="line">    url = <span class="string">'http://bj.meituan.com/meishi/pn&#123;&#125;'</span>.format(page)</span><br><span class="line">    response = requests.get(url, headers=headers)</span><br><span class="line">    items = re.findall(<span class="string">r'"poiLists":(.`?),"comHeader"'</span>, response.text, re.S)</span><br><span class="line">    item = json.loads(items[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> item.get(<span class="string">'poiInfos'</span>):</span><br><span class="line">        <span class="keyword">for</span> info <span class="keyword">in</span> item.get(<span class="string">'poiInfos'</span>):</span><br><span class="line">            id = info.get(<span class="string">'poiId'</span>)</span><br><span class="line">            title = info.get(<span class="string">'title'</span>)</span><br><span class="line">            addr = info.get(<span class="string">'address'</span>)</span><br><span class="line">            avgScore = <span class="string">'平均分'</span> + str(info.get(<span class="string">'avgScore'</span>))</span><br><span class="line">            allCommentNum = str(info.get(<span class="string">'allCommentNum'</span>)) + <span class="string">'条评论'</span></span><br><span class="line">            avgPrice = str(info.get(<span class="string">'avgPrice'</span>)) + <span class="string">'元/人'</span></span><br><span class="line">            print(id, title, addr, avgPrice, avgScore, allCommentNum)</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    <summary type="html">
    
      美团网的美食信息，
    
    </summary>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫之淘宝(二)</title>
    <link href="http://yoursite.com/taobaospider2/"/>
    <id>http://yoursite.com/taobaospider2/</id>
    <published>2018-04-18T08:00:33.000Z</published>
    <updated>2019-01-15T02:49:22.537Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/tb/tb-logo.png" alt=""></p><hr><p>上一篇介绍了如何用 selenium 自动化工具去帮助爬取淘宝商品的各项数据，但是，我发现，淘宝的网页源代码中就包含有宝贝信息，信息被放在了 script 标签中，既然信息被包含在源代码中，那就证明可以通过正则或者其它的网页解析方式可以获取到需要的信息，因此这篇就来写一写对于淘宝商品来说更为简单的小爬虫。</p><p>这次的流程就比较简单了，先做好准备工作：</p><p>1.获取网页源代码。</p><p>2.使用网页解析工具进行解析，构造翻页网址，提取所需信息。</p><p>3.整理并保存信息。</p><hr><h1 id="获取网页源代码"><a href="#获取网页源代码" class="headerlink" title="获取网页源代码"></a>获取网页源代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 搜索关键词为python的商品列表页</span></span><br><span class="line">url = <span class="string">'https://s.taobao.com/search?q=python'</span></span><br><span class="line"><span class="comment"># 请求网页</span></span><br><span class="line">response = requests.get(url, headers=headers)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure><p>通过 requests 的 get 方法请求网页之后，得到网页源代码。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/tb/taobao2-1.jpg" alt=""></p><h1 id="提取信息"><a href="#提取信息" class="headerlink" title="提取信息"></a>提取信息</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">title = re.findall(<span class="string">r'"raw_title":"(.`?)"'</span>, response.text, re.I)</span><br><span class="line">price = re.findall(<span class="string">r'"view_price":"(.`?)"'</span>, response.text, re.I)</span><br><span class="line">location = re.findall(<span class="string">r'"item_loc":"(.`?)"'</span>, response.text, re.I)</span><br><span class="line">n = len(title)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    print(title[i], price[i], location[i])</span><br></pre></td></tr></table></figure><p>在这里只对商品的标题，价格和发货地进行了正则匹配，因为它们都是列表，要想让宝贝信息都一一对应的显示出来，就要进行遍历，最后把宝贝信息都进行输出。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/tb/taobao2-2.jpg" alt=""></p><h1 id="整理信息"><a href="#整理信息" class="headerlink" title="整理信息"></a>整理信息</h1><p>获取到了宝贝信息之后我们就可以把它们存入文件或者数据库了。这里我们先把它们存入文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">file = open(<span class="string">'taobao.txt'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">n = len(title)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    info = str(page ` <span class="number">44</span> + i + <span class="number">1</span>) + <span class="string">'标题：'</span>+ title[i] + <span class="string">'\n'</span> + <span class="string">'价格：'</span> + price[i] + <span class="string">'\n'</span> + <span class="string">'发货地：'</span> + location[i] + <span class="string">'\n'</span></span><br><span class="line">    file.write(info)</span><br><span class="line">file.close()</span><br></pre></td></tr></table></figure><p><img src="https://hexo-pics.nos-eastchina1.126.net/tb/taobao2-3.jpg" alt=""></p><h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">payload = &#123;<span class="string">'q'</span>:<span class="string">'python'</span>, <span class="string">'s'</span>:<span class="string">'1'</span>, <span class="string">'ie'</span>:<span class="string">'utf8'</span>&#125;</span><br><span class="line">file = open(<span class="string">'taobao.txt'</span>, <span class="string">'a'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    payload[<span class="string">'s'</span>] = <span class="number">44</span> ` page + <span class="number">1</span></span><br><span class="line">    url = <span class="string">'https://s.taobao.com/search?q=python'</span></span><br><span class="line">    response = requests.get(url, params=payload, headers=headers)</span><br><span class="line">    title = re.findall(<span class="string">r'"raw_title":"(.`?)"'</span>, response.text, re.I)</span><br><span class="line">    price = re.findall(<span class="string">r'"view_price":"(.`?)"'</span>, response.text, re.I)</span><br><span class="line">    location = re.findall(<span class="string">r'"item_loc":"(.`?)"'</span>, response.text, re.I)</span><br><span class="line">    n = len(title)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        info = str(page ` <span class="number">44</span> + i + <span class="number">1</span>) + <span class="string">'标题：'</span>+ title[i] + <span class="string">'\n'</span> + <span class="string">'价格：'</span> + price[i] + <span class="string">'\n'</span> + <span class="string">'发货地：'</span> + location[i] + <span class="string">'\n'</span></span><br><span class="line">        file.write(info)</span><br><span class="line">file.close()</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    <summary type="html">
    
      上一篇介绍了如何用 selenium 自动化工具去帮助爬取淘宝商品的各项数据，但是，我发现，淘宝的网页源代码中就包含有宝贝信息，信息被放在了 script 标签中，既然信息被包含在源代码中，那就证明可以通过正则或者其它的网页解析方式可以获取到需要的信息，因此这篇就来写一写对于淘宝商品来说更为简单的小爬虫。
    
    </summary>
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫之淘宝(一)</title>
    <link href="http://yoursite.com/taobaospider1/"/>
    <id>http://yoursite.com/taobaospider1/</id>
    <published>2018-04-17T07:59:42.000Z</published>
    <updated>2019-01-15T02:49:21.708Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/tb/tb-logo.png" alt=""></p><hr><p>淘宝网也是动态加载的网页，虽然其页面数据也是通过 Ajax 获取的，但是若想像前面一样去分析 Ajax ，在淘宝这里是很复杂的，因为其参数会包含加密密钥，自己构造 Ajax 参数过于复杂。所以并不建议使用和爬取今日头条一样的方法来爬取淘宝。</p><p>先来分析下淘宝的接口，来观察 Ajax 复杂程度。</p><p>打开淘宝，再打开开发者工具，搜索关键词 python ，截获 Ajax 请求，这里看到只有一条请求，并且其内容为 json 形式的商品信息。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/tb/taobao1ajax.jpg" alt=""></p><p><img src="https://hexo-pics.nos-eastchina1.126.net/tb/taobao1ajax1.jpg" alt=""></p><p>再来查看其构造参数。可以看到其中的 ksTS 和 rn 参数不能直接找到其规律，如果想找，只会消耗大量时间。但是如果使用 selenium 来模拟浏览器操作的话，那就不需要再关注这些参数了。因此接下来的部分使用 selenium 来爬取淘宝宝贝信息。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/tb/taobao1ajax2.jpg" alt=""></p><hr><p>selenium 是一个用于 Web 应用程序测试的工具。Selenium 直接运行在浏览器中，就好像真实用户在操作一样，因此在爬虫中主要用来解决 js 渲染问题。其支持很多浏览器，常见的 Chrome， Firefox， ie，safari 和无头浏览器 phantomjs 。</p><p>这次就使用 selenium 这个工具来操作对淘宝商品的爬虫。</p><p>selenium 的使用方法步骤：首先导入所需要的包，然后声明浏览器对象，再去请求网页，最后进行查找节点等等操作。</p><p>导入所需要的包：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br></pre></td></tr></table></figure></p><p>声明浏览器对象(使用 Chrome 浏览器)：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">browser = webdriver.Chrome()</span><br></pre></td></tr></table></figure></p><p>访问网页<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">'https://www.taobao.com'</span></span><br><span class="line">browser.get(url)</span><br></pre></td></tr></table></figure></p><hr><p>下面来写整个项目，按照惯例，先列出步骤。</p><p>1.使用 selenium 访问淘宝商品列表网页，获取网页源代码。</p><p>2.使用网页解析工具采集所需数据。</p><p>3.导出并整理数据。</p><hr><h1 id="使用-selenium-获取网页源代码"><a href="#使用-selenium-获取网页源代码" class="headerlink" title="使用 selenium 获取网页源代码"></a>使用 selenium 获取网页源代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">url = <span class="string">'https://s.taobao.com/search?q=python'</span></span><br><span class="line">browser.get(url)</span><br><span class="line"><span class="comment"># 使用page_source获取网页源代码</span></span><br><span class="line">html = browser.page_source</span><br></pre></td></tr></table></figure><h1 id="获取所需数据"><a href="#获取所需数据" class="headerlink" title="获取所需数据"></a>获取所需数据</h1><p>在商品列表页中，要爬取的信息有商品图片、商品价格、商品成交量、商品名称、店铺名称和位置这六项信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"></span><br><span class="line">doc = pq(html)</span><br><span class="line">items = doc(<span class="string">'#mainsrp-itemlist .items .item'</span>).items()</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">    product = &#123;</span><br><span class="line">        <span class="string">'image'</span>: item.find(<span class="string">'.pic .img'</span>).attr(<span class="string">'data-src'</span>).replace(<span class="string">'//'</span>, <span class="string">'http://'</span>),</span><br><span class="line">        <span class="string">'price'</span>: item.find(<span class="string">'.price'</span>).text().replace(<span class="string">'\n'</span>, <span class="string">''</span>),</span><br><span class="line">        <span class="string">'deal'</span>: item.find(<span class="string">'.deal-cnt'</span>).text(),</span><br><span class="line">        <span class="string">'title'</span>: item.find(<span class="string">'.title'</span>).text(),</span><br><span class="line">        <span class="string">'shop'</span>: item.find(<span class="string">'.shop'</span>).text(),</span><br><span class="line">        <span class="string">'location'</span>: item.find(<span class="string">'.location'</span>).text()</span><br><span class="line">    &#125;</span><br><span class="line">    print(product)</span><br></pre></td></tr></table></figure><p>获取到以下数据。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/tb/taobao1pro.jpg" alt=""></p><h1 id="翻页"><a href="#翻页" class="headerlink" title="翻页"></a>翻页</h1><p>在 index_page 中首先访问了 url ，然后判断当前页码，如果大于1，就进行跳页操作，否则等待页面加载完成。</p><p>等待加载时，使用了 WebDriverWait 对象，它可以指定等待条件，这里指定为10s，如果在这个时间内成功匹配了等待条件，就立即返回结果并向下执行，否则就抛出超时异常。</p><p>比如在这里要等待商品信息加载出来，就制定了 presence_of_element_located 这个条件，然后传入 .m-itemlist .items .item 选择器，而这个选择器对应的页面内容就是每个商品的新消息块，如果加载成功了，就会执行后续的 get_products() 方法。</p><p>关于翻页操作，这里先获取了页码的输入框，赋值为 input ，然后再获取<code>确定</code>按钮，赋值为 submit 。获取到两个元素后，先调用 clear() 方法将页码输入框进行清空，再调用 send_keys() 方法将页码填充进去。</p><p>那么如何确定浏览器有没有跳转到对应的页码呢，可以看到，成功跳转到某一页时，当前页码会在网页底部高亮显示，因此可以拿到高亮显示的 css 选择器与当前传入的页码做对比，如果一致，则跳转成功。继续等待商品加载完成…<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"><span class="keyword">from</span> selenium.common.exceptions <span class="keyword">import</span> TimeoutException</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.wait <span class="keyword">import</span> WebDriverWait</span><br><span class="line"></span><br><span class="line">wait = WebDriverWait(browser, <span class="number">10</span>)</span><br><span class="line">KEYWORD = <span class="string">'python'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(page)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        url = <span class="string">'https://s.taobao.com/search?q='</span> + quote(KEYWORD)</span><br><span class="line">        browser.get(url)</span><br><span class="line">        <span class="keyword">if</span> page &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 页码输入框</span></span><br><span class="line">            input = wait.until(</span><br><span class="line">                EC.presence_of_element_located((By.CSS_SELECTOR, <span class="string">'#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; input'</span>))</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># 页码确定按钮</span></span><br><span class="line">            submit = wait.until(</span><br><span class="line">                EC.presence_of_element_located((By.CSS_SELECTOR,<span class="string">'#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; span.btn.J_Submit'</span>))</span><br><span class="line">            )</span><br><span class="line">            input.clear()</span><br><span class="line">            input.send_keys(page)</span><br><span class="line">            submit.click()</span><br><span class="line">        wait.until(</span><br><span class="line">            EC.text_to_be_present_in_element((By.CSS_SELECTOR, <span class="string">'#mainsrp-pager li.item.active &gt; span'</span>), str(page))</span><br><span class="line">        )</span><br><span class="line">        wait.until(</span><br><span class="line">            EC.presence_of_element_located((By.CSS_SELECTOR, <span class="string">'.m-itemlist .items .item'</span>))</span><br><span class="line">        )</span><br><span class="line">        get_products()</span><br><span class="line">    <span class="keyword">except</span> TimeoutException:</span><br><span class="line">        index_page(page)</span><br></pre></td></tr></table></figure></p><h1 id="保存数据到-MongoDB"><a href="#保存数据到-MongoDB" class="headerlink" title="保存数据到 MongoDB"></a>保存数据到 MongoDB</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">client = pymongo.MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</span><br><span class="line">db = client[<span class="string">'taobao'</span>]</span><br><span class="line">collection = db[<span class="string">'taobao1_spider'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_to_mongo</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> collection.insert(data):</span><br><span class="line">        print(<span class="string">'保存到MongoDB成功'</span>)</span><br></pre></td></tr></table></figure><p>采集到的数据如下。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/tb/taobao1mongo.jpg" alt=""></p><h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"><span class="keyword">from</span> selenium.common.exceptions <span class="keyword">import</span> TimeoutException</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.wait <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">wait = WebDriverWait(browser, <span class="number">10</span>)</span><br><span class="line">KEYWORD = <span class="string">'python'</span></span><br><span class="line">client = pymongo.MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</span><br><span class="line">db = client[<span class="string">'taobao'</span>]</span><br><span class="line">collection = db[<span class="string">'taobao1_spider'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(page)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        url = <span class="string">'https://s.taobao.com/search?q='</span> + quote(KEYWORD)</span><br><span class="line">        browser.get(url)</span><br><span class="line">        <span class="keyword">if</span> page &gt; <span class="number">1</span>:</span><br><span class="line">            input = wait.until(</span><br><span class="line">                EC.presence_of_element_located((By.CSS_SELECTOR, <span class="string">'#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; input'</span>))</span><br><span class="line">            )</span><br><span class="line">            submit = wait.until(</span><br><span class="line">                EC.presence_of_element_located((By.CSS_SELECTOR,<span class="string">'#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; span.btn.J_Submit'</span>))</span><br><span class="line">            )</span><br><span class="line">            input.clear()</span><br><span class="line">            input.send_keys(page)</span><br><span class="line">            submit.click()</span><br><span class="line">        wait.until(</span><br><span class="line">            EC.text_to_be_present_in_element((By.CSS_SELECTOR, <span class="string">'#mainsrp-pager li.item.active &gt; span'</span>), str(page))</span><br><span class="line">        )</span><br><span class="line">        wait.until(</span><br><span class="line">            EC.presence_of_element_located((By.CSS_SELECTOR, <span class="string">'.m-itemlist .items .item'</span>))</span><br><span class="line">        )</span><br><span class="line">        get_products()</span><br><span class="line">    <span class="keyword">except</span> TimeoutException:</span><br><span class="line">        index_page(page)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_products</span><span class="params">()</span>:</span></span><br><span class="line">    html = browser.page_source</span><br><span class="line">    doc = pq(html)</span><br><span class="line">    items = doc(<span class="string">'#mainsrp-itemlist .items .item'</span>).items()</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        product = &#123;</span><br><span class="line">            <span class="string">'image'</span>: item.find(<span class="string">'.pic .img'</span>).attr(<span class="string">'data-src'</span>),</span><br><span class="line">            <span class="string">'price'</span>: item.find(<span class="string">'.price'</span>).text(),</span><br><span class="line">            <span class="string">'deal'</span>: item.find(<span class="string">'.deal-cnt'</span>).text(),</span><br><span class="line">            <span class="string">'title'</span>: item.find(<span class="string">'.title'</span>).text(),</span><br><span class="line">            <span class="string">'shop'</span>: item.find(<span class="string">'.shop'</span>).text(),</span><br><span class="line">            <span class="string">'location'</span>: item.find(<span class="string">'.location'</span>).text()</span><br><span class="line">        &#125;</span><br><span class="line">        print(product)</span><br><span class="line">        save_to_mongo(product)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_to_mongo</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> collection.insert(data):</span><br><span class="line">        print(<span class="string">'保存到MongoDB成功'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">101</span>):</span><br><span class="line">        index_page(page)</span><br><span class="line">    browser.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ = <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    <summary type="html">
    
      淘宝网也是动态加载的网页，虽然其页面数据也是通过 Ajax 获取的，但是若想像前面一样去分析 Ajax ，在淘宝这里是很复杂的，因为其参数会包含加密密钥，自己构造 Ajax 参数过于复杂。所以并不建议使用和爬取今日头条一样的方法来爬取淘宝。
    
    </summary>
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="selenium" scheme="http://yoursite.com/tags/selenium/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫--京东商品评论</title>
    <link href="http://yoursite.com/jd_comments/"/>
    <id>http://yoursite.com/jd_comments/</id>
    <published>2018-04-15T12:11:19.000Z</published>
    <updated>2019-01-15T02:50:24.507Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/jdcomments/jd-logo.png" alt=""></p><hr><p>京东网站是我经常去购物的网站，现在来爬京东商品的评论。有时候网速慢，打开评论的时候还要等一会，一直在加载，也证明了其评论是网页动态加载的。<br>而去爬取动态加载的网页，无非就是查看其 js 请求和使用 selenium 工具，这里使用了前者的方法，比较简单。</p><hr><h1 id="查看动态-js-请求"><a href="#查看动态-js-请求" class="headerlink" title="查看动态 js 请求"></a>查看动态 js 请求</h1><p>先打开开发者工具，然后再打开京东商品 iPhone X 评论页面，可以看到，请求列表中有一个 productPageComments 文件，打开它，点击 Preview 标签，下面的数据是 json 形式，再点击 Comments ，查看其中正是需要的评论数据。</p><p>js 文件网址 : <a href="https://sclub.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv75398&amp;productId=5089253&amp;score=0&amp;sortType=5&amp;page=0&amp;pageSize=10&amp;isShadowSku=0&amp;rid=0&amp;fold=1" target="_blank" rel="noopener">https://sclub.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv75398&amp;productId=5089253&amp;score=0&amp;sortType=5&amp;page=0&amp;pageSize=10&amp;isShadowSku=0&amp;rid=0&amp;fold=1</a></p><p><img src="https://hexo-pics.nos-eastchina1.126.net/jdcomments/jd_com1.jpg" alt=""></p><h1 id="对-js-文件进行请求"><a href="#对-js-文件进行请求" class="headerlink" title="对 js 文件进行请求"></a>对 js 文件进行请求</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">url = <span class="string">'https://sclub.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv75398&amp;productId=5089253&amp;score=0&amp;sortType=5&amp;page=0&amp;pageSize=10&amp;isShadowSku=0&amp;fold=1'</span></span><br><span class="line">response = requests.get(url, headers=headers)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure><p>在这段代码中，先查看对 js 文件网址请求之后，得到的数据是什么。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/jdcomments/jd_com2.jpg" alt=""></p><p>输出的数据为 json 形式，但是数据首尾都有一些多余数据，为了把它变成正常的 json 形式，我们用 replace 方法替换掉多余部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">result = response.replace(<span class="string">'fetchJSON_comment98vv75398('</span>, <span class="string">''</span>)</span><br><span class="line">result = result.replace(<span class="string">');'</span>, <span class="string">''</span>)</span><br><span class="line">json_so = json.loads(result)</span><br><span class="line">print(json_so)</span><br></pre></td></tr></table></figure><p><img src="https://hexo-pics.nos-eastchina1.126.net/jdcomments/jd_com3.jpg" alt=""></p><p>这样我们输出的数据就是正常的 json 格式，可以进行提取信息的操作了。</p><h1 id="提取数据并构造翻页网址"><a href="#提取数据并构造翻页网址" class="headerlink" title="提取数据并构造翻页网址"></a>提取数据并构造翻页网址</h1><p>把上一步输出的数据放入 <a href="https://www.json.cn/" target="_blank" rel="noopener">json在线解析</a> 网页中进行解析，得到如下图。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/jdcomments/jd_com4.jpg" alt=""></p><p>证明我们上一步操作没问题，接下来提取数据。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">items = json_so.get(<span class="string">'comments'</span>)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">   id = item.get(<span class="string">'id'</span>)</span><br><span class="line">   content = item.get(<span class="string">'content'</span>)</span><br><span class="line">   create_time = item.get(<span class="string">'creationTime'</span>)</span><br><span class="line">   nick_name = item.get(<span class="string">'nickname'</span>)</span><br><span class="line">   client = item.get(<span class="string">'userClientShow'</span>)</span><br><span class="line">   print(id, nick_name, content, create_time, client)</span><br></pre></td></tr></table></figure></p><p>这样就可以得到商品评论的 id， 内容，评论时间，评论人昵称和评论人客户端信息，如下图。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/jdcomments/jd_com5.jpg" alt=""></p><p>接下来要构造翻页网址，通过观察每一评论页网址，发现其参数中有个 page 参数，这个是页码参数，所以我们只要对 url 中传入不同的 page 值，就可以进行翻页。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    url = <span class="string">'https://sclub.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv75398&amp;productId=5089253&amp;score=0&amp;sortType=5&amp;page='</span> + str(page) + <span class="string">'&amp;pageSize=10&amp;isShadowSku=0&amp;fold=1'</span></span><br></pre></td></tr></table></figure></p><h1 id="整理数据"><a href="#整理数据" class="headerlink" title="整理数据"></a>整理数据</h1><p>最后把评论信息都写入文件中，如下图。</p><h2 id="保存至本地文本文件"><a href="#保存至本地文本文件" class="headerlink" title="保存至本地文本文件"></a>保存至本地文本文件</h2><p><img src="https://hexo-pics.nos-eastchina1.126.net/jdcomments/jd_com6.jpg" alt=""></p><h2 id="保存至MongoDB数据库"><a href="#保存至MongoDB数据库" class="headerlink" title="保存至MongoDB数据库"></a>保存至MongoDB数据库</h2><p><img src="https://hexo-pics.nos-eastchina1.126.net/jdcomments/jdcommongo.jpg" alt=""></p><h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">file = open(<span class="string">'jd_com.txt'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">client = pymongo.MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</span><br><span class="line">db = client[<span class="string">'jd'</span>]</span><br><span class="line">collection = db[<span class="string">'jdcom_spider'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    url = <span class="string">'https://sclub.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv75398&amp;productId=5089253&amp;score=0&amp;sortType=5&amp;page='</span> + str(page) + <span class="string">'&amp;pageSize=10&amp;isShadowSku=0&amp;fold=1'</span></span><br><span class="line">    response = requests.get(url, headers=headers)</span><br><span class="line">    html = response.text</span><br><span class="line">    result = html.replace(<span class="string">'fetchJSON_comment98vv75398('</span>, <span class="string">''</span>)</span><br><span class="line">    result = result.replace(<span class="string">');'</span>, <span class="string">''</span>)</span><br><span class="line">    json_so = json.loads(result)</span><br><span class="line">    items = json_so.get(<span class="string">'comments'</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        id = item.get(<span class="string">'id'</span>)</span><br><span class="line">        content = item.get(<span class="string">'content'</span>)</span><br><span class="line">        create_time = item.get(<span class="string">'creationTime'</span>)</span><br><span class="line">        nick_name = item.get(<span class="string">'nickname'</span>)</span><br><span class="line">        client = item.get(<span class="string">'userClientShow'</span>)</span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">'id'</span>: id,</span><br><span class="line">            <span class="string">'内容'</span>: content,</span><br><span class="line">            <span class="string">'评论时间'</span>: create_time,</span><br><span class="line">            <span class="string">'昵称'</span>: nick_name,</span><br><span class="line">            <span class="string">'客户端'</span>: client</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment"># 保存至本地文本文件</span></span><br><span class="line">        file.write(str(data) + <span class="string">'\n'</span>)</span><br><span class="line">        <span class="comment"># 保存至MongoDB数据库</span></span><br><span class="line">        <span class="keyword">if</span> collection.insert(data):</span><br><span class="line">            print(<span class="string">'保存至MongoDB成功'</span>)</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    <summary type="html">
    
      京东网站是我经常去购物的网站，现在来爬京东商品的评论。有时候网速慢，打开评论的时候还要等一会，一直在加载，也证明了其评论是网页动态加载的。
    
    </summary>
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>爬虫之今日头条街拍</title>
    <link href="http://yoursite.com/jrtt_spider/"/>
    <id>http://yoursite.com/jrtt_spider/</id>
    <published>2018-04-11T09:00:28.000Z</published>
    <updated>2019-01-15T02:50:55.813Z</updated>
    
    <content type="html"><![CDATA[<hr><p><img src="https://hexo-pics.nos-eastchina1.126.net/jrtt/jrtt-logo.png" alt=""></p><hr><p>前面的爬虫都是静态页面，遇到动态页面该如何爬取，当时困惑了好久，不知道如何下手，参考了几篇其他大佬的文章，才慢慢有一点懂。</p><p>这次的网页是动态加载的今日头条街拍图集网页。看了崔大大的教程，自己动手码一下代码，熟悉一下动态页面的爬虫步骤。</p><p>动态页面肯定不能像静态页面一样直接取数据，因为它的数据都是通过 js 渲染进来的，因此先找到对应的数据 js 文件。</p><p>首先，先来规划下步骤：</p><p>1.观察 js 请求，查看数据通过哪个文件传输。</p><p>2.对 js 文件进行请求，获得信息。</p><p>3.提取数据，对数据进行处理。</p><hr><h1 id="查看-js-请求"><a href="#查看-js-请求" class="headerlink" title="查看 js 请求"></a>查看 js 请求</h1><p>打开今日头条，搜索框内输入关键词“街拍”，跳转到街拍页面，往下拉，能看到图片一直在加载新的，而网址没有改变，动态的没错了，打开 F12 ，网页继续往下拉，Network 下出现了新的请求，并且这些请求构造都差不多，随便点击一个请求，打开 Preview ，里面是 json 格式的，其中有图片标题，也有图片的 url 信息，看到里面就是想要采集的数据，要找的就是它。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/jrtt/jrtt1.jpg" alt=""></p><p>请求方式为 GET , 再来看下 Form Data ，有 offset , format , keyword , autoload , count , cur_tab , from 这几个参数，offset 是偏移量，也就是一共刷新出来的图片数量，keyword 是关键词， count 是每一页刷新出来的图片数量， 其它参数没什么重要意义，构造网址时直接加上去就行了。</p><p><img src="https://hexo-pics.nos-eastchina1.126.net/jrtt/jrtt2.jpg" alt=""></p><h1 id="对js进行请求"><a href="#对js进行请求" class="headerlink" title="对js进行请求"></a>对js进行请求</h1><p>想要对 js 进行请求，需要先对网址进行构造。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">headers = &#123;</span><br><span class="line">  <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Form Data 中的参数</span></span><br><span class="line">params = &#123;</span><br><span class="line">        <span class="string">'offset'</span>: page,</span><br><span class="line">        <span class="string">'format'</span>: <span class="string">'json'</span>,</span><br><span class="line">        <span class="string">'keyword'</span>: <span class="string">'街拍'</span>,</span><br><span class="line">        <span class="string">'autoload'</span>: <span class="string">'true'</span>,</span><br><span class="line">        <span class="string">'count'</span>: <span class="string">'20'</span>,</span><br><span class="line">        <span class="string">'cur_tab'</span>: <span class="string">'3'</span>,</span><br><span class="line">        <span class="string">'from'</span>: <span class="string">'gallery'</span></span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment"># 对网址进行构造， 使用urlencode方法</span></span><br><span class="line">url = <span class="string">'https://www.toutiao.com/search_content/?'</span> + urlencode(params)</span><br></pre></td></tr></table></figure><p>构造完毕，对网址进行请求。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对网址进行请求，获取到的json格式</span></span><br><span class="line">response = requests.get(url, headers=headers)</span><br><span class="line"><span class="keyword">if</span> response.status_code == <span class="number">200</span>:  </span><br><span class="line">    <span class="keyword">return</span> response.json()</span><br></pre></td></tr></table></figure></p><h1 id="提取信息"><a href="#提取信息" class="headerlink" title="提取信息"></a>提取信息</h1><p>经过上一步对 js 进行请求后，得到和之前看到的 Preview 里面一样的信息，开始对数据进行提取处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">json = response.json()</span><br><span class="line"><span class="comment"># 判断json的data，如果存在继续下一步，其实这里用.get方法更好，因为.get方法如果获取一个不存在的属性时不会报错，而直接使用['']这种方法，如果不存在此属性，就会直接报错，影响程序效率</span></span><br><span class="line"><span class="keyword">if</span> json[<span class="string">'data'</span>]:</span><br><span class="line">  <span class="comment"># data是个列表，对列表中数据继续遍历</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> json[<span class="string">'data'</span>]:</span><br><span class="line">      <span class="comment"># 获取到title信息和图片地址信息</span></span><br><span class="line">        title = item.get(<span class="string">'title'</span>)</span><br><span class="line">        image_urls = item.get(<span class="string">'image_list'</span>)</span><br><span class="line">        <span class="comment"># 因为有多张图片，因此图片信息被装在一个列表中，需要继续遍历</span></span><br><span class="line">        <span class="keyword">for</span> image_url <span class="keyword">in</span> image_urls:</span><br><span class="line">          <span class="comment"># 这里把图片url中的list换成large，提取大图</span></span><br><span class="line">            image = <span class="string">'http://'</span> + image_url.get(<span class="string">'url'</span>).strip(<span class="string">'//'</span>).replace(<span class="string">'list'</span>, <span class="string">'large'</span>)</span><br><span class="line">            <span class="comment"># 返回image和title信息</span></span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">'image'</span>: image,</span><br><span class="line">                <span class="string">'title'</span>: title</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure><hr><h1 id="保存数据"><a href="#保存数据" class="headerlink" title="保存数据"></a>保存数据</h1><h2 id="保存到本地文件夹"><a href="#保存到本地文件夹" class="headerlink" title="保存到本地文件夹"></a>保存到本地文件夹</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> md5</span><br><span class="line"><span class="comment"># 判断当前文件路径下是否存在以图片的标题命名的文件夹，如果不存在，就新建文件夹</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(item.get(<span class="string">'title'</span>)):</span><br><span class="line">    os.mkdir(item.get(<span class="string">'title'</span>))</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">  <span class="comment"># 请求图片的url地址</span></span><br><span class="line">    response = requests.get(item.get(<span class="string">'image'</span>))</span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">        file_path = <span class="string">'&#123;0&#125;/&#123;1&#125;.&#123;2&#125;'</span>.format(item.get(<span class="string">'title'</span>), md5(response.content).hexdigest(), <span class="string">'jpg'</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(file_path):</span><br><span class="line">            <span class="keyword">with</span> open(file_path, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(response.content)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">'已经下载过！'</span>)</span><br><span class="line"><span class="keyword">except</span> requests.ConnectionError:</span><br><span class="line">    print(<span class="string">'连接失败!'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://hexo-pics.nos-eastchina1.126.net/jrtt/jrtt3.jpg" alt=""></p><p><img src="https://hexo-pics.nos-eastchina1.126.net/jrtt/jrtt4.jpg" alt=""></p><h2 id="保存到-MongoDB-数据库。"><a href="#保存到-MongoDB-数据库。" class="headerlink" title="保存到 MongoDB 数据库。"></a>保存到 MongoDB 数据库。</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_to_mongo</span><span class="params">(data)</span>:</span></span><br><span class="line">    client = pymongo.MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</span><br><span class="line">    db = client[<span class="string">'jrtt'</span>]</span><br><span class="line">    collection = db[<span class="string">'jrtt_spider'</span>]</span><br><span class="line">    <span class="keyword">if</span> collection.insert(data):</span><br><span class="line">        print(<span class="string">'保存到MongoDB成功'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://hexo-pics.nos-eastchina1.126.net/jrtt/jrttmongo.jpg" alt=""></p><h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> md5</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_page_source</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url, headers=headers)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.json()</span><br><span class="line">    <span class="keyword">except</span> Exception:</span><br><span class="line">        print(<span class="string">'error'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_content</span><span class="params">(json)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> json[<span class="string">'data'</span>]:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> json[<span class="string">'data'</span>]:</span><br><span class="line">            title = item.get(<span class="string">'title'</span>)</span><br><span class="line">            image_urls = item.get(<span class="string">'image_list'</span>)</span><br><span class="line">            <span class="keyword">for</span> image_url <span class="keyword">in</span> image_urls:</span><br><span class="line">                image = <span class="string">'http://'</span> + image_url.get(<span class="string">'url'</span>).strip(<span class="string">'//'</span>).replace(<span class="string">'list'</span>, <span class="string">'large'</span>)</span><br><span class="line">                <span class="keyword">yield</span> &#123;</span><br><span class="line">                    <span class="string">'image'</span>: image,</span><br><span class="line">                    <span class="string">'title'</span>: title</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_to_file</span><span class="params">(item)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(item.get(<span class="string">'title'</span>)):</span><br><span class="line">        os.mkdir(item.get(<span class="string">'title'</span>))</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(item.get(<span class="string">'image'</span>))</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            file_path = <span class="string">'&#123;0&#125;/&#123;1&#125;.&#123;2&#125;'</span>.format(item.get(<span class="string">'title'</span>), md5(response.content).hexdigest(), <span class="string">'jpg'</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(file_path):</span><br><span class="line">                <span class="keyword">with</span> open(file_path, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                    f.write(response.content)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">'已经下载过！'</span>)</span><br><span class="line">    <span class="keyword">except</span> requests.ConnectionError:</span><br><span class="line">        print(<span class="string">'连接失败!'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_to_mongo</span><span class="params">(data)</span>:</span></span><br><span class="line">    client = pymongo.MongoClient(<span class="string">'localhost'</span>, <span class="number">27017</span>)</span><br><span class="line">    db = client[<span class="string">'jrtt'</span>]</span><br><span class="line">    collection = db[<span class="string">'jrtt_spider'</span>]</span><br><span class="line">    <span class="keyword">if</span> collection.insert(data):</span><br><span class="line">        print(<span class="string">'保存到MongoDB成功'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(page)</span>:</span></span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">'offset'</span>: page,</span><br><span class="line">        <span class="string">'format'</span>: <span class="string">'json'</span>,</span><br><span class="line">        <span class="string">'keyword'</span>: <span class="string">'街拍'</span>,</span><br><span class="line">        <span class="string">'autoload'</span>: <span class="string">'true'</span>,</span><br><span class="line">        <span class="string">'count'</span>: <span class="string">'20'</span>,</span><br><span class="line">        <span class="string">'cur_tab'</span>: <span class="string">'3'</span>,</span><br><span class="line">        <span class="string">'from'</span>: <span class="string">'gallery'</span></span><br><span class="line">    &#125;</span><br><span class="line">    url = <span class="string">'https://www.toutiao.com/search_content/?'</span> + urlencode(params)</span><br><span class="line">    json = get_page_source(url)</span><br><span class="line">    items = get_content(json)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">      <span class="comment"># 保存至本地文件夹</span></span><br><span class="line">        save_to_file(item)</span><br><span class="line">      <span class="comment"># 保存标题和链接到MongoDB数据库</span></span><br><span class="line">        save_to_mongo(item)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    START = <span class="number">1</span></span><br><span class="line">    END = <span class="number">20</span></span><br><span class="line">    pool = Pool()</span><br><span class="line">    groups = ([x ` <span class="number">20</span> <span class="keyword">for</span> x <span class="keyword">in</span> range(START, END+<span class="number">1</span>)])</span><br><span class="line">    pool.map(main, groups)</span><br><span class="line">    pool.close()</span><br><span class="line">    pool.join()</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    <summary type="html">
    
      前面的爬虫都是静态页面，遇到动态页面该如何爬取，当时困惑了好久，不知道如何下手，参考了几篇其他大佬的文章，才慢慢有一点懂。
    
    </summary>
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="动态网页" scheme="http://yoursite.com/tags/%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5/"/>
    
  </entry>
  
</feed>
